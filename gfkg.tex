\documentclass[11pt, a4paper]{report}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{interval}
\usepackage{faktor}
\usepackage{mleftright}
\usepackage{tikz-cd}
\usepackage[tracking=true, kerning=true]{microtype}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{inconsolata}
\usepackage{hyperref}
\usepackage[strict]{csquotes}
\usepackage[titles]{tocloft}
\usepackage[nobottomtitles*]{titlesec}
\usepackage{titling}
\usepackage{parskip}

\DeclareMicrotypeAlias{lmss}{cmr}

\frenchspacing

\allowdisplaybreaks[2]

\setlength{\topmargin}{0pt}
\setlength{\textheight}{612pt}
\setlength{\textwidth}{368pt}
\setlength{\hoffset}{-4pt}

\setlength{\droptitle}{-92pt}

\setlength{\cftchapnumwidth}{2.2em}
\cftsetindents{section}{2.2em}{2.9em}

\let\endtitlepage\relax

\titleclass{\part}{top}
\titleformat{\part}[display]{\huge\bfseries}{\Large\partname\ \thepart}{0pt}{}{}
\titlespacing*{\part}{0pt}{*0}{*3}

\titleclass{\chapter}{straight}
\titleformat{\chapter}{\LARGE\bfseries}{\thechapter}{0.8em}{}{}
\titlespacing*{\chapter}{0pt}{*4}{*3}

\newcommand*{\chapterbreak}{\ifnum\value{chapter}>0 \clearpage\fi}
\renewcommand*\contentsname{}

\counterwithin{chapter}{part}
\counterwithout{footnote}{chapter}

\theoremstyle{definition}
\newtheorem{ex}{Exercise}[part]
\newtheorem{sol}{Solution}[part]

\mathtoolsset{%
    showonlyrefs,
    showmanualtags
}

\tikzcdset{
    arrow style=Latin Modern
}

\mleftright{}

\intervalconfig{soft open fences}

\newtagform{nowidth}{ \llap\bgroup(}{)\egroup}  % chktex 9
\newenvironment{nowidthtags}{\usetagform{nowidth}}{\ignorespacesafterend}

\newcommand*{\delimitershortfallcmd}[1]{\delimitershortfall=#1}
\newenvironment{dsfalign}
    {\delimitershortfallcmd{0pt}\align}
    {\endalign\ignorespacesafterend}

\newenvironment{epigraph}
    {\begin{quote}\small\itshape} % chktex 6
    {\end{quote}\ignorespacesafterend\vspace{\parskip}}

\newcommand*{\settightmatrix}{\ensuremath\setlength{\arraycolsep}{0.7ex}}
\newenvironment{tightmatrix}
    {\settightmatrix}{}

\newcommand*{\https}[1]{\normalsize\texttt{\href{https://#1/}{#1}}}
\newcommand*{\norm}[1]{\ensuremath{\left\Vert#1\right\Vert}}

\DeclarePairedDelimiterX{\innerp}[2]{\langle}{\rangle}{#1, #2}

\newcommand*{\SetSymbol}[1][]{\nonscript\:#1\vert\allowbreak\nonscript\:\mathopen{}}
\DeclarePairedDelimiterX{\setbuilder}[1]{\{}{\}}
    {\newcommand*{\given}{\SetSymbol[\delimsize]}#1}

\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

\newcommand*{\m}{\ensuremath\:\, \mathllap{-}}

\newcommand*{\GL}{\ensuremath\text{GL}}
\newcommand*{\SL}{\ensuremath\text{SL}}
\renewcommand*{\O}{\ensuremath\text{O}}
\newcommand*{\SO}{\ensuremath\text{SO}}
\newcommand*{\U}{\ensuremath\text{U}}
\newcommand*{\SU}{\ensuremath\text{SU}}

\NewCommandCopy{\binstar}{\star}
\renewcommand*{\star}{\ensuremath\mathop{}\mathopen\binstar\mathord{}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Vect}{Vect}
\newcommand*{\id}{\ensuremath\mathop{}\mathopen{}\mathrm{id}}

\title{Gauge Fields, Knots and Gravity Solutions}
\author{Fionn Fitzmaurice \hspace{2em} \https{fionn.github.io/gfkg}}
\date{}

\makeatletter
\hypersetup{pdftitle = \@title,
            pdfauthor = Fionn\ Fitzmaurice,
            pdfcreator = TeX,
            hidelinks,
            pdfpagemode = UseNone
}
\makeatother

\begin{document}

\thispagestyle{empty}

\maketitle

\vspace{-20ex}

\tableofcontents

\pagenumbering{roman}
\thispagestyle{empty}

\clearpage

\pagenumbering{arabic}

\part{Electromagnetism}\label{part:electromagnetism}

\chapter{Maxwell's Equations}

\begin{epigraph}
    We are, as it were, on an unruffled sea, without stars, compass, sounding, wind or tide,
    and we cannot tell in what direction we are going.
\end{epigraph}

\begin{ex}\label{ex:i1}

Let $\vec{k}$ be a vector in $\mathbb{R}^3$ and let $\omega = |\vec{k}|$. Fix $\vec{E} \in \mathbb{C}^3$ with $\vec{k} \cdot \vec{E} = 0$ and $i \vec{k} \times \vec{E} = \omega \vec{E}$. Show that
\[
    \vec{\mathcal{E}}(t, \vec{x}) = \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})}
\]
satisfies the vacuum Maxwell equations.

\end{ex}

\begin{sol}

Recall that Maxwell's equations are
\begin{gather*}
    \nabla \cdot \vec{B} = 0, \qquad
    \nabla \times \vec{E} + \frac{\partial \vec{B}}{\partial t} = 0, \\
    \nabla \cdot \vec{E} = \rho, \qquad
    \nabla \times \vec{B} - \frac{\partial \vec{E}}{\partial t} = \vec{\jmath}.
\end{gather*}
The vacuum equations are invariant under
\[
    \vec{B} \mapsto \vec{E}, \qquad
    \vec{E} \mapsto - \vec{B}
\]
(electromagnetic duality, see \S\ref{sec:secondpairmaxwellequations}) or, equivalently, for a complex-valued vector field $\vec{\mathcal{E}} = \vec{E} + i \vec{B}$,
\[
    \vec{\mathcal{E}} \mapsto i \vec{\mathcal{E}}.
\]
This lets us express the vacuum equations in terms of $\vec{\mathcal{E}}$ as
\[
    \nabla \cdot \vec{\mathcal{E}} = 0, \qquad
    \nabla \times \vec{\mathcal{E}} = i \frac{\partial \vec{\mathcal{E}}}{\partial t}.
\]

For the divergence,
\begin{align*}
    \nabla \cdot \vec{\mathcal{E}}(t, \vec{x})
        &= \sum_{j = 1}^3 \partial_j \left( E_j  e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= \sum_{j = 1}^3 E_j i k_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= i \vec{k} \cdot \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= 0.
\end{align*}

For the curl (dropping the summation and using Einstein notation),
\begin{align*}
    {\bigl( \nabla \times \vec{\mathcal{E}}(t, \vec{x}) \bigr)}_i &= \epsilon_{ijk} \partial_j \mathcal{E}_k(t, \vec{x}) \\
        &= \epsilon_{ijk} \partial_j \left(E_k e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= \epsilon_{ijk} E_k \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \epsilon_{ijk}i k_j E_k e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= {\left(i \vec{k} \times \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right)}_i \\
        &= \omega E_i e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \omega \mathcal{E}_i(t, \vec{x}),
\end{align*}
so $\nabla \times \vec{\mathcal{E}} = \omega \vec{\mathcal{E}}$.
But
\begin{align*}
    \frac{\partial}{\partial t} \vec{\mathcal{E}}(t, \vec{x}) &= \frac{\partial}{\partial t} \left(\vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= -i \omega \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= -i \omega \vec{\mathcal{E}}(t, \vec{x}),
\end{align*}
giving
\[
    \nabla \times \vec{\mathcal{E}} = \omega \vec{\mathcal{E}} = i \frac{\partial \vec{\mathcal{E}}}{\partial t}
\]
and satisfying the second vacuum equation.

\end{sol}

\chapter{Manifolds}

\begin{epigraph}
    Space and time cannot be defined in such a way that differences of the spatial coordinates can be directly measured by the unit measuring-rod, or differences in the time coordinate by a standard clock.
\end{epigraph}

\begin{ex}

Show that a function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous according to the above definition if and only if it is continuous according to the epsilon--delta definition: for all $x \in \mathbb{R}^n$ and all $\epsilon > 0$, there exists $\delta > 0$ such that $\norm{y - x} < \delta$ implies $\norm{f(y) - f(x)} < \epsilon$.

\end{ex}

\begin{sol}

A function $f: X \to Y$ from one topological space to another is defined to be continuous if, given any open set $U \subseteq Y$, the inverse image $f^{-1}(U) \subseteq X$ is open.

Suppose $f$ is continuous according to the epsilon--delta definition of continuity.
Let $V \subseteq \mathbb{R}^m$ be an open set.
For any $x \in f^{-1}(V)$, since $f(x) \in V$ there exists a ball of radius $\epsilon$, $B \bigl( f(x), \epsilon \bigr) \subseteq V$, centered at $f(x)$.
Then by the epsilon--delta condition there exists a ball of radius $\delta$, $B(x, \delta) \subseteq \mathbb{R}^n$ such that
\[
    f \bigl( B(x, \delta) \bigr) \subset B \bigl( f(x), \epsilon \bigr).
\]
Since $x$ was arbitrary, $f^{-1}(V)$ is open as all points sufficiently close to $x$ are also in $f^{-1}(V)$.

Suppose $f$ is continuous according to the topological definition of continuity.
Let $x \in \mathbb{R}^n$ and $\epsilon > 0$.
Consider the open set $f^{-1} \bigl( B(f(x), \epsilon) \bigr) \subseteq \mathbb{R}^n$.
There exists a $\delta > 0$ such that
\[
    B(x, \delta) \subset f^{-1} \bigl( B(f(x), \epsilon) \bigr).
\]
Therefore for any point $y \in B(x,\delta)$, $f(y) \in B \bigl( f(x), \epsilon \bigr)$ or, equivalently, $\norm{y - x} < \delta$ implies $\norm{f(y) - f(x)} < \epsilon$.

\end{sol}

\begin{ex}

Given a topological space $X$ and a subset $S \subseteq X$, define the \emph{induced topology} on $S$ to be the topology in which the open sets are of the form $U \cap S$, where $U$ is open in $X$.

Let $S^n$, the $n$-sphere, be the unit sphere in $\mathbb{R}^{n + 1}$:
\[
    S^n = \biggl\{\vec{x} \in \mathbb{R}^{n + 1} \Bigm| \sum_{i = 1}^{n + 1} {\bigl(x^i\bigr)}^2 = 1 \biggr\}.
\]
Show that $S^n \subset \mathbb{R}^{n + 1}$ with its induced topology is a manifold.

\end{ex}

\begin{sol}\label{sol:stereographicprojection}

We need to show that:
\begin{itemize}
    \item the open sets of the induced topology $\{U_\alpha\}$ cover $S^n$,
    \item there exists an atlas of charts $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ for all $\alpha$,
    \item the transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}: \mathbb{R}^n \to \mathbb{R}^n$ are smooth where defined (since we include ``smooth'' in our definition of a manifold).
\end{itemize}

Consider the sets
\[
    U_1 = S^n \setminus \left\{(0, \ldots, 0, 1)\right\}, \qquad
    U_{-1} = S^n \setminus \left\{(0, \ldots, 0, -1)\right\}
\]
which each exclude a single pole. Each $U_\alpha$ is of the form $U \cap S^n$ where $U$ is open in $\mathbb{R}^{n + 1}$.
The induced topology $\left\{U_1, U_{-1}\right\}$ is a cover of $S^n$.

Let $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ be the stereographic projection (for $\alpha \in \{-1, 1\})$.
For some $\vec{p} \in S^n$, $\varphi_\alpha(\vec{p}) \in \mathbb{R}^n$ should be a point on the line that intersects $S^n$ at $\vec{s}_\alpha = (0, \ldots, 0, \alpha)$.
Take a segment of this line parameterised by $t \in [0, 1]$ as
\begin{align*}
    (1 - t) \vec{s}_\alpha + t\vec{p} &= \bigl( t p_1, \ldots t p_n, \alpha(1 - t) + tp_{n + 1} \bigr) \\
        &= \bigl( t p_1, \ldots t p_n, \alpha + t(p_{n + 1} - \alpha) \bigr).
\end{align*}
This intersects $\mathbb{R}^n$ when the last coordinate $\alpha + t(p_{n + 1} - \alpha) = 0$, so $t = \frac{1}{1 - \alpha p_{n + 1}}$ and the projection is therefore given by
\[
    \varphi_\alpha: \vec{p} \mapsto \left(\frac{p_1}{1 - \alpha p_{n + 1}}, \ldots, \frac{p_n}{1 - \alpha p_{n + 1}}\right).
\]
Each projection is a chart and the collection of these charts is an atlas, since the union of their domains covers $S^n$.

Denoting $\varphi_\alpha: \vec{p} \mapsto \vec{x}_\alpha = \left(x_\alpha^1, \ldots, x_\alpha^n\right)$, the $L\!^2$-norm
\begin{align*}
    r_\alpha^2 &= \sum_{i = 1}^n {\bigl( x_\alpha^i \bigr)}^2 \\
               &= \frac{p_1^2 + \cdots + p_n^2}{{(1 - \alpha p_{n + 1})}^2} \\
               &= \frac{1 - p_{n + 1}^2}{{(1 - \alpha p_{n + 1})}^2} \\
               &= \frac{(1 + p_{n + 1})(1 - p_{n + 1})}{{(1 - \alpha p_{n + 1})}^2} \\
               &= {\left(\frac{1 + p_{n + 1}}{1 - p_{n + 1}}\right)}^\alpha,
\end{align*}
so
\[
    p_{n + 1} = \alpha\frac{r_\alpha^2 - 1}{r_\alpha^2 + 1}.
\]
This gives us a general expression for the points $\vec{p} = (p_1, \ldots, p_n)$ on the manifold in terms of our chart's coordinate system as
\begin{align*}
    p_i &= x_\alpha^i (1 - \alpha p_{n + 1}) \\
        &= \frac{2 x_\alpha^i}{r_\alpha^2 + 1},
\end{align*}
so the inverse projections $\varphi_\alpha^{-1}: \mathbb{R}^n \to S^n$ are given by
\[
    \varphi_\alpha^{-1}: \vec{x} \mapsto \left(\frac{2 x^1}{r^2 + 1}, \ldots, \frac{2 x^n}{r^2 + 1}, \alpha \frac{r^2 - 1}{r^2 + 1} \right).
\]
For inverse map $\varphi_\beta^{-1}$, note that the point $p_{n + 1}$ is given by
\[
    p_{n + 1} = \beta \frac{r^2 - 1}{r^2 + 1}.
\]
From this, and assuming $\alpha$, $\beta$ are distinct so $\alpha \beta = -1$, we get that
\[
    \frac{1}{1 - \alpha p_{n + 1}} = \frac{r^2 + 1}{2 r^2}.
\]
The transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}: \mathbb{R}^n \to \mathbb{R}^n$ (with distinct $\alpha$, $\beta$) are then given by
\begin{align*}
    \varphi_\alpha \circ \varphi_\beta^{-1} (\vec{x}) &= \varphi_\alpha \left(\frac{2 x^1}{r^2 + 1}, \ldots, \frac{2 x^n}{r^2 + 1}, \beta \frac{r^2 - 1}{r^2 + 1} \right) \\
        &= \left(\frac{2 x^1}{r^2 + 1} \cdot \frac{r^2 + 1}{2 r^2},
                 \ldots,
                 \frac{2 x^n}{r^2 + 1} \cdot \frac{r^2 + 1}{2 r^2}
            \right) \\
        &= \frac{\vec{x}}{\norm{x}^2}.
\end{align*}
These transition functions are inversions on the $n$-sphere and are smooth where they are defined.

\end{sol}

\begin{ex}\label{ex:opensubsetismanifold}

Show that if $M$ is a manifold and $U$ is an open subset of $M$, then $U$ with its induced topology is a manifold.

\end{ex}

\begin{sol}

All subsets $U_\alpha \subset U$ are of the form $V \cap U$ where $V$ is open in $M$, so the open sets of the induced topology cover $U$.

We can construct an atlas by taking the charts on $M$, $\varphi_\alpha: V_\alpha \to \mathbb{R}^n$, and defining
\begin{align*}
    \SwapAboveDisplaySkip
    \varphi^U_\alpha&: U_\alpha \to \mathbb{R}^n, \\
                    &: u \mapsto \varphi_\alpha(u),
\end{align*}
i.e. $\varphi^U_\alpha = \varphi_\alpha$ for all $U_\alpha$.
Since $U_\alpha$ is open, we have well defined transition functions so $U$ with the induced topology is a manifold.

\end{sol}

\begin{ex}

Given topological spaces $X$ and $Y$, we give $X \times Y$ the \emph{product topology} in which a set is open if and only if it is a union of sets of the form $U \times V$, where $U$ is open in $X$ and $V$ is open in $Y$. Show that if $M$ is an $m$-dimensional manifold and $N$ is an $n$-dimensional manifold, $M \times N$ is an $(m + n)$-dimensional manifold.

\end{ex}

\begin{sol}\label{sol:producttopology}

For every point $(u, v) \in M \times N$, there exists a set $U \times V$ where $U$ is open in $M$ and $V$ is open in $N$ such that $u \in U$, $v \in V$.
Therefore $U \times V$ is an open set under the product topology and $M \times N$ is a topological space.

Given $M$, $N$ are manifolds, they have atlases
\[
    \left\{\varphi^M_\alpha: U_\alpha \to \mathbb{R}^m \right\}, \qquad
    \left\{\varphi^N_\beta: V_\beta \to \mathbb{R}^n \right\}
\]
for all $U_\alpha$ open in $M$, $V_\beta$ open in $N$.

For some $u \in U_\alpha$, $v \in V_\beta$, denote
\[
    \varphi^M_\alpha: u \mapsto \vec{x} = (x_1, \ldots, x_m), \quad
    \varphi^N_\beta: v \mapsto \vec{y} = (y_1, \ldots, y_n).
\]

We can construct maps $\tilde{\varphi}_{\alpha\beta}:\ U_\alpha \times V_\beta \to \mathbb{R}^m \times \mathbb{R}^n$ as
\begin{align*}
    \tilde{\varphi}_{\alpha\beta} (u, v) &= \left(\varphi^M_\alpha(u), \varphi^N_\beta(v)\right) \\
        &= (\vec{x}, \vec{y}).
\end{align*}
This is obviously invertible via
\[
    \tilde{\varphi}_{\alpha\beta}^{-1}(\vec{x}, \vec{y}) = \left({(\varphi^M_\alpha)}^{-1}(\vec{x}), {(\varphi^N_\beta)}^{-1}(\vec{y})\right) = (u, v)
\]
because the inverse charts are guaranteed to exist.

The product space $\mathbb{R}^m \times \mathbb{R}^n$ is homeomorphic to $\mathbb{R}^{m + n}$ under
\[
    h(\vec{x}, \vec{y}) = (x_1, \ldots, x_m, y_1, \ldots, y_n),
\]
so we can construct new smooth maps $\varphi_{\alpha\beta} = h \circ \tilde{\varphi}_{\alpha\beta}$ that target $\mathbb{R}^{m + n}$.
The transition functions
\[
    \varphi_{\alpha\beta} \circ \varphi_{\alpha\beta}^{-1}: \mathbb{R}^{m + n} \to \mathbb{R}^{m + n}
\]
are similarly obviously smooth where defined, so $\varphi_{\alpha\beta}$ is a chart and the collection of these charts for all $U_\alpha$, $V_\beta$ is an atlas, therefore $M \times N$ is a manifold.

\end{sol}

\begin{ex}

Given topological spaces $X$ and $Y$, we give $X \cup Y$ the \emph{disjoint union topology} in which a set is open if and only if it is the union of an open subset of $X$ and an open subset of $Y$.
Show that if $M$ and $N$ are $n$-dimensional manifolds the disjoint union $M \cup N$ is an $n$-dimensional manifold.

\end{ex}

\begin{sol}

Any point $p \in M \cup N$ is either in $M$ or $N$. Consider a neighbourhood $X$ of $p$. This will be of the form $U \cup V$ for $U$, $V$ open subsets of $M$, $N$ since $p \in X$ is equivalent to $p \in X \cup \varnothing$.

Given $M$, $N$ are manifolds, they have atlases
\[
    \left\{\varphi^M_\alpha: U_\alpha \to \mathbb{R}^n \right\}, \qquad
    \left\{\varphi^N_\beta: V_\beta \to \mathbb{R}^n \right\}
\]
for all $U_\alpha$ open in $M$, $V_\beta$ open in $N$. Therefore any neighbourhood of $p \in M \cup N$ has a chart, for all $p$.

Since the transition functions exist independently, they are automatically smooth. Therefore $M \cup N$ is an $n$-dimensional manifold.

\end{sol}

\chapter{Vector Fields}\label{ch:vectorfields}

\begin{epigraph}
    Ignorant men have long been in advance of the learned about vectors.
\end{epigraph}

\begin{ex}

Show that $v + w$ and $gw \in \Vect(M)$.

\end{ex}

\begin{sol}

For the sum,
\begin{align*}
    (v + w)(f + g) &= v(f + g) + w(f + g) \\
                   &= v(f) + v(g) + w(f) + w(g) \\
                   &= (v + w)(f) + (v + w)(g),
\end{align*}
\begin{align*}
    (v + w)(\alpha f) &= v(\alpha f) + w(\alpha f) \\
                      &= \alpha v(f) + \alpha w(f) \\
                      &= \alpha \bigl(v(f) + w(f)\bigr) \\
                      &= \alpha (v + w)(f),
\end{align*}
\begin{align*}
    (v + w)(fg) &= v(fg) + w(fg) \\
                &= v(f)g + fv(g) + w(f)g + fw(g) \\
                &= \bigl(v(f) + w(f)\bigr)g + f \cdot \bigl(v(g) + w(g)\bigr) \\
                &= (v + w)(f)g + f \cdot (v + w)(g).
\end{align*}

For the product,
\begin{align*}
    gw(f + h) &= g \cdot \bigl(w(f) + w(h)\bigr) \\
              &= gw(f) + gw(h),
\end{align*}
\begin{align*}
    gw(\alpha f) &= g \cdot \alpha w(f) \\
        &= \alpha g w(f),
\end{align*}
\begin{align*}
    gw(fh) &= g \cdot \bigl(w(f)h + f w(h)\bigr) \\
           &= g w(f) h + g f w(h) \\
           &= gw(f) h + fgw(h).
\end{align*}

\end{sol}

\begin{ex}\label{ex:module}

Show that the following rules [hold] for all $v, w \in \Vect(M)$ and $f, g \in C^\infty(M)$:
\begin{align*}
    f(v + w) &= fv + fw, \\
    (f + g)v &= fv + gv, \\
    (fg)v &= f(gv), \\
    1v &= v.
\end{align*}
(Here ``$1$'' denotes the constant function equal to $1$ on all of $M$.) Mathematically, we summarize these rules by saying that $\Vect(M)$ is a \emph{module over} $C^\infty(M)$.

\end{ex}

\begin{sol}\label{sol:module}

For all $g \in C^\infty(M)$,
\[
    f(v + w)g = fv(g) + fw(g) = (fv + fw)(g),
\]
so $f(v + w) = fv + fw$.

For all $h \in C^\infty(M)$,
\[
    (f + g)v(h) = fv(h) + gv(h) = (fv + gv)(h),
\]
so $(f + g)v = fv + gv$.


For all $h \in C^\infty(M)$,
\[
    (fg)v(h) = f \cdot gv(h) = f(gv)(h)
\]
so $(fg)v = f(gv)$.

For all $f \in C^\infty(M)$,
\[
    (1v)(f) = 1v(f) = v(f).
\]

Therefore $\Vect(M)$ is a module over $C^\infty(M)$.

\end{sol}

\begin{ex}

Show that if $v^\mu\partial_\mu = 0$, that is, $v^\mu \partial_\mu f = 0$ for all $f \in C^\infty(\mathbb{R}^n)$, we must have $v^\mu = 0$ for all $\mu$.

\end{ex}

\begin{sol}\label{sol:basis}

Choose a function $f: \vec{x} \mapsto x^\nu$ for some index $0 < \nu \leq n$. Then
\[
    v^\mu \partial_\mu x^\nu = v^\mu \delta_\mu^\nu = v^\nu.
\]
If $v^\mu\partial_\mu = 0$, we get $v^\mu$ = 0 from above.

\end{sol}

\section{Tangent Vectors}

\begin{ex}

Let $v, w \in \Vect(M)$. Show that $v = w$ if and only if $v_p = w_p$ for all $p \in M$.

\end{ex}

\begin{sol}\label{sol:tangentvectorpointequality}

If $v = w$, then
\[
    v_p(f) = v(f)(p) = w(f)(p) = w_p(f)
\]
so $v_p = w_p$.

The other way around, if $v_p(f) = w_p(f)$ then $v(f)(p) = w(f)(p)$, which must be true for all $p \in M$, so $v(f) = w(f)$ and therefore $v = w$.

\end{sol}

\begin{ex}

Show that $T_p M$ is a vector space over the real numbers.

\end{ex}

\begin{sol}

We must show that tangent vectors $v_p \in T_p M$ satisfy the axioms of vector spaces.

Let $u, v, w \in T_p M$ and $\alpha, \beta \in \mathbb{R}$.

To check associativity,
\begin{align*}
    (u + (v + w))(f) &= u(f) + (v + w)(f) \\
                     &= u(f) + v(f) + w(f) \\
                     &= \bigl(u(f) + v(f)\bigr) + w(f) \\
                     &= (u + v)(f) + w(f),
\end{align*}
so $u + (v + w) = (u + v) + w$.

Commutativity holds since $\mathbb{R}$ is commutative.

An additive identity vector $0$ exists since
\[
    (v + 0)(f) = v(f) + 0(f) = v(f)
\]
by defining $0$ to be the tangent vector that maps all functions to $0$.

We can construct for every tangent vector $v$ an additive inverse $-v$ as $(-v)(f) = -v(f)$.

We have compatibility of scalar and field multiplication since
\[
    \alpha(\beta v)(f) = \alpha \bigl(\beta v(f)\bigr)
        = \alpha \beta v(f) = (\alpha \beta) v(f).
\]

The existence of a scalar multiplicative identity follows from solution~\ref{sol:module}.

For distributivity,
\[
    \alpha(u + v)(f) = \alpha \bigl(u(f) + v(f)\bigr) = \alpha u(f) + \alpha v(f)
\]
and
\[
    (\alpha + \beta) v(f) = \alpha v(f) + \beta v(f).
\]

\end{sol}

\begin{ex}\label{ex:tangentvector}

Check that $\gamma'(t) \in T_{\gamma(t)}M$ using the definitions.

\end{ex}

\begin{sol}

We have that
\[
    \gamma'(t): f \mapsto \frac{d}{dt} f\bigl( \gamma(t) \bigr).
\]
Notice that
\begin{align*}
    \SwapAboveDisplaySkip
    &\gamma'(t)(f + g) = \gamma'(t)(f) + \gamma'(t)(g), \\
    &\gamma'(t)(\alpha f) = \alpha \gamma'(t)(f), \\
    &\gamma'(t)(f g) = \gamma'(t)(f) g + f \gamma'(t)(g),
\end{align*}
so $\gamma'(t)$ is a tangent vector.

\end{sol}

\section{Covariant Versus Contravariant}

\begin{ex}

Let $\phi: \mathbb{R} \to \mathbb{R}$ be given by $\phi(t) = e^t$.
Let $x$ be the usual coordinate function on $\mathbb{R}$.
Show that $\phi^* x = e^x$.

\end{ex}

\begin{sol}\label{sol:pullbackexponential}

The pullback $\phi^*: C^\infty(N) \to C^\infty(M) $ of $f: N \to \mathbb{R}$ by $\phi: M \to N$ is defined as
\begin{nowidthtags}
\[
    \phi^*f = f \circ \phi. \tag{pullback of a function}\label{eq:pullbackfunction}
\]
\end{nowidthtags}

Consider a chart $\varphi: M \to \mathbb{R}^n$ mapping $p \in M$ to $\varphi(p) = \left\{x^\mu(p)\right\}$.
Note that each $x^\mu$ is a function taking $p$ to the $\mu$\textsuperscript{th} coordinate of its image in $\mathbb{R}^n$.

Since our manifold is $\mathbb{R}$, the ``usual coordinate function'' in this case is the identity (under trivial coordinate transformation $t \to x$, say), so
\[
    (\phi^* x)(t) = x \bigl( \phi(t) \bigr) = x(e^t) = e^x
\]
(where we abuse notation and identify the coordinate transformation function and its target as $x$).

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ be rotation counterclockwise by an angle $\theta$. Let $x$, $y$ be the usual coordinate functions on $\mathbb{R}^2$. Show that
\begin{align*}
    \phi^* x &= \cos(\theta) x - \sin(\theta) y, \\
    \phi^* y &= \sin(\theta) x + \cos(\theta) y.
\end{align*}

\end{ex}

\begin{sol}\label{sol:pullbackrotation}

If $\phi$ is a positive rotation by a (fixed) angle $\theta$, we can express it as
\[
    \phi: \begin{pmatrix}
            u \\ v
        \end{pmatrix}
        \mapsto
        \begin{pmatrix*}[r]
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix*}
        \begin{pmatrix}
            u \\ v
        \end{pmatrix}
        =
        \begin{pmatrix}
            \cos(\theta) u - \sin(\theta) v \\
            \sin(\theta) u + \cos(\theta) v
        \end{pmatrix}.
\]
As before, consider the chart $\varphi(p) = \left\{x^\mu(p)\right\} = \left\{x(p), y(p)\right\}$.
Then $\phi^* x(p) = x(\phi(p))$ is the $x$-coordinate, so for $p = (u, v)$,
\begin{align*}
    \phi^* x(p) &= x \bigl( \phi(p) \bigr) \\
                &= \cos(\theta) u - \sin(\theta) v \\
                &= \cos(\theta) x(p) - \sin(\theta) y(p)
\end{align*}
and similarly for $\phi^* y$.

\end{sol}

\begin{ex}

Show that this definition of smoothness is consistent with the previous definitions of smooth functions $f: M \to \mathbb{R}$ and smooth curves $\gamma: \mathbb{R} \to M$.

\end{ex}

\begin{sol}

Recall the definition of smooth functions between manifolds.

\begin{quote}
    $\phi: M \to N$ is smooth if $f \in C^\infty(N)$ implies that $\phi^* f \in C^\infty(M)$.
\end{quote}

Our other two definitions of smoothness are:
\begin{itemize}
    \item a function $f: M \to \mathbb{R}$ is smooth if for all $\alpha$, $f \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$ is smooth,
    \item a curve $\gamma: \mathbb{R} \to M$ is smooth if $f \bigl( \gamma(t) \bigr)$ depends smoothly on $t$ for any $f \in C^\infty(M)$.
\end{itemize}

If $N = \mathbb{R}$, our definition of smooth functions between manifolds is that $\phi: M \to \mathbb{R}$ is smooth if $f \in C^\infty(\mathbb{R})$ implies that $\phi^* f \in C^\infty(M)$.
But if we assume $f \in C^\infty(\mathbb{R})$ then $\phi^* f = f \circ \phi \in C^\infty(M)$ requires that $\phi \in C^\infty(M)$ and $\phi: M \to \mathbb{R}$ is smooth if for all $\alpha$, $\phi \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$ is smooth.

Let $\phi: M \to \mathbb{R}$ be a smooth function (i.e.\ for all $\alpha$, $\phi \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$).
Let $f \in C^\infty(\mathbb{R})$.
Then $f \circ \phi \circ \varphi_\alpha^{-1}$ is smooth since it is the composition of smooth functions, so $f \circ \phi = \phi^* f$ is smooth.

If the domain is $\mathbb{R}$, our definition of smooth functions between manifolds is that $\gamma: \mathbb{R} \to M$ is smooth if $f \in C^\infty(M)$ implies that $\gamma^* f \in C^\infty(\mathbb{R})$.
But if we assume $f \in C^\infty(M)$ then $\gamma^* f = f \circ \gamma \in C^\infty(\mathbb{R})$ is smooth by the definition of smooth curves.

Let $\gamma: \mathbb{R} \to M$ be smooth, i.e. $f\circ \gamma$ is smooth for all $f \in C^\infty(M)$.
Since $\gamma^* f = f \circ \gamma$, $\gamma^* f$ is smooth too.

\end{sol}

\begin{ex}

Prove that $(\phi \circ \gamma)'(t) = \phi_* \bigl( \gamma'(t) \bigr)$.

\end{ex}

\begin{sol}

The pushforward $\phi_*: T_p M \to T_{\phi(p)} N$ of $v \in T_p M$ by $\phi: M \to N$ is given by
\begin{nowidthtags}
\[
    (\phi_* v) (f) = v (\phi^* f). \tag{pushforward of a vector}\label{eq:pushforwardvector}
\]
\end{nowidthtags}
Then
\begin{align*}
    \SwapAboveDisplaySkip
    (\phi \circ \gamma)'(t)(f) &= \frac{d}{dt} f \bigl( (\phi \circ \gamma)(t) \bigr) \\
        &= \frac{d}{dt} (f \circ \phi \circ \gamma)(t) \\
        &= \frac{d}{dt} (f \circ \phi) \bigl( \gamma(t) \bigr) \\
        &= \gamma'(t)(f \circ \phi) \\
        &= \gamma'(t)(\phi^* f) \\
        &= (\phi_* (\gamma'(t)))(f).
\end{align*}

\end{sol}

\begin{ex}

Show that the pushforward operation
\[
    \phi_*: T_p M \to T_{\phi(p)} N
\]
is linear.

\end{ex}

\begin{sol}

Let $v, w \in T_p M$, $\alpha, \beta \in \mathbb{R}$, $f \in C^\infty(N)$. $\phi_*$ is linear since
\begin{align*}
    \bigl( \phi_*(\alpha v + \beta w) \bigr)(f) &= (\alpha v + \beta w) (\phi^* f) \\
        &= \alpha v (\phi^* f) + \beta w (\phi^* f) \\
        &= \alpha (\phi_* v)(f) + \beta (\phi_* w)(f) \\
        &= \bigl( \alpha (\phi_* v) + \beta (\phi_* w) \bigr)(f).
\end{align*}

\end{sol}

\begin{ex}

Show that if $\phi: M \to N$ is a diffeomorphism, we can push forward a vector field $v$ on $M$ to obtain a vector field $\phi_*v$ on $N$ satisfying
\[
    {(\phi_* v)}_q = \phi_*(v_p)
\]
whenever $\phi(p) = q$.

\end{ex}

\begin{sol}\label{sol:pushforwardvectorfield}

Note that the definition of the pushforward is sloppy, since the left side must be evaluated on $N$ while the right side is evaluated on $M$.

Looking at the action of $\phi_* v$ on a function $f \in C^\infty (N)$ and denoting the points that each side act on as $p \in M$, $q \in N$,
\begin{align*}
    {(\phi_* v)}_q (f) &= (\phi_* v) (f) (q) \\
                       &= v(\phi^* f)(p) \\
                       &= v_p(\phi^* f) \\
                       &= (\phi_* v_p)(f).
\end{align*}
But
\begin{align*}
    \SwapAboveDisplaySkip
    v_p (\phi^* f) &= v_p (f \circ \phi) \\
                   &= v(f(\phi(p))) \\
                   &= w_{\phi(p)}(f)
\end{align*}
for some $w \in \Vect(N)$.

It's tempting to write this as $v_{\phi(p)}(f)$, but $v \in \Vect(M)$ whereas $\phi(p) \in N$.
Instead we need exactly the pushforward of $v$, so we get $w_{\phi(p)} = {(\phi_* v)}_{\phi(p)}$ and the equality holds when $\phi(p) = q$.

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ be [a] rotation counterclockwise by an angle $\theta$. Let $\partial_x$, $\partial_y$ be the coordinate vector fields on $\mathbb{R}^2$. Show that at any point of $\mathbb{R}^2$,
\begin{align*}
    \SwapAboveDisplaySkip
    \phi_* \partial_x &= \cos(\theta) \partial_x + \sin(\theta) \partial_y, \\
    \phi_* \partial_y &= -\sin(\theta) \partial_x + \cos(\theta) \partial_y.
\end{align*}

\end{ex}

\begin{sol}\label{sol:pushforwardrotation}

Denote $\phi: (x, y) \mapsto \bigl( u(x, y), v(x, y) \bigr)$ where $u, v$ are functions as per solution~\ref{sol:pullbackrotation} and let $f \in C^\infty(\mathbb{R}^2)$.

For a vector $\partial_i$, the pushforward acting on $f$ is
\begin{align*}
    (\phi_* \partial_i)(f) &= \partial_i (\phi^* f) \\
        &= \partial_i (f \circ \phi) \\
        &= \partial_u f \cdot \partial_i u + \partial_v f \cdot \partial_i v
\end{align*}
and at a point $p = (x, y) \in \mathbb{R}^2$,
\[
     {(\phi_* \partial_i)}_p(f) = \partial_i u \cdot \partial_u f(u, v) + \partial_i v \cdot \partial_v f(u, v).
\]
We want to consider $f$ at $p$ rather than at $\phi(p)$, so change variables as $\partial_u f(u, v) = \partial_x f(x, y)$, $\partial_v f(u, v) = \partial_y f(x, y)$.

Consider $\phi_* \partial_x$ and $\phi_* \partial_y$,
\begin{align*}
    {(\phi_* \partial_x)}_p(f) &= \partial_x u \cdot \partial_x f(x, y) + \partial_x v \cdot \partial_y f(x, y) \\
        &= \cos(\theta) \partial_x f(x, y) + \sin(\theta) \partial_y f(x, y), \\[2\jot]
    {(\phi_* \partial_y)}_p(f) &= \partial_x u \cdot \partial_x f(x, y) + \partial_y v \cdot \partial_y f(x, y) \\
        &= -\sin(\theta) \partial_x f(x, y) + \cos(\theta) \partial_y f(x, y),
\end{align*}
giving us
\begin{align*}
    \SwapAboveDisplaySkip
    \phi_* \partial_x &= \cos(\theta) \partial_x + \sin(\theta) \partial_y, \\
    \phi_* \partial_y &= -\sin(\theta) \partial_x + \cos(\theta) \partial_y.
\end{align*}
We can see that this is consistent by taking the result from solution~\ref{sol:pullbackrotation},
\begin{align*}
    (\phi_* \partial_x) x &= \partial_x(\phi^* x) = \cos(\theta), &
    (\phi_* \partial_x) y &= \partial_x(\phi^* y) = \sin(\theta), \\
    (\phi_* \partial_y) x &= \partial_y(\phi^* x) = -\sin(\theta), &
    (\phi_* \partial_y) y &= \partial_y(\phi^* y) = \cos(\theta),
\end{align*}
where we get back the $x$- and $y$-components of $\phi_* \partial_x$, $\phi_* \partial_y$, respectively.

\end{sol}

\section{Flows and the Lie Bracket}\label{sec:flowsandtheliebracket}

\begin{ex}

Let $v$ be the vector field $x^2 \partial_x + y \partial_y$ on $\mathbb{R}^2$. Calculate the integral curves $\gamma(t)$ and see which ones are defined for all $t$.

\end{ex}

\begin{sol}

Integral curves satisfy $\gamma'(t) = v_{\gamma(t)}$, $\gamma(0) = p$.

Denote $\gamma(t) = (x(t), y(t)) \in \mathbb{R}^2$. Then from the definition of tangent curves,
\begin{align*}
    \frac{d}{dt} f(\gamma(t)) &= \frac{d}{dt} f(x, y) \\
        &= \partial_x f(x, y) \dot{x} + \partial_y f(x, y) \dot{y} \\
        &\overset{!}{=} x^2 \partial_x f(x, y) + y \partial_y f(x, y)
\end{align*}
giving us differential equations $\dot{x}(t) = {x(t)}^2$, $\dot{y}(t) = y(t)$ with solutions
\[
    x(t) = \frac{1}{\alpha - t}, \qquad
    y(t) = \beta e^t.
\]
Fix the constants $\alpha$, $\beta$ with initial condition $\gamma(0) = p = \bigl( x(0), y(0) \bigr)$. Then
\[
    x(t) = \frac{x(0)}{1 - x(0)t}, \qquad
    y(t) = y(0) e^t.
\]
When $x(0) = 0$ we get $x(t) = 0$ for all $t$. Otherwise, we get a singularity at $t = \frac{1}{x(0)}$, so the integral curves $\gamma$ are defined for all $t$ when starting at $p = (0, b)$ for any $b \in \mathbb{R}$.

\end{sol}

\begin{ex}

Show that $\phi_0$ is the identity map $\id: X \to X$ and that for all $s, t \in \mathbb{R}$ we have $\phi_t \circ \phi_s = \phi_{t + s}$.

\end{ex}

\begin{sol}

By definition, the flow $\phi_t(p)$ is defined to be the point on the integral curve a parameter distance $t$ from $p$, therefore at $t = 0$, $\phi_0(p) = p$.

Pick some value $t = t_0$ and label the point $\phi_{t_0}(p) = q$. Let $t_1 = t_0 + s$, so $\phi_{t_1}(p) = \phi_{t_0 + s}(p)$. But this is a parameter distance $s$ from $q$, so $\phi_{t_1}(p) = \phi_s(q)$ and thus
\[
    \phi_{t_0 + s}(p) = \phi_s \circ \phi_{t_0}(p).
\]
It follows from this that $\phi_s^{-1} = \phi_{-s}$, so the flow is an Abelian group.

\end{sol}

\begin{ex}\label{ex:liebracketpolarcoordinates}

Consider the normalised vector fields in the $r$ and $\theta$ directions on the plane in polar coordinates (not defined at the origin):
\[
    v = \frac{x \partial_x + y \partial_y}{\sqrt{x^2 + y^2}}, \qquad
    w = \frac{x \partial_y - y \partial_x}{\sqrt{x^2 + y^2}}.
\]
Calculate $[v, w]$.

\end{ex}

\begin{sol}

Since $x = r \cos(\theta)$, $y = r \sin(\theta)$, we have for some $f \in C^\infty(\mathbb{R}^2)$,
\begin{align*}
    \partial_r f &=\cos(\theta) \partial_x f + \sin(\theta) \partial_y f, \\
    \partial_\theta f &= -r \sin(\theta) \partial_x f + r \cos(\theta) \partial_y f
\end{align*}
so $v = \partial_r$, $w = \frac{\partial_\theta}{r}$.
Then
\allowdisplaybreaks[3]
\begin{align*}
    [v, w]f &= v \bigl( w(f)) - w(v(f) \bigr) \\
        &= v\left(\frac{\partial_\theta f}{r}\right) - w(\partial_r f) \\
        &= \partial_r\left(\frac{\partial_\theta f}{r}\right)
            - \frac{\partial_\theta}{r}(\partial_r f) \\
        &= \frac{r \partial_r \partial_\theta f - \partial_\theta f}{r^2}
            - \frac{\partial_\theta \partial_r f}{r} \\
        &= \frac{1}{r} \left(\partial_r \partial_\theta f - \frac{\partial_\theta f}{r}
            - \partial_\theta \partial_r f\right) \\
        &= -\frac{\partial_\theta f}{r^2} \\
        &= -\frac{w}{r} f
\end{align*}
so $[v, w] = -\frac{w}{r}$.
\allowdisplaybreaks[2]

We could also do this the hard way,
\begin{align*}
    [v, w]f &= v \bigl( w(f)) - w(v(f) \bigr) \\
        &= \frac{(x \partial_x + y \partial_y)(x \partial_y f - y \partial_x f) - (x \partial_y - y \partial_x)(x \partial_x f + y \partial_y f)}{x^2 + y^2} \\
        &= \frac{\splitdfrac{x \partial_x (x\partial_y f)
                             - x\partial_x(y \partial_x f)
                             + y \partial_y(x\partial_y f)
                             - y \partial_y(y \partial_x f)}
                            {{}-x\partial_y(x\partial_x f)
                             - x\partial_y(y\partial_y f)
                             + y\partial_x(x \partial_x f)
                             + y\partial_x (y \partial_y f)}
                }{x^2 + y^2} \\
        &= \frac{y \partial_x f - x \partial_y f}{x^2 + y^2}
\end{align*}
giving the same result
\[
    [v, w] = \frac{y \partial_x - x \partial_y}{x^2 + y^2} = -\frac{w}{r}.
\]

\end{sol}

\begin{ex}\label{ex:liebracketflows}

Check the equation above.

\end{ex}

\begin{sol}

We need to check that for any $f \in C^\infty(M)$,
\[
    [v, w](f)(p) = \frac{\partial^2}{\partial t\, \partial s} \Bigl( f(\psi_s(\phi_t(p))) - f(\phi_t(\psi_s(p))) \Bigr) \Big|_{s = t = 0}
\]
where $\phi_t$, $\psi_s$ are flows generated by $v$ and $w$, respectively.

We have that
\[
    (vf)(p) = \frac{d}{dt} f \bigl( \phi_t(p) \bigr) \Big|_{t=0}, \qquad
    (wf)(p) = \frac{d}{ds} f \bigl( \psi_s(p) \bigr) \Big|_{s=0},
\]
so
\begin{align*}
    \SwapAboveDisplaySkip
    (vw)(f)(p) &= \frac{d}{dt} wf \bigl( \phi_t(p) \bigr) \Big|_{t = 0} \\
        &= \frac{\partial^2}{\partial t \, \partial s} f(\psi_s(\phi_t(p))) \Big|_{s = t = 0}
\intertext{and similarly}
    (wv)(f)(p) &= \frac{d}{ds} vf \bigl( \psi_s(p) \bigr) \Big|_{s = 0} \\
        &= \frac{\partial^2}{\partial s \, \partial t} f(\phi_t(\psi_s(p))) \Big|_{t = s = 0}.
\end{align*}
The result follows immediately.

\end{sol}

\begin{ex}

Show that for all vector fields $u$, $v$, $w$ on a manifold, and all real numbers $\alpha$, $\beta$, we have:
\begin{enumerate}
    \item $[v, w] = - [w, v]$,
    \item $[u, \alpha v + \beta w] = \alpha[u, v] + \beta [u, w]$,
    \item the \emph{Jacobi identity}: $\big[u, [v, w]\big] + \big[v, [w, u]\big] + \big[w, [u, v]\big] = 0$.
\end{enumerate}

\end{ex}

\begin{sol}\label{sol:liebracketvectorfields}
\mbox{}

\begin{enumerate}

\item The Lie bracket is antisymmetric.
    \[
        [v, w] = vw - wv = -(wv - vw) = -[w, v].
    \]

\item The Lie bracket is linear.
    \begin{align*}
        [u, \alpha v + \beta w] &= u (\alpha v + \beta w) - (\alpha v + \beta w) u \\
            &= \alpha uv + \beta uw - \alpha vu - \beta wu \\
            &= \alpha(uv - vu) + \beta(uw - wu) \\
            &= \alpha [u, v] + \beta [u, w].
    \end{align*}

\item The Lie bracket satisfies the Jacobi identity.
    \begin{align*}
        \big[u, [v, w]\big] &= u[v, w] - [v, w]u \\
            &= u(vw - wv) - (vw - wv)u \\
            &= uvw - uwv - vwu + wvu,
    \end{align*}
    so similarly,
    \begin{align*}
        \big[v, [w, u]\big] &= vwu - vuw - wuv + uwv, \\
        \big[w, [u, v]\big] &= wuv - wvu - uvw + vuw.
    \end{align*}
    Combining everything, we get
    \begin{align*}
        \big[u, [v, w]\big] + \big[v, [w, u]\big] + \big[w, [u, v]\big] =&\ uvw - uwv - vwu + wvu \\
            &+ vwu - vuw - wuv + uwv \\
            &+ wuv - wvu - uvw + vuw \\
            =&\ 0.
    \end{align*}
\end{enumerate}

\end{sol}

\chapter{Differential Forms}

\begin{epigraph}
    As a herald it's my duty \\
    \mbox{}\hspace{1em} to explain those forms of beauty.
\end{epigraph}

\section{1-forms}

\begin{ex}

Show that $\omega + \mu$ and $f\omega$ are really 1-forms, i.e., show linearity over $C^\infty(M)$.

\end{ex}

\begin{sol}

Let $g, h \in C^\infty(M)$, $v, w \in \Vect(M)$.

$\omega + \mu$ is linear over $C^\infty(M)$ since
\begin{align*}
    (\omega + \mu)(gv + hw) &= (\omega + \mu)(gv) + (\omega + \mu)(hw) \\
        &= \omega(gv) + \mu(gv) + \omega(hw) + \mu(hw) \\
        &= g\omega(v) + g\mu(v) + h\omega(w) + h\mu(w) \\
        &= g(\omega + \mu)(v)  + h(\omega + \mu)(w)
\end{align*}
and $f\omega$ is linear over $C^\infty(M)$ since
\begin{align*}
    (f\omega)(gv + hw) &= f\omega(gv + hw) \\
                       &= fg\omega(v) + fh\omega(w) \\
                       &= gf\omega(v) + hf\omega(w) \\
                       &= g(f\omega)(v) + h(f\omega)(w).
\end{align*}

\end{sol}

\begin{ex}

Show that $\Omega^1(M)$ is a module over $C^\infty(M)$ (see the definition in exercise~\ref{ex:module}).

\end{ex}

\begin{sol}

Let $\omega, \mu \in \Omega^1(M)$, $v\in \Vect(M)$.

For all $f \in C^\infty(M)$,
\[
    f(\omega + \mu)(v) = f(\omega v + \mu v) = f\omega v + f\mu v
\]
so $f(\omega + \mu) = f\omega + f\mu$.

For all $f, g \in C^\infty(M)$,
\[
    (f + g)\omega(v) = f\omega(v) + g\omega(v)
\]
so $(f + g)\omega = f\omega + g\omega$.

For all $f, g \in C^\infty(M)$,
\[
    (fg)\omega(v) = f(g\omega)(v) = (fg\omega)(v)
\]
so $(fg)\omega = fg\omega$.

Let $1$ be the constant function equal to $1$ on all of $M$. Then
\[
    (1\omega)(v) = 1\omega(v) = \omega(v).
\]

Therefore $\Omega^1(M)$ is a module over $C^\infty(M)$.

\end{sol}

\begin{ex}

Show that
\begin{align*}
    & d(f + g) = df + dg, \\
    & d(\alpha f) = \alpha\, df, \\
    & (f + g) \, dh = f\, dh + g\, dh, \\
    & d(fg) = f \, dg + g\, df
\end{align*}
for any $f, g, h \in C^\infty(M)$ and any $\alpha \in \mathbb{R}$.

\end{ex}

\begin{sol}

Let $v \in \Vect(M)$. First consider linearity.
\begin{align*}
    d(f + g)v &= v(f + g) \\
              &= vf + vg \\
              &= df(v) + dg(v) \\
              &= (df + dg)(v),
\end{align*}
\[
    d(\alpha f)(v) = v(\alpha f) = \alpha v(f) = \alpha \, df(v),
\]
\begin{align*}
    (f + g) \, dh(v) &= (f + g)v(h) \\
                     &= fv(h) + gv(h) \\
                     &= f\, dh(v) + g \, dh(v).
\end{align*}
The Leibniz law holds since
\begin{align*}
    d(fg)(v) &= v(fg) \\
             &= f v(g) + g v(f) \\
             &= f \, dg(v) + g \, df(v).
\end{align*}

\end{sol}

\begin{ex}

Suppose $f(x^1, \ldots, x^n)$ is a function on $\mathbb{R}^n$. Show that
\[
    df = \partial_\mu f \, dx^\mu.
\]

\end{ex}

\begin{sol}\label{sol:gradient}

Recall from solution~\ref{sol:basis} that $\left\{\partial_\mu\right\}$ forms a basis for $\mathbb{R}^n$, so $v = v^\mu \partial_\mu$ for some components $\left\{v^\mu\right\}$, $v \in \Vect(\mathbb{R}^n)$. Consider some test vector $v$,
\[
    df(v) = v(f) = v^\mu \partial_\mu f.
\]
On the other hand,
\begin{align*}
    \partial_\mu f \, dx^\mu (v) &= \partial_\mu f v(x^\mu) \\
        &= v^\nu \partial_\mu f \partial_\nu x^\mu \\
        &= v^\nu \partial_\mu f \delta_\nu^\mu \\
        &= v^\mu \partial_\mu f,
\end{align*}
giving $df(v) = \partial_\mu f\, dx^\mu(v)$ and therefore $df = \partial_\mu f\, dx^\mu$.

\end{sol}

\begin{ex}

Show that the 1-forms $\left\{dx^\mu\right\}$ are linearly independent, i.e., if
\[
    \omega = \omega_\mu dx^\mu = 0
\]
then all the functions $\omega_\mu$ are zero.

\end{ex}

\begin{sol}

As in solution~\ref{sol:gradient}, consider some vector field $v$.
\begin{align*}
    \omega (v) &= \omega_\mu dx^\mu (v) \\
               &= \omega_\mu v(x^\mu) \\
               &= v^\nu \omega_\mu \delta_\nu^\mu \\
               &= v^\mu \omega_\mu
\end{align*}
so $\omega(v) = 0$ implies $v^\mu \omega_\mu = 0$. But since $v$ is arbitrary, $\omega_\mu = 0$ for all $\mu$.

\end{sol}

\section{Cotangent Vectors}\label{sec:cotangentvectors}

\begin{ex}

For the mathematically inclined: show that the $\omega_p$ is really well-defined by the formula above.
That is, show that $\omega(v)(p)$ really depends only on $v_p$, not on the values of $v$ at other points.
Also, show that a 1-form is determined by its values at points.
In other words, if $\omega, \nu$ are two 1-forms on $M$ with $\omega_p = \nu_p$ for every point $p \in M$, then $\omega = \nu$.

\end{ex}

\begin{sol}\label{sol:welldefined1forms}

Let $u, w \in \Vect(M)$ with $u \neq w$. Let $u_p = w_p$, with $u_q \neq w_q$ necessarily, $q \in M, q \neq p$.
Consider the vector field $v = u - w$. Then to show that $\omega_p$ is well-defined, it is sufficient to show that for any $\omega = df$,
\begin{align*}
    \omega_p(v_p) &= \omega(v)(p) \\
                  &= df(v)(p) \\
                  &= v(f)(p) \\
                  &= (u - w)(f)(p) \\
                  &= u(f)(p) - w(f)(p) \\
                  &= u_p(f) - w_p(f) \\
                  &= (u_p - w_p)(f) \\
                  &= 0.
\end{align*}
Just as in solution~\ref{sol:tangentvectorpointequality}, if $\omega_p = \nu_p$ for every point $p \in M$ then $\omega_p(v_p) = \nu_p(v_p)$ for some $v_p \in T_p M$. But
\begin{align*}
\omega (v)(p) &= \omega_p(v_p) \\
              &= \nu_p(v_p) \\
              &= \nu (v)(p)
\end{align*}
for all $p \in M$ and therefore, since $v$ is arbitrary, $\omega = \nu$.

\end{sol}

\begin{ex}\label{ex:dualidentity}

Show that the dual of the identity map on a vector space $V$ is the identity map on $V^*$.
Suppose that we have linear maps $f: V \to W$ and $g: W \to X$. Show that ${(gf)}^* = f^* g^*$.

\end{ex}

\begin{sol}

The dual of a linear map $f: V \to W$ is defined by
\[
    (f^*\omega)(v) = \omega \bigl( f(v) \bigr)
\]
where $f^*: W^* \to V^*$.

Let $\id: V \to V$ be the identity map on $V$. For some $v \in V$,
\begin{align*}
    (\id^*\omega)(v) &= \omega \bigl( \id(v) \bigr) \\
                           &= \omega(v)
\end{align*}
giving $\id^*\omega = \omega$, therefore $\id^*: V^* \to V^*$ is the identity map in the dual space.

For the composition $gf = g \circ f$, recall the definition of the \ref{eq:pullbackfunction}. %chktex 2
Let $h: X \to Y$ and consider the pullback
\begin{align*}
    {(g \circ f)}^* h &= h \circ (g \circ f) \\
                      &= h \circ g \circ f \\
                      &= (h \circ g) \circ f \\
                      &= (g^* h) \circ f \\
                      &= f^* g^* h,
\end{align*}
giving ${(gf)}^* = f^* g^*$.

We can also pretend that we don't know this is a pullback and use only the definition of the dual space above, by saying
\begin{align*}
    ({(g \circ f)}^*\omega)(v) &= \omega \bigl( (g \circ f)(v) \bigr) \\
        &= (g^* \omega) \bigl( f(v) \bigr) \\
        &= (f^* g^* \omega)(v).
\end{align*}

\end{sol}

\begin{ex}

Show that the pullback of 1-forms defined by the formula above really exists and is unique.

\end{ex}

\begin{sol}\label{sol:pullback1form}

Let $\phi: M \to N$, $p \mapsto \phi(p) = q$. Then for $v \in T_p M$, $\omega \in T_q^* N$, the pullback $\phi^*: T_q^* N \to T_p^* M$ of $\omega$ by $\phi$ is defined as
\begin{nowidthtags}
\[
    (\phi^* \omega)(v) = \omega(\phi_* v) \tag{pullback of a 1-form}\label{eq:pullback1form}
\]
\end{nowidthtags}
and globally we get ${(\phi^* \omega)}_p = \phi^*(\omega_q)$.

To see this, take a test vector $v \in T_p M$ and, similar to solution~\ref{sol:pushforwardvectorfield},
\begin{align*}
    {(\phi^* \omega)}_p v_p &= (\phi^* \omega) (v) (p) \\
        &= \omega(\phi_* v)(q) \\
        &= \omega_q (\phi_* v_q) \\
        &= \phi^* (\omega_q) v_q.
\end{align*}

Let $\phi^*\nu \in T_p^* M$ be some 1-form where ${(\phi^*\omega)}_p = {(\phi^*\nu)}_p$. It follows from solution~\ref{sol:welldefined1forms} that $\omega = \nu$.

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R} \to \mathbb{R}$ be given by $\phi(t) = \sin(t)$. Let $dx$ be the usual 1-form on $\mathbb{R}$. Show that $\phi^*dx = \cos(t) \, dt$.

\end{ex}

\begin{sol}

Using the fact that the exterior derivative is \emph{natural}, i.e. $\phi^*(df) = d(\phi^*f)$, for some vector $v = f(t)\partial_t$
\begin{align*}
    {(\phi^* dx)}_t v &= d(\phi^* x) (v)(t) \\
        &= v(\phi^* x) (t) \\
        &= v(x \circ \phi)(t) \\
        &= v \bigl( \sin(t) \bigr) \\
        &= f(t) \partial_t \sin(t) \\
        &= f(t) \cos(t) \\
        &= f(t) \cos(t) \partial_t t \\
        &= \cos(t) v(t) \\
        &= \cos(t) \, dt(v).
\end{align*}

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ denote rotation counterclockwise by the angle $\theta$. Let $dx$, $dy$ be the usual basis of 1-forms on $\mathbb{R}^2$. Show that
\begin{align*}
    \phi^* dx &= \cos(\theta) \, dx - \sin(\theta) \, dy, \\
    \phi^* dy &= \sin(\theta) \, dx + \cos(\theta) \, dy.
\end{align*}

\end{ex}

\begin{sol}

Let $v = f_i(x, y)\partial_i$ be some vector in $\Vect(\mathbb{R}^2)$ and $p = (x, y) \in \mathbb{R}^2$. For $\phi$ as in solutions~\ref{sol:pullbackrotation},~\ref{sol:pushforwardrotation},
\begin{align*}
    {(\phi^* dx)}_p v &= d(\phi^* x)(v)(p) \\
        &= d(x \circ \phi)(v)(p) \\
        &= v \bigl( \cos(\theta) x - \sin(\theta) y \bigr) \\
        &= f_1(x, y) \partial_x \bigl( \cos(\theta) x - \sin(\theta) y \bigr) \\
        &\mathrel{\phantom{=}}{} + f_2(x, y) \partial_y \bigl( \cos(\theta) x
                                 - \sin(\theta) y \bigr) \\
        &= f_1(x, y) \cos(\theta) - f_2(x, y) \sin(\theta) \\
        &= \cos(\theta) f_1(x, y) \partial_x x
           - \sin(\theta) f_2(x, y) \partial_y y \\
        &= \cos(\theta) v(x) - \sin(\theta) v(y) \\
        &= \cos(\theta) \, dx(v) - \sin(\theta) \, dy(v)
\end{align*}
and similarly for $\phi^* dy$.

\end{sol}

\section{Change of Coordinates}

\begin{epigraph}
    The introduction of numbers as coordinates \textup{\textelp{}} is an act of violence\ldots
\end{epigraph}

\begin{ex}\label{ex:coordinatetransformation1form}

Show that the coordinate 1-forms $dx^\mu$ really are the differentials of the local coordinates $x^\mu$ on $U$.

\end{ex}

\begin{sol}

The statement requires us to be ``working in the chart'', so for now we'll be explicit and denote the local coordinates on $U$ as $\varphi^*x^\mu$. Then the exterior derivative is
\[
    d(\varphi^*x^\mu) = \varphi^* dx^\mu.
\]
To show that this really forms a basis of coordinate 1-forms, consider the basis vectors ``in the chart'', $\varphi_*^{-1}\partial_\mu$.
\begin{align*}
    d(\varphi^* x^\mu)(\varphi_*^{-1}\partial_\nu) &= \varphi_*^{-1}\partial_\nu (\varphi^* x^\mu) \\
        &= \partial_\nu \bigl( (\varphi^* x^\mu) \circ \varphi^{-1} \bigr) \\
        &= \delta_\nu^\mu.
\end{align*}

\end{sol}

\begin{ex}\label{ex:transform1form}

In the situation above, show that
\[
    dx'^\nu = \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu.
\]
Show that for any 1-form $\omega$ on $\mathbb{R}^n$, writing
\[
    \omega = \omega_\mu dx^\mu = \omega'_\nu dx'^\nu,
\]
your components $\omega'_\nu$ are related to my components $\omega_\mu$ by
\[
    \omega'_\nu = \frac{\partial x^\mu}{\partial x'^\nu} \omega_\mu.
\]

\end{ex}

\begin{sol}

Since 1-forms form a basis, we can write
\[
    dx'^\nu = T_\mu^\nu dx^\mu
\]
for some linear transformation $T_\mu^\nu$. Acting on $\partial_\mu$, we get
\begin{align*}
    dx'^\nu \partial_\mu &= T_\lambda^\nu dx^\lambda \partial_\mu \\
        &= T_\lambda^\nu \delta_\mu^\lambda \\
        &= T_\mu^\nu,
\shortintertext{but}
    dx'^\nu\partial_\mu &= \partial_\mu x'^\nu \\
        &= \frac{\partial x'^\lambda}{\partial x^\mu} \partial'_\lambda x'^\nu \\
        &= \frac{\partial x'^\lambda}{\partial x^\mu} \delta_\lambda^\nu \\
        &= \frac{\partial x'^\nu}{\partial x^\mu}
\end{align*}
so the transformation rule for coordinate 1-forms is
\[
    dx'^\nu = \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu.
\]
We can use this to write any 1-form $\omega$ on $\mathbb{R}^n$ in a different basis, as
\[
    \omega = \omega_\mu dx^\mu = \omega_\mu \frac{\partial x^\mu}{\partial x'^\nu} dx'^\nu.
\]
In this coordinate system, we identify the components of $\omega$ as
\[
    \omega'_\nu = \frac{\partial x^\mu}{\partial x'^\nu} \omega_\mu.
\]

\end{sol}

\begin{ex}\label{ex:pullbackdx}

Show that
\[
    \phi^*(dx'^\nu) = \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu.
\]

\end{ex}

\begin{sol}

Consider the action on the coordinate vector field $\partial_\lambda$,
\begin{nowidthtags}
\begin{align*}
    \phi^*(dx'^\nu)\partial_\lambda &= d(\phi^*x'^\nu) \partial_\lambda \\
        &= \partial_\lambda(\phi^*x'^\nu) \\
        &\equiv \frac{\partial x'^\nu}{\partial x^\lambda} \tag{``get used to it''}\\
        &= \frac{\partial x'^\nu}{\partial x^\mu} \delta_\lambda^\mu \\
        &= \frac{\partial x'^\nu}{\partial x^\mu} \partial_\lambda x^\mu \\
        &= \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu \partial_\lambda.
\end{align*}
\end{nowidthtags}
We could instead use the result from exercise~\ref{ex:coordinatetransformation1form}, again acting on the coordinate vector field $\partial_\lambda$,
\begin{align*}
   \phi^*(dx'^\nu)\partial_\lambda &= \phi^*\left(\frac{\partial x'^\nu}{\partial x^\mu} dx^\mu \right) \partial_\lambda \\
        &= \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu (\phi_* \partial_\lambda) \\
        &\equiv \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu \partial_\lambda
\end{align*}
where we are sloppy about the pullback in the last line, as is the convention.

\end{sol}

\begin{ex}\label{ex:changebasisinvertible}

Let
\[
    e_\mu = T_\mu^\nu\partial_\nu
\]
where $\partial_\nu$ are the coordinate vector fields associated to local coordinates on an open set $U$, and $T_\mu^\nu$ are functions on $U$.
Show that the vector fields $e_\mu$ are a basis of vector fields on $U$ if and only if for each $p \in U$ the matrix $T_\mu^\nu(p)$ is invertible.

\end{ex}

\begin{sol}

For $\left\{e_\mu\right\}$ to form a basis, they must be linearly independent and span $U$.

Suppose $T$ is invertible at $p$. Then acting on both sides by $S = T^{-1}$ gives us
\begin{align*}
    S_\mu^\lambda e_\lambda &= S_\mu^\lambda T_\lambda^\nu \partial_\nu \\
        &= \delta_\mu^\nu \partial_\nu \\
        &= \partial_\mu.
\end{align*}
Any vector $u \in U$ can therefore be expressed as
\[
    u = u^\mu \partial_\mu = u^\mu S_\mu^\lambda e_\lambda = u'^\mu e_\mu
\]
so $\left\{e_\mu\right\}$ forms a basis for $U$.

Assume $\left\{e_\mu\right\}$ forms a basis for $U$. Then for some smooth functions on $U$, $S_\mu^\nu$,
\begin{align*}
    \partial_\mu &= S_\mu^\nu e_\nu \\
                 &= S_\mu^\nu T_\nu^\lambda \partial_\lambda.
\end{align*}
We must identify $S_\mu^\nu T_\nu^\lambda = \delta_\mu^\lambda$, so $T$ is invertible.

\end{sol}

\begin{ex}\label{ex:dualbasisexists}

Use \hyperref[ex:changebasisinvertible]{the previous exercise} to show that the dual basis exists and is unique.

\end{ex}

\begin{sol}

If $\left\{e_\mu\right\}$ is a basis of vector fields on $U$, we automatically get a dual basis of 1-forms $\left\{f^\mu\right\}$ satisfying
\[
    f^\mu(e_\nu) = \delta_\nu^\mu.
\]

We can express
\[
    f^\mu = S_\nu^\mu dx^\nu
\]
for some smooth functions $S_\nu^\mu$ on $U$. Then
\begin{align*}
    f^\mu(e_\nu) &= S_\kappa^\mu dx^\kappa (T_\nu^\lambda \partial_\lambda) \\
                 &= S_\kappa^\mu T_\nu^\lambda dx^\kappa \partial_\lambda \\
                 &= S_\kappa^\mu T_\nu^\lambda \delta_\lambda^\kappa \\
                 &= S_\lambda^\mu T_\nu^\lambda
\end{align*}
so the dual basis exists, since $T$ is invertible (from exercise~\ref{ex:changebasisinvertible}).

Suppose there exists 1-forms $\left\{g^\mu\right\}$ also satisfying $g^\mu(e_\nu) = \delta^\mu_\nu$.
Then for some smooth functions $S'^\mu_\nu$ on $U$, $g^\mu = S'^\mu_\nu dx^\nu$ and, eventually, $S'^\mu_\lambda T^\lambda_\nu = \delta ^\mu_\nu$. But the inverse of $T$ is unique, so $S' = S$ and therefore $g^\mu = f^\mu$.

\end{sol}

\begin{ex}

Let $e_\mu$ be a basis of vector fields on $U$ and let $f^\mu$ be the dual basis of 1-forms. Let
\[
    e'_\mu = T_\mu^\nu e_\nu
\]
be another basis of vector fields and let $f'^\mu$ be the corresponding basis of 1-forms. Show that
\[
    f'^\mu = {(T^{-1})}^\mu_\nu f^\nu.
\]
Show that if $v = v^\mu e_\mu = v'^\mu e'_\mu$, then
\[
    v'^\mu = {(T^{-1})}^\mu_\nu v^\nu
\]
and that if $\omega = \omega_\mu f^\mu = \omega'_\mu f'^\mu$, then
\[
    \omega'_\mu = T_\mu^\nu \omega_\nu.
\]

\end{ex}

\begin{sol}

We know that $f'^\mu = S_\nu^\mu f^\nu$ for some functions $S_\nu^\mu$ on $U$.
Then
\begin{align*}
    f'^\mu(e'_\nu) &= f'^\mu(T^\lambda_\nu e_\lambda) \\
                   &= S^\mu_\kappa f^\kappa T^\lambda_\nu e_\lambda \\
                   &= S^\mu_\kappa T^\lambda_\nu f^\kappa e_\lambda \\
                   &= S^\mu_\kappa T^\lambda_\nu \delta^\kappa\lambda \\
                   &= S^\mu_\lambda T^\lambda_\nu.
\end{align*}
But $f'^\mu(e'_\nu) = \delta^\mu_\nu$ from the definition of the dual basis, so $S = T^{-1}$.

If $v = v^\mu e_\mu = v'^\mu e'_\mu$, then $v^\nu e_\nu = v'^\lambda T_\lambda^\nu e_\nu$ and equating coefficients gets us $v^\nu = T_\lambda^\nu v'^\lambda$.
Applying $S = T^{-1}$,
\begin{align*}
    S^\mu_\nu v^\nu &= S^\mu_\nu T_\lambda^\nu v'^\lambda \\
                    &= \delta^\mu_\lambda v'^\lambda \\
                    &= v'^\mu
\end{align*}
so the components of a vector are contravariant.

If $\omega = \omega_\mu f^\mu = \omega'_\mu f'^\mu$, then $\omega_\nu f^\nu = \omega'_\lambda S^\lambda_\nu f^\nu$ and equating coefficients gets us $\omega_\nu = S_\nu^\lambda \omega'_\lambda$.
Applying $T$,
\begin{align*}
    T_\mu^\nu \omega_\nu &= T_\mu^\nu S_\nu^\lambda \omega'_\lambda \\
                         &= \delta^\lambda_\mu \omega'_\lambda \\
                         &= \omega'_\mu
\end{align*}
so the components of a 1-form are covariant.

\end{sol}

\section{\emph{p}-forms}

\begin{ex}\label{ex:triplewedgeproduct}

Show that
\[
    u \wedge v \wedge w = \det \begin{pmatrix}
            u_x & u_y & u_z \\
            v_x & v_y & v_z \\
            w_x & w_y & w_z
        \end{pmatrix} dx \wedge dy \wedge dz.
\]
Compare this to $\vec{u} \cdot (\vec{v} \times \vec{w})$.

\end{ex}

\begin{sol}

Let $u, v, w$ be vectors,
\begin{align*}
    u &= u_x dx + u_y dy + u_z dz, \\
    v &= v_x dx + v_y dy + v_z dz, \\
    w &= w_x dx + w_y dy + w_z dz.
\end{align*}
Then
\begin{align*}
    \SwapAboveDisplaySkip
    v \wedge w =&\ (v_x w_y - v_y w_x) \, dx \wedge dy \\
                &+ (v_y w_z - v_z w_y) \, dy \wedge dz \\
                &+ (v_z w_x - v_x w_z) \, dz \wedge dx,
\end{align*}
so the triple product
\begin{align*}
    u \wedge v \wedge w =&\ u_x (v_y w_z - v_z w_y) \, dx \wedge dy \wedge dz \\
        &+ u_y (v_z w_x - v_x w_z) \, dy \wedge dz \wedge dx \\
        &+ u_z (v_x w_y - v_y w_x) \, dz \wedge dx \wedge dy \\
        =&\ u_x (v_y w_z - v_z w_y) \, dx \wedge dy \wedge dz \\
        &- u_y (v_x w_z - v_z w_x) \, dx \wedge dy \wedge dz \\
        &+ u_z (v_x w_y - v_y w_x) \, dx \wedge dy \wedge dz \\
        =&\ \det \begin{pmatrix}
                u_x & u_y & u_z \\
                v_x & v_y & v_z \\
                w_x & w_y & w_z
            \end{pmatrix} dx \wedge dy \wedge dz.
\end{align*}

Consider the traditional vectors $\vec{u}, \vec{v}, \vec{w}$ on $\mathbb{R}^3$.
\[
    \vec{v} \times \vec{w}
        = (v_y w_z - v_z w_y) \vec{\imath}
          - (v_z w_x - v_x w_z) \vec{\jmath}
          + (v_x w_y - v_y w_x) \vec{k},
\]
so the triple product
\[
    \vec{u} \cdot (\vec{v} \times \vec{w})
        = u_x (v_y w_z - v_z w_y)
          - u_y (v_x w_z - v_z w_x)
          + u_z (v_x w_y - v_y w_x),
\]
the single component of $u \wedge v \wedge w$.

\end{sol}

\begin{ex}

Show that if $a, b, c, d$ are four vectors in a 3-dimensional space then $a \wedge b \wedge c \wedge d = 0$.

\end{ex}

\begin{sol}

Using $dx, dy, dz$ as a basis, we have from exercise~\ref{ex:triplewedgeproduct} that
\[
    b \wedge c \wedge d = \alpha \, dx \wedge dy \wedge dz, \quad
    \alpha = \det \begin{pmatrix}
        b_x & b_y & b_z \\
        c_x & c_y & c_z \\
        d_x & d_y & d_z
    \end{pmatrix}.
\]
Then
\begin{align*}
    a \wedge b \wedge c \wedge d =&\ a \wedge \alpha \, dx \wedge dy \wedge dz \\
        =&\ (a_x dx + a_y dy + a_z dz) \wedge \alpha \, dx \wedge dy \wedge dz \\
        =&\ \alpha a_x \, dx \wedge dx \wedge dy \wedge dz \\
        &+ \alpha a_y \, dy \wedge dx \wedge dy \wedge dz \\
        &+ \alpha a_z \, dz \wedge dx \wedge dy \wedge dz \\
        =&\ 0
\end{align*}
since $w \wedge w = 0$ by antisymmetry and each term contains one repeated basis element.

\end{sol}

\begin{ex}

Describe $\Lambda V$ if $V$ is 1-dimensional, 2-dimensional, or 4-dimensional.

\end{ex}

\begin{sol}

Let $u, v \in V$ over a field $\mathbb{F}$.

If $\dim(V) = 1$,
\[
    u = u_x dx, \quad v = v_x dx
\]
so $u \wedge v = 0$ by antisymmetry. Therefore $\Lambda V$ consists of $\mathbb{F}$ and all linear combinations of $dx$ (i.e. $V$).

If $\dim(V) = 2$,
\[
    u = u_x dx + u_y dy, \quad v = v_x dx + v_y dy
\]
so
\begin{align*}
    \SwapAboveDisplaySkip
    u \wedge v &= u_x v_y \, dx \wedge dy + u_y v_x \, dy \wedge dx \\
               &= (u_x v_y - u_y v_x) \, dx \wedge dy.
\end{align*}
Therefore $\Lambda V$ consists of $\mathbb{F}$, $V$ and all linear combinations of the 2-forms $dx \wedge dy$ above.

If $\dim(V) = 4$ with basis $\left\{dt, dx, dy, dz\right\}$, $\Lambda V$ will consist of $\mathbb{F}$, $V$ and all linear combinations of
\begin{gather*}
    dt \wedge dx, \quad
    dt \wedge dy, \quad
    dt \wedge dz, \quad
    dx \wedge dy, \quad
    dx \wedge dz, \quad
    dy \wedge dz, \\
    dt \wedge dx \wedge dy, \quad
    dt \wedge dx \wedge dz, \quad
    dt \wedge dy \wedge dz, \quad
    dx \wedge dy \wedge dz, \\
    dt \wedge dx \wedge dy \wedge dz.
\end{gather*}

\end{sol}

\begin{ex}

Let $V$ be an $n$-dimensional vector space. Show that $\Lambda^p V$ is empty for $p > n$ and that for $0 \leq p \leq n$ the dimension of $\Lambda^p V$ is $\frac{n!}{p! (n - p)!}$.

\end{ex}

\begin{sol}

Let $\left\{e_1, \ldots, e_n\right\}$ be a basis for $V$.
The subspace $\Lambda^p V$ consists of all linear combinations of the form $e_{i_1} \wedge \cdots \wedge e_{i_p}$.

$\Lambda^n V$ has the single basis element $e_1 \wedge \cdots \wedge e_n$.
The exterior product of any element of $\Lambda^n V$ with any $v \in V$ is necessarily zero since we have exhausted our supply of linearly independent vectors $e_i \in V$. Therefore $\Lambda^p V$ is empty for $p > n$.

The dimension of $\Lambda^p V$ is the number of subsets of size $p$ we can form from the set of $n$ basis vectors of $V$, so
\[
    \dim(\Lambda^p V) = \binom{n}{p} = \frac{n!}{p! (n - p)!}.
\]
This correctly reproduces edge cases such as $\dim(\Lambda^0 V) = \binom{n}{0} = 1$ (for a vector space $V(\mathbb{F})$, this is the underlying field $\mathbb{F}$) and $\dim(\Lambda^{n+1}V) = 0$.

\end{sol}

\begin{ex}

Show that $\Lambda V$ is the direct sum of the subspaces $\Lambda^p V$:
\[
    \Lambda V = \bigoplus \Lambda^p V,
\]
and that the dimension of $\Lambda V$ is $2^n$ if $V$ is $n$-dimensional.
\end{ex}

\begin{sol}

$\Lambda^p V$ is the subspace of $\Lambda V$ consisting of linear combinations of $p$-fold products of vectors in $V$.

For any $q \neq p$, the elements of $\Lambda^q V$ and $\Lambda^p V$ are linearly independent. Therefore for any $w \in \Lambda V$,
$w = w_0 + \cdots + w_n$
where each $w_p \in \Lambda^p V$, so
\begin{align*}
    \Lambda V &= \Lambda^0 V \oplus \cdots \oplus \Lambda^n V \\
              &= \bigoplus_{p=0}^n \Lambda^p V.
\end{align*}
The dimension of $\Lambda V$ is therefore
\begin{align*}
    \dim(\Lambda V) &= \sum_{p = 0}^n \dim(\Lambda^p V) \\
                    &= \sum_{p = 0}^n \binom{n}{p} \\
                    &= 2^n
\end{align*}
by the binomial theorem.

\end{sol}

\begin{ex}

Given a vector space $V$, show that $\Lambda V$ is a \emph{graded commutative} or \emph{supercommutative} algebra, that is, if $\omega \in \Lambda^p V$ and $\mu \in \Lambda^q V$ then
\[
    \omega \wedge \mu = {(-1)}^{pq} \mu \wedge \omega.
\]
Show that for any manifold $M$, $\Omega(M)$ is graded commutative.

\end{ex}

\begin{sol}

Let $\omega = \omega_1 \wedge \cdots \wedge \omega_p$ and $\mu = \mu_1 \wedge \cdots \wedge \mu_q$.
Then
\begin{align*}
    \omega \wedge \mu &= \omega_1 \wedge \cdots \wedge \omega_p \wedge \mu_1 \wedge \cdots \wedge \mu_q \\
        &= {(-1)}^p \mu_1 \wedge \omega_1 \wedge \cdots \wedge \omega_p \wedge \mu_2 \wedge \cdots \wedge \mu_q \\
        &= {(-1)}^{2p} \mu_1 \wedge \mu_2 \wedge \omega_1 \wedge \cdots \wedge \omega_p \wedge \mu_3 \wedge \cdots \wedge \mu_q \\
        &\vdotswithin{=} \\
        &= {(-1)}^{pq} \mu_1 \wedge \cdots \wedge \mu_q \wedge \omega_1 \wedge \cdots \wedge \omega_p \\
        &= {(-1)}^{pq} \mu \wedge \omega.
\end{align*}

The above result holds analogously for any $\omega \in \Omega^p(M)$ and $\mu \in \Omega^q(M)$. Since $\Omega(M) = \bigoplus \Omega^p(M)$, $\Omega(M)$ is graded commutative over any manifold $M$.

\end{sol}

\begin{ex}

Show that differential forms are contravariant. That is, show that if $\phi: M \to N$ is a map from the manifold $M$ to the manifold $N$, there is a unique pullback map
\[
    \phi^*: \Omega(N) \to \Omega(M)
\]
agreeing with the usual pullback on 0-forms (functions) and 1-forms and satisfying
\begin{align*}
    \SwapAboveDisplaySkip
    \phi^* (\alpha \omega) &= \alpha \phi^* \omega \\
    \phi^*(\omega + \mu) &= \phi^* \omega + \phi^* \mu \\
    \phi^*(\omega \wedge \mu) &= \phi^* \omega \wedge \phi^* \mu
\end{align*}
for all $\omega, \mu \in \Omega(N)$ and $\alpha \in \mathbb{R}$.

\end{ex}

\begin{sol}

Since any $\mu \in \Omega(N)$ can be expressed as $\mu = \mu_0 + \cdots + \mu_n$ where each $\mu_p \in \Omega^p(N)$, we can construct a pullback $\phi^*$ satisfying
\begin{align*}
    \phi^* \mu &= \phi^* (\mu_0 + \cdots + \mu_n) \\
               &= \phi^* \mu_0 + \cdots + \phi^* \mu_n
\end{align*}
by linearity and only consider how $\phi^*$ acts on each $p$-form.

The pullback of a $p$-form $\omega = \omega_1 \wedge \cdots \wedge \omega_p \in \Omega^p(N)$ should generalise the \ref{eq:pullback1form}.  %chktex 2
So on a collection of vectors $v_1, \ldots, v_p \in \Vect(M)$ we would like to get
\begin{align*}
    (\phi^* \omega)(v_1, \ldots, v_p) &= \omega(\phi_* v_1, \ldots, \phi_* v_p) \\
        &=\omega_1 \wedge \cdots \wedge \omega_p (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \phi^* \omega_1 \wedge \cdots \wedge \phi^* \omega_p (v_1, \ldots, v_p).
\end{align*}
which holds since each $\omega_i$ acts on $\phi_* v_i$ independently. Then in terms of components,
\begin{align*}
    \phi^* \omega &= \phi^* \left( \tfrac{1}{p!} \omega_{i_1,\ldots,i_p}\, e^{i_1} \wedge \cdots \wedge e^{i_p} \right) \\
        &= \phi^* \tfrac{1}{p!} \omega_{i_1,\ldots,i_p} \phi^* (e^{i_1} \wedge \cdots \wedge e^{i_p}) \\
        &= \tfrac{1}{p!} \phi^* \omega_{i_1,\ldots,i_p} \phi^* e_{i^1} \wedge \cdots \wedge \phi^* e_{i^p}.
\end{align*}

Let $\omega, \mu \in \Omega^p(N)$. Then
\begin{align*}
    \phi^* (\alpha \omega) (v_1, \ldots, v_p) &= \alpha \omega (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \alpha \phi^* \omega (v_1, \ldots, v_p)
\end{align*}
so $\phi^* (\alpha \omega) = \alpha \phi^* \omega$,
\begin{align*}
    \phi^*(\omega + \mu) (v_1, \ldots, v_p) &= (\omega + \mu) (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \omega(\phi_* v_1, \ldots, \phi_* v_p) + \mu(\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \phi^* \omega (v_1, \ldots, v_p) + \phi^* \mu (v_1, \ldots, v_p) \\
        &= (\phi^* \omega + \phi^* \mu) (v_1, \ldots, v_p)
\end{align*}
so $\phi^*(\omega + \mu) = \phi^* \omega + \phi^* \mu$,
\begin{align*}
    \phi^*(\omega \wedge \mu) (v_1, \ldots, v_p) &= (\omega \wedge \mu) (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \phi^*\omega \wedge \phi^*\mu (v_1, \ldots, v_p)
\end{align*}
so $\phi^* (\omega \wedge \mu) = \phi^* \omega \wedge \phi^* \mu$.

\end{sol}

\begin{ex}

Compare how 1-forms and 2-forms on $\mathbb{R}^3$ transform under \emph{parity}. That is, let $P: \mathbb{R}^3 \to \mathbb{R}^3$ be the map
\[
    P(x, y, z) = (-x, -y, -z),
\]
known as the ``parity transformation''. Note that $P$ maps right-handed bases to left-handed bases and vice versa. Compute $\phi^*(\omega)$ when $\omega$ is the 1-form $\omega_\mu dx^\mu$ and when it is the 2-form $\frac{1}{2}\omega_{\mu\nu}dx^\mu \wedge dx^\nu$.

\end{ex}

\begin{sol}\label{sol:parity1form2form}

Assume $\phi^*$ is the pullback by $P$. Consider the pullback of $dx^\mu$ acting on the coordinate vector field $\partial_\nu$,
\begin{align*}
    (\phi^* dx^\mu) \partial_\nu &= d(\phi^* x^\mu) \partial_\nu \\
        &= \partial_\nu (\phi^* x^\mu) \\
        &= \partial_\nu (x^\mu \circ \phi) \\
        &= -\delta_\nu^\mu \\
        &= -\partial_\nu x^\mu \\
        &= - dx^\mu \partial_\nu,
\end{align*}
so $\phi^* dx^\mu = -dx^\mu$.

If $\omega \in \Omega^1(\mathbb{R}^3)$, then
\[
    \phi^* \omega = \phi^*(\omega_\mu dx^\mu) = -\omega
\]
and if $\omega \in \Omega^2(\mathbb{R}^3)$, then
\begin{align*}
    \phi^* \omega &= \phi^*\left(\tfrac{1}{2}\omega_{\mu\nu} \, dx^\mu \wedge dx^\nu\right) \\
        &= \tfrac{1}{2}\omega_{\mu\nu} \, \phi^*(dx^\mu \wedge dx^\nu) \\
        &= \tfrac{1}{2}\omega_{\mu\nu} \, \phi^* dx^\mu \wedge \phi^* dx^\nu \\
        &= \tfrac{1}{2}\omega_{\mu\nu} (-dx^\mu) \wedge (-dx^\nu) \\
        &= \omega.
\end{align*}

\end{sol}

\section{The Exterior Derivative}

\begin{ex}

Show that on $\mathbb{R}^n$ the exterior derivative of any 1-form is given by
\[
    d(\omega_\mu dx^\mu) = \partial_\nu \omega_\mu \, dx^\nu \wedge dx^\mu.
\]

\end{ex}

\begin{sol}

Since $\omega_\mu$ is a 0-form,
\begin{align*}
    d(\omega_\mu dx^\mu) &= d(\omega_\mu \wedge dx^\mu) \\
        &= d\omega_\mu \wedge dx^\mu + \omega_\mu \wedge d(dx^\mu) \\
        &= d\omega_\mu \wedge dx^\mu \\
        &= \partial_\nu \omega_\mu \, dx^\nu \wedge dx^\mu.
\end{align*}

\end{sol}

\chapter{Rewriting Maxwell's Equations}

\begin{epigraph}
    Hence space of itself, and time of itself, will sink into mere shadows, and only a union of the two shall survive.
\end{epigraph}

\section{The First Pair of Equations}

\begin{ex}

Show that any 2-form $F$ on $\mathbb{R} \times S$ can be uniquely expressed as $B + E \wedge dt$ in such a way that for any local coordinates $x^i$ on $S$ we have $E = E_i dx^i$ and $B = \frac{1}{2}B_{ij}dx^i \wedge dx^j$.

\end{ex}

\begin{sol}\label{sol:bplusedt}

Since $\mathbb{R} \times S$ is a manifold, we have an atlas $\left\{\varphi_\alpha\right\}$ for all open sets $U_\alpha$ giving local coordinates $x^\mu = \varphi_\alpha(u)$, $u \in U_\alpha$.

Notice that $\left\{dx^i \wedge dt, dx^i \wedge dx^j\right\}$ spans $\Omega^2(U_\alpha)$.
If $F \in \Omega^2(U_\alpha)$, we can express it as
\begin{align*}
    F &= \frac{1}{2} F_{\mu\nu}dx^\mu \wedge dx^\nu \\
      &= \frac{1}{2} \left( F_{0i} dt \wedge dx^i + F_{i0} dx^i \wedge dt + F_{ij} dx^i \wedge dx^j \right) \\
      &= \frac{1}{2} \left( 2 F_{i0} dx^i \wedge dt + F_{ij} dx^i \wedge dx^j \right) \\
      &= \frac{1}{2} F_{ij} dx^i \wedge dx^j + F_{i0} dx^i \wedge dt
\end{align*}
where $F_{0i} = -F_{i0}$ by antisymmetry. Comparing coefficients, we get
\[
    F = B + E \wedge dt
\]
where $F_{ij} = B_{ij}$ and $F_{i0} = E_i$. Uniqueness is automatic since each component is determined by its basis 2-form.

\end{sol}

\begin{ex}\label{ex:pformspaceandtime}

Show that for any form $\omega$ on $\mathbb{R} \times S$ there is a unique way to write $d\omega = dt \wedge \partial_t \omega + d_S \omega$ such that for any local coordinates $x^i$ on $S$, writing $t = x^0$, we have
\begin{align*}
    d_S\omega &= \partial_i \omega_I \, dx^i \wedge dx^I, \\
    dt \wedge \partial_t \omega &= \partial_0 \omega_I \, dx^0 \wedge dx^I.
\end{align*}

\end{ex}

\begin{sol}

Similarly to solution~\ref{sol:bplusedt}, since $\omega \in \Omega(U_\alpha)$ we have that $\omega = \omega_I dx^I$, so
\begin{align*}
    d\omega &= \partial_\mu \omega_I \, dx^\mu \wedge dx^I \\
        &= \partial_0 \omega_I \, dx^0 \wedge dx^I + \partial_i \omega_I \, dx^i \wedge dx^I \\
        &= dx^0 \wedge \partial_0 \omega_I \wedge dx^I + \partial_i \omega_I \, dx^i \wedge dx^I \\
        &= dx^0 \wedge \partial_0 \omega + \partial_i \omega_I \, dx^i \wedge dx^I \\
        &= dt \wedge \partial_t \omega + d_S \omega.
\end{align*}
Again, this is guaranteed to be unique by linearity.

\end{sol}

\section{The Metric}

\begin{ex}

Use the non-degeneracy of the metric to show that the map from $V$ to $V^*$ given by
\[
    v \mapsto g(v, \cdot)
\]
is an isomorphism, that is, one-to-one and onto.

\end{ex}

\begin{sol}\label{sol:metricisomorphism}

Let $v, w \in V$. By bilinearity,
\[
    g(v,\cdot) - g(w,\cdot) = g(v - w, \cdot)
\]
so $g(v,\cdot) - g(w,\cdot) = 0$ implies $v - w = 0$ by non-degeneracy or, equivalently, $g(v,\cdot) = g(w,\cdot)$ implies $v = w$. Therefore the map is injective.

Since the map is injective and, from solution~\ref{sol:gradient}, $\dim(V) = \dim(V^*)$, pick a basis $\left\{e_\mu\right\}$ for $V$ and we get a corresponding basis $\left\{f^\mu\right\}$ for $V^*$.

We claim that we can express any $\omega \in V^*$ as $\omega = g(v,\cdot)$ for some $v \in V$.
\begin{align*}
    \omega &= \omega_\nu f^\nu \\
           &= \omega_\nu g(e_\nu, \cdot) \\
           &= \omega(e_\nu) g(e_\nu, \cdot) \\
           &= g(v, e_\nu) g(e_\nu, \cdot) \\
           &= g(v^\mu e_\mu, e_\nu) g(e_\nu, \cdot) \\
           &= v^\mu g(e_\mu, e_\nu) g(e_\nu, \cdot).
\end{align*}
Because $g$ is non-degenerate, the above is solvable for $v^\mu$ and therefore the map is surjective.

\end{sol}

\begin{ex}\label{ex:loweringindex}

Let $v = v^\mu e_\mu$ be a vector field on a chart. Show that the corresponding 1-form $g(v, \cdot)$ is equal to $v_\nu f^\nu$, where $f^\nu$ is the dual basis of 1-forms and
\[
    v_\nu = g_{\mu\nu}v^\mu.
\]

\end{ex}

\begin{sol}

We'll use the same argument as in solution~\ref{sol:metricisomorphism}. Denote $\omega = g(v, \cdot)$, but since $\omega$ is a 1-form we can express it in components as
\begin{align*}
    \omega &= \omega_\nu f^\nu \\
           &= \omega(e_\nu) f^\nu \\
           &= g(v, e_\nu) f^\nu \\
           &= g(v^\mu e_\mu, e_\nu) f^\nu \\
           &= v^\mu g(e_\mu e_\nu) f^\nu \\
           &= v^\mu g_{\mu\nu} f^\nu \\
           &= g_{\mu\nu} v^\mu f^\nu \\
           &= v_\nu f^\nu
\end{align*}
where we identify $g_{\mu\nu}v^\mu = v_\nu$.

\end{sol}

\begin{ex}\label{ex:raisingindex}

Let $\omega = \omega_\mu f^\mu$ be a 1-form on a chart. Show that the corresponding vector field is equal to $\omega^\nu e_\nu$, where
\[
    \omega^\nu = g^{\mu\nu} \omega_\mu.
\]

\end{ex}

\begin{sol}

Recall that the metric $g$ is symmetric, so $g_{\mu\nu} = g(e_\mu, e_\nu) = g(e_\nu, e_\mu) = g_{\nu\mu}$.
From exercise~\ref{ex:loweringindex} we have that for a vector field $\omega^\mu e_\mu$, the corresponding 1-form is
\[
    \omega = \omega_\mu f^\mu = g_{\mu\nu}\omega^\nu f^\mu.
\]
Applying the inverse $g^{\mu\nu}$ to the components $\omega_\mu = g_{\mu\nu}\omega^\nu$,
\begin{align*}
    g^{\mu\nu} \omega_\mu &= g^{\mu\nu} g_{\mu\nu} \omega^\nu \\
                          &= \omega^\nu.
\end{align*}

\end{sol}

\begin{ex}\label{ex:minkowskimetric}

Let $\eta$ be the Minkowski metric on $\mathbb{R}^4$ as defined above.
Show that its components in the standard basis are
\[
    \eta_{\mu\nu} = \begin{pmatrix*}[r]
            -1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1
        \end{pmatrix*}.
\]

\end{ex}

\begin{sol}

For $v, w \in \Vect(\mathbb{R}^4)$, the Minkowski metric $\eta$ is defined by
\[
    \eta(v, w) = -v^0 w^0 + v^1 w^1 + v^2 w^2 + v^3 w^3.
\]
Then in an orthonormal basis $\left\{e_\mu\right\}$,
\[
    \eta_{\mu\nu} = \eta(e_\mu, e_\nu) = \begin{dcases*}
            -1            & if $\mu = \nu = 0$, \\ %chktex 1
            \phantom{-} 1 & if $\mu = \nu$, $1 \leq \mu \leq 3$, \\ %chktex 1
            \phantom{-} 0 & otherwise,
        \end{dcases*}
\]
which we can write in matrix form as above.

\end{sol}

\begin{ex}

Show that $g^\mu_\nu$ is equal to the Kronecker delta $\delta^\mu_\nu$, that is, $1$ if $\mu = \nu$ and $0$ otherwise. Note that here the order of indices does not matter, since $g_{\mu\nu} = g_{\nu\mu}$.

\end{ex}

\begin{sol}

Lowering the index, $g_{\mu\lambda}g^\mu_\nu = g_{\lambda\nu}$.
But $g_{\lambda\nu} = g_{\mu\lambda}\delta^\mu_\nu$, so we identify $g^\mu_\nu = \delta^\mu_\nu$.

Alternatively, since $g_{\mu\nu}$ and $g^{\mu\nu}$ are inverses, $g^{\mu\lambda} g_{\lambda\nu} = \delta^\mu_\nu$ by definition.
But $g^{\mu\lambda} g_{\lambda\nu} = g^\mu_\nu$ so $g^\mu_\nu = \delta^\mu_\nu$.

\end{sol}

\begin{ex}\label{ex:innerproductpforms}

Show that the inner product of $p$-forms is non-degenerate by supposing that $(e^1, \ldots, e^n)$ is any orthonormal basis of $1$-forms in some chart, with
\[
    g(e^i, e^i) = \epsilon(i),
\]
where $\epsilon(i) = \pm 1$. Show the $p$-fold wedge products
\[
    e^{i_1} \wedge \cdots \wedge e^{i_p}
\]
form an orthonormal basis of $p$-forms with
\[
    \innerp{e^{i_1} \wedge \cdots \wedge e^{i_p}}
           {e^{i_1} \wedge \cdots \wedge e^{i_p}}
        = \epsilon(i_1) \cdots \epsilon(i_p).
\]

\end{ex}

\begin{sol}

Let $\mu = \mu^1 \wedge \cdots \wedge \mu^p$ be a $p$-form. If $\innerp{\mu}{ \omega} = 0$ for all $p$-forms $\omega = \omega^1 \wedge \cdots \wedge \omega^p$, then
\begin{align*}
    \innerp{\mu}{\omega} &= \innerp{\mu^1 \wedge \cdots \wedge \mu^p}
                                   {\omega^1 \wedge \cdots \wedge \omega^p} \\
        &= \det \bigl( g(\mu^i, \omega^j) \bigr) \\
        &= 0.
\end{align*}
But $g$ is non-degenerate, so the determinant of $g(\mu^i, \omega^j)$ must be non-zero unless $\mu = 0$.

The inner product of basis 1-forms is
\[
    g(e^i, e^j) = g^{ij} = \begin{pmatrix}
            \epsilon(1) & & \\
            & \ddots & \\
            & & \epsilon(p)
        \end{pmatrix}.
\]
From the definition of the inner product of $p$-forms,
\[
    \innerp{e^{i_1} \wedge \cdots \wedge e^{i_p}}
           {e^{j_1} \wedge \cdots \wedge e^{j_p}}
        = \det \bigl( g(e^{i_k}, e^{j_k}) \bigr),
\]
but $g(e^{i_k}, e^{j_k}) = 0$ if $i_k \neq j_k$ and so too is its determinant.
Taking the inner product of a basis $p$-form with itself,
\begin{align*}
    \innerp{e^{i_1} \wedge \cdots \wedge e^{i_p}}
           {e^{i_1} \wedge \cdots \wedge e^{i_p}}
        &= \det \bigl( g(e^{i_k}, e^{i_k}) \bigr) \\
        &= \prod_{k = 1}^p \epsilon(i_k) \\
        &= \epsilon(i_1) \cdots \epsilon(i_p)
\end{align*}
since $g^{ij}$ is diagonal. Therefore $\left\{e^{i_1} \wedge \cdots \wedge e^{i_p}\right\}$ forms an orthonormal basis.

\end{sol}

\begin{ex}\label{ex:esquaredbsquared}

Let $E = E_x dx + E_y dy + E_z dz$ be a 1-form on $\mathbb{R}^3$ with its Euclidean metric. Show that
\[
    \innerp{E}{E} = E_x^2 + E_y^2 + E_z^2.
\]
Similarly, let
\[
    B = B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy
\]
be a 2-form. Show that
\[
    \innerp{B}{B} = B_x^2 + B_y^2 + B_z^2.
\]
In physics, the quantity
\[
    \frac{1}{2} \bigl( \innerp{E}{E} + \innerp{B}{B} \bigr)
\]
is called the \emph{energy density} of the electromagnetic field. The quantity
\[
    \frac{1}{2} \bigl( \innerp{E}{E} - \innerp{B}{B} \bigr) \label{eq:lagrangianvacuumem}
\]
is called the \emph{Lagrangian} for the vacuum Maxwell's equations, which we discuss more in Chapter 4 of Part II in greater generality.

\end{ex}

\begin{sol}

From the definition of the inner product of 1-forms,
\begin{align*}
    \innerp{E}{E} &= g^{ij} E_i E_j \\
                  &= \delta^{ij} E_i E_j \\
                  &= E_x^2 + E_y^2 + E_z^2.
\end{align*}
From exercise~\ref{ex:innerproductpforms},
\begin{align*}
    \innerp{dx^a \wedge dx^b}{dx^c \wedge dx^d} &= \det \bigl(g(dx^i, dx^j)\bigr) \\
        &= g(dx^a, dx^c) g(dx^b, dx^d) \\
        &= \delta^{ac} \delta^{bd},
\end{align*}
so by bilinearity,
\begin{align*}
    \innerp{B}{B} =&\ \innerp{B}{B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy} \\
        =&\ \innerp{B}{B_x dy \wedge dz}
            + \innerp{B}{B_y dz \wedge dx}
            + \innerp{B}{B_z dx \wedge dy} \\
        =&\ \innerp{B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy}
                   {B_x dy \wedge dz} \\
        &+ \innerp{B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy}
                  {B_y dz \wedge dx} \\
        &+ \innerp{B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy}
                  {B_z dx \wedge dy} \\
        &+ \innerp{B_z dx \wedge dy}{B_x dy \wedge dz} \\
        =&\ \innerp{B_x dy \wedge dz}{B_x dy \wedge dz} \\
        &+ \innerp{B_y dz \wedge dx}{B_y dz \wedge dx} \\
        &+ \innerp{B_z dx \wedge dy}{B_z dx \wedge dy} \\
        =&\ B_x^2 + B_y^2 + B_z^2.
\end{align*}
Alternatively, we could use the Hodge star and calculate $\innerp{\star B}{\star B}$ instead.

\end{sol}

\begin{ex}

In $\mathbb{R}^4$, let $F$ be the 2-form given by $F = B + E \wedge dt$, where $E$ and $B$ are given by \hyperref[ex:esquaredbsquared]{the formul\ae\ above}. Using the Minkowski metric on $\mathbb{R}^4$, calculate $-\frac{1}{2} \innerp{F}{F}$ and relate it to \hyperref[eq:lagrangianvacuumem]{the Lagrangian above}.

\end{ex}

\begin{sol}

The inner product of the 2-form $F$ with itself is
\begin{align*}
    \innerp{F}{F} &= \innerp{B + E \wedge dt}{B + E \wedge dt} \\
        &= \innerp{B}{B} + \innerp{B}{E \wedge dt} + \innerp{E \wedge dt}{B} + \innerp{E \wedge dt}{E \wedge dt} \\
        &= \innerp{B}{B} + \innerp{E \wedge dt}{E \wedge dt}
\end{align*}
since each component of $B$ is orthogonal to each component of $E \wedge dt$.
Focusing on the electric term,
\begin{align*}
    \innerp{E \wedge dt}{E \wedge dt} &= \det \begin{pmatrix}
                \eta(E, E) & \eta(E, dt) \\
                \eta(dt, E) & \eta(dt, dt)
            \end{pmatrix} \\
        &= -\innerp{E}{E}
\end{align*}
so
\[
    -\frac{1}{2} \innerp{F}{F} = \frac{1}{2} \bigl( \innerp{E}{E} - \innerp{B}{B} \bigr),
\]
the Lagrangian density for vacuum electromagnetism on Minkowski spacetime.

\end{sol}

\section{The Volume Form}

\begin{ex}

Show that any even permutation of a given basis has the same orientation, while any odd permutation has the opposite orientation.

\end{ex}

\begin{sol}

Let $\left\{e_\mu\right\}$ and $\left\{f_\mu\right\}$ be two bases related by $T: e_\mu \mapsto f_\mu$. We say that $\left\{e_\mu\right\}$ and $\left\{f_\mu\right\}$ have the same orientation if $\det(T) > 0$ and the opposite orientation if $\det(T) < 0$.

Permuting the basis by some permutation $\pi$ corresponds to a transformation by permutation matrix $T_\pi: e_\mu \mapsto f_\mu$. Since $\det(T_\pi) = \text{sign}(\pi)$, this preserves the orientation when $\pi$ is even and reverses it when $\pi$ is odd.

\end{sol}

\begin{ex}

Let $M$ be an oriented manifold. Show that we can cover $M$ with \emph{oriented charts} $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$, that is, charts such that the basis $dx^\mu$ of cotangent vectors on $\mathbb{R}^n$, pulled back to $U_\alpha$ by $\varphi_\alpha$, is positively oriented.

\end{ex}

\begin{sol}

Let $p \in U_\alpha$ and $\dim(M) = n$. We have an oriented chart $\varphi_\alpha: p \mapsto x^\mu(p)$ which gives us a basis $\left\{dx^\mu\right\}$ of the cotangent space $T_p^*M$.
Pulling back by $\varphi_\alpha^*$, we get a basis of $U_\alpha$, $\left\{\varphi_\alpha^*dx^\mu\right\} = \left\{d\varphi_\alpha^*x^\mu\right\}$.

The cotangent basis $\left\{dx^\mu\right\}$ admits a volume form
\[
    \omega = dx^1 \wedge \cdots \wedge dx^n.
\]
Pulling back,
\begin{align*}
    \varphi_\alpha^* \omega &= \varphi_\alpha^* (dx^1 \wedge \cdots \wedge dx^n) \\
        &= \varphi_\alpha^* dx^1 \wedge \cdots \wedge \varphi_\alpha^* dx^n \\
        &= d\varphi_\alpha^* x^1 \wedge \cdots \wedge d\varphi_\alpha^* x^n,
\end{align*}
but this is the volume form corresponding to our basis of $U_\alpha$ and is positively oriented. Since $M$ is oriented, we can cover $M$ in such charts.

\end{sol}

\begin{ex}

Given a diffeomorphism $\phi: M \to N$ from one oriented manifold to another, we say that $\phi$ is \emph{orientation-preserving} if the pullback of any right-handed basis of a cotangent space in $N$ is a right-handed basis of a cotangent space in $M$. Show that if we can cover $M$ with charts such that the transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}$ are orientation-preserving, we can make $M$ into an oriented manifold by using the charts to transfer the standard orientation on $\mathbb{R}^n$ to an orientation on $M$.

\end{ex}

\begin{sol}

Let $\dim(M) = n$ and let $p \in U_\alpha$, $q \in U_\beta$ where $U_\alpha$, $U_\beta$ are overlapping open sets with charts $\varphi_\alpha: p \mapsto \left\{x^\mu\right\}$, $\varphi_\beta: q \mapsto \left\{x'^\nu\right\}$. Each chart admits volume forms
\[
    \omega = dx^1 \wedge \cdots \wedge dx^n, \quad
    \omega' = dx'^1 \wedge \cdots \wedge dx'^n.
\]
On the overlap $U_\alpha \cap U_\beta$, we have
\[
{(\varphi_\alpha \circ \varphi_\beta^{-1})}^* dx'^\nu = T_\mu^\nu dx^\mu
\]
with the explicit representation of $T$ given by partial derivatives as per exercise~\ref{ex:pullbackdx}, so
\begin{align*}
    {(\varphi_\alpha \circ \varphi_\beta^{-1})}^* \omega' &= {(\varphi_\alpha \circ \varphi_\beta^{-1})}^* (dx'^1 \wedge \cdots \wedge dx'^n) \\
        &= {(\varphi_\alpha \circ \varphi_\beta^{-1})}^* dx'^1 \wedge \cdots \wedge {(\varphi_\alpha \circ \varphi_\beta^{-1})}^* dx'^n \\
        &= T_\mu^1 dx^\mu \wedge \cdots \wedge T_\nu^n dx^\nu \\
        &= \det(T) \, dx^1 \wedge \cdots \wedge dx^n \\
        &= \det(T) \, \omega.
\end{align*}
But since the transition function is orientation-preserving, this transfers the standard orientation on $\mathbb{R}^n$ to an orientation on $M$.

\end{sol}

\begin{ex}\label{ex:canonicalvolumeform}

Let $M$ be an oriented $n$-dimensional semi-Riemannian manifold and let $\left\{e^\mu\right\}$ be an oriented orthonormal basis of cotangent vectors\footnote{We use upper indices since we're in the cotangent space.} at some point $p \in M$. Show that
\[
    e^1 \wedge \cdots \wedge e^n = \text{vol}_p,
\]
where $\text{vol}$ is the volume form associated to the metric on $M$ and $\text{vol}_p$ is its value at $p$.

\end{ex}

\begin{sol}

The canonical volume form on $M$ with metric $g_{\mu\nu} = g(\partial_\mu, \partial_\nu)$ is given by
\[
    \text{vol} = \sqrt{|\det(g)|} \, dx^1 \wedge \cdots \wedge dx^n.
\]
We have that $e^\mu = T^\mu_\nu dx^\nu$ with $T$ as per exercise~\ref{ex:transform1form}. Taking the inner product
\begin{align*}
    \innerp{dx^\mu}{dx^\nu} &= \innerp{{(T^{-1})}^\mu_\alpha e^\alpha}{{(T^{-1})}^\nu_\beta e^\beta} \\
        &= {(T^{-1})}^\mu_\alpha {(T^{-1})}^\nu_\beta \innerp{e^\alpha}{e^\beta} \\
        &= {(T^{-1})}^\mu_\alpha {(T^{-1})}^\nu_\beta \delta^{\alpha\beta} \epsilon(\alpha) \\
        &= \pm {(T^{-1})}^\mu_\alpha {(T^{-1})}^\nu_\alpha
\end{align*}
with $\epsilon$ as per exercise~\ref{ex:innerproductpforms}.
But $\innerp{dx^\mu}{dx^\nu} = g^{\mu\nu}$, the inverse of $g_{\mu\nu}$, so
\[
g^{\mu\nu} = \pm {(T^{-1})}^\mu_\alpha {(T^{-1})}^\nu_\alpha
\]
and taking the determinant gives us $\det(T) = \sqrt{|\det(g)|}$. Then
\begin{align*}
    e^1 \wedge \cdots \wedge e^n &= \det(T) \, dx^1 \wedge \cdots \wedge dx^n \\
        &= \sqrt{|\det(g)|} \, dx^1 \wedge \cdots \wedge dx^n \\
        &= \text{vol}
\end{align*}
and, evaluated at $p$,
\[
e_p^1 \wedge \cdots \wedge e_p^n = \text{vol}_p.
\]

\end{sol}

\section{The Hodge Star Operator}

\begin{ex}

Show that if we define the Hodge star operator in a chart using this formula, it satisfies the property $\omega \wedge \star \mu = \innerp{\omega}{\mu} \, \text{vol}$. Use the result from exercise~\ref{ex:canonicalvolumeform}.

\end{ex}

\begin{sol}

Let $\left\{e^\mu\right\}$ be a positively oriented orthonormal basis on an $n$-dimensional manifold. Then we define the Hodge star operator in a chart as
\[
    \star (e^{i_1} \wedge \cdots \wedge e^{i_p}) = \pm e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n}
\]
where the sign is determined by $\text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p)$.

$p$-forms $\omega = \omega_I e^I$ and $\mu = \mu_J e^J$ in terms of basis 1-forms are
\[
    \omega = \omega_{i_1 \cdots i_p} e^{i_1} \wedge \cdots \wedge e^{i_p}, \quad
    \mu = \mu_{j_1 \cdots j_p} e^{j_1} \wedge \cdots \wedge e^{j_p}.
\]
Taking the inner product,
\begin{align*}
    \innerp{\omega}{\mu} &= \omega_I \mu_J \innerp{e^{i_1} \wedge \cdots \wedge e^{i_p}}{e^{j_1} \wedge \cdots \wedge e^{j_p}} \\
        &= \omega_I \mu_J \, \det \bigl(g(e^{i_k}, e^{j_k})\bigr) \\
        &= \omega_I \mu_J \delta^{IJ} \epsilon(i_1) \cdots \epsilon(i_p)
\end{align*}
where we denote $\delta^{IJ} = \delta^{i_1 j_1} \cdots \delta^{i_p j_p}$.

The Hodge dual of $\mu$ is
\[
    \star \mu = \pm \mu_J e^{j_{p + 1}} \wedge \cdots \wedge e^{j_n}
\]
and so
\[
    \omega \wedge \star \mu = \pm \omega_I \mu_J e^{i_1} \wedge \cdots \wedge e^{i_p} \wedge e^{j_{p + 1}} \wedge \cdots \wedge e^{j_n}.
\]
Notice that this will vanish if any basis elements $e^{i_k}$ of $\omega$ are equal to any basis elements $e^{j_l}$ of $\star \mu$ by antisymmetry or, by Hodge duality, are not equal to any basis element $e^{j_l}$ of $\mu$.
Then,
\begin{align*}
    \omega \wedge \star \mu &= \pm \omega_I \mu_J \delta^{IJ} e^{i_1} \wedge \cdots \wedge e^{i_p} \wedge e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n} \\
        &= \pm \omega_I \mu_J \delta^{IJ} e^{i_1} \wedge \cdots \wedge e^{i_n} \\
        &= \text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \omega_I \mu_J \delta^{IJ} e^{i_1} \wedge \cdots \wedge e^{i_n} \\
        &= {\text{sign}(i_1, \ldots, i_n)}^2 \epsilon(i_1) \cdots \epsilon(i_p) \omega_I \mu_J \delta^{IJ} e^1 \wedge \cdots \wedge e^n \\
        &= \omega_I \mu_J \delta^{IJ} \epsilon(i_1) \cdots \epsilon(i_p) e^1 \wedge \cdots \wedge e^n \\
        &= \innerp{\omega}{\mu} \, \text{vol}.
\end{align*}

\end{sol}

\begin{ex}

Calculate $\star d\omega$ when $\omega$ is a 1-form on $\mathbb{R}^3$.

\end{ex}

\begin{sol}\label{sol:curl1form}

Denote $\omega = \omega_x dx + \omega_y dy + \omega_z dz$. The gradient is
\begin{align*}
    d\omega &= d(\omega_x dx + \omega_y dy + \omega_z dz) \\
        &= d(\omega_x dx) + d(\omega_y dy) + d(\omega_z dz) \\
        &= d\omega_x \wedge dx + d\omega_y \wedge dy + d\omega_z \wedge dz \\
        &= \partial_y \omega_x dy \wedge dx + \partial_z \omega_x dz \wedge dx \\
        &\mathrel{\phantom{=}}{} + \partial_x \omega_y dx \wedge dy + \partial_z \omega_y dz \wedge dy \\
        &\mathrel{\phantom{=}}{} + \partial_x \omega_z dx \wedge dz + \partial_y \omega_z dy \wedge dz \\
        &= (\partial_y \omega_z - \partial_z \omega_y) \, dy \wedge dz \\
        &\mathrel{\phantom{=}}{} + (\partial_z \omega_x - \partial_x \omega_z) \, dz \wedge dx \\
        &\mathrel{\phantom{=}}{} +(\partial_x \omega_y - \partial_y \omega_x) \, dx \wedge dy.
\end{align*}
Then the Hodge dual of $d\omega$ is
\begin{align*}
    \star d\omega &= (\partial_y \omega_z - \partial_z \omega_y) \star (dy \wedge dz) \\
        &\mathrel{\phantom{=}}{} + (\partial_z \omega_x - \partial_x \omega_z) \star (dz \wedge dx) \\
        &\mathrel{\phantom{=}}{} + (\partial_x \omega_y - \partial_y \omega_x) \star (dx \wedge dy) \\
        &= \left(\partial_y \omega_z - \partial_z \omega_y\right) \, dx
            + \left(\partial_z \omega_x - \partial_x \omega_z\right) \, dy
            + \left(\partial_x \omega_y - \partial_y \omega_x\right) \, dz,
\end{align*}
analogous to the curl of $\omega$.

\end{sol}

\begin{ex}

Calculate $\star d \star \omega$ when $\omega$ is a 1-form on $\mathbb{R}^3$.

\end{ex}

\begin{sol}\label{sol:divergence1form}

Denote $\omega = \omega_x dx + \omega_y dy + \omega_z dz$. The Hodge dual is
\begin{align*}
    \star \omega &= \star (\omega_x dx + \omega_y dy + \omega_z dz) \\
        &= \omega_x dy \wedge dz + \omega_y dz \wedge dx + \omega_z dx \wedge dy.
\end{align*}
The gradient of the Hodge dual is then
\begin{align*}
    d \star \omega &= d(\omega_x dy \wedge dz + \omega_y dz \wedge dx + \omega_z dx \wedge dy) \\
        &= d(\omega_x dy \wedge dz) + d(\omega_y dz \wedge dx) + d(\omega_z dx \wedge dy) \\
        &= d\omega_x \wedge dy \wedge dz + d\omega_y \wedge dz \wedge dx + d\omega_z \wedge dx \wedge dy \\
        &= \partial_x \omega_x dx \wedge dy \wedge dz + \partial_y \omega_y dy \wedge dz \wedge dx + \partial_z \omega_z dz \wedge dx \wedge dy \\
        &= (\partial_x \omega_x + \partial_y \omega_y + \partial_z \omega_z) \, dx \wedge dy \wedge dz.
\end{align*}
Taking the Hodge dual of this gives
\[
    \star d \star \omega = \partial_x \omega_x + \partial_y \omega_y + \partial_z \omega_z,
\]
analogous to the divergence of $\omega$.

\end{sol}

\begin{ex}

Give $\mathbb{R}^4$ the Minkowski metric and the orientation in which $(dt, dx, dy, dz)$ is positively oriented. Calculate the Hodge star operator on all wedge products of $dx^\mu$s. Show that on $p$-forms,
\[
    \star^2 = {(-1)}^{p (4 - p) + 1}.
\]

\end{ex}

\begin{sol}

The Hodge dual of the 0-form is $\star 1 = dt \wedge dx \wedge dy \wedge dz = \text{vol}$.

For the 1-forms,
\begin{align*}
    \star dt &= - dx \wedge dy \wedge dz, &
    \star dx &= - dy \wedge dz \wedge dt, \\
    \star dy &= dz \wedge dt \wedge dx, &
    \star dz &= - dt \wedge dx \wedge dy.
\end{align*}
The 2-forms,
\begin{align*}
    \star (dt \wedge dx) &= - dy \wedge dz, &
    \star (dx \wedge dy) &= dt \wedge dz, \\
    \star (dt \wedge dy) &= - dz \wedge dx, &
    \star (dx \wedge dz) &= - dt \wedge dy, \\
    \star (dt \wedge dz) &= - dx \wedge dy, &
    \star (dy \wedge dz) &= dt \wedge dx.
\end{align*}
The 3-forms,
\begin{align*}
    \star(dt \wedge dx \wedge dy) &= - dz, &
    \star(dx \wedge dy \wedge dz) &= - dt, \\
    \star(dy \wedge dz \wedge dt) &= - dx, &
    \star(dz \wedge dt \wedge dx) &= dy.
\end{align*}
Lastly, the Hodge dual of the volume form is $\star (dt \wedge dx \wedge dy \wedge dz) = -1$.

Since all $p$-forms can be written as a linear combination of all wedge products, we can see that $\star^2 = {(-1)}^{p (4 - p) + 1}$ holds by inspection.

\end{sol}

\begin{ex}

Let $M$ be an oriented semi-Riemannian manifold of dimension $n$ and signature $(n-s, s)$. Show that on $p$-forms,
\[
    \star^2 = {(-1)}^{p (n - p) + s}.
\]

\end{ex}

\begin{sol}

Let $\omega = \omega_I e^{i_1} \wedge e^{i_p}$ be a $p$-form on $M$. Then
\[
    \star \omega = \text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \omega_I e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n}
\]
so
\begin{align*}
    \SwapAboveDisplaySkip
    \star^2 \omega &= \text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \star (\omega_I e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n}) \\
        &= \text{sign}(i_1, \ldots, i_n) \text{sign}(i_{p + 1}, \ldots, i_n, i_1, \ldots, i_p) \epsilon(i_1) \cdots \epsilon(i_n) \omega \\
        &= {(-1)}^{p (n - p)} {(-1)}^s \omega \\
        &= {(-1)}^{p (n - p) + s} \omega.
\end{align*}

\end{sol}

\begin{ex}

Let $M$ be an oriented semi-Riemannian manifold of dimension $n$ and signature $(s, n - s)$. Let $e^\mu$ be an orthonormal basis of 1-forms on some chart. Define the \emph{Levi--Civita symbol} for $1 \leq i_j \leq n$ by
\[
    \epsilon_{i_1 \cdots i_n} = \begin{dcases*}
            \text{sign}(i_1, \ldots, i_n) & \text{all $i_j$ distinct}, \\
            0                             & otherwise.
        \end{dcases*}
\]
Show that for any $p$-form
\[
    \omega = \frac{1}{p!} \omega_{i_1 \cdots i_p} e^{i_1} \wedge \cdots \wedge e^{i_p}
\]
we have
\[
    {(\star \omega)}_{j_1 \cdots j_{n - p}} = \frac{1}{p!} {\epsilon^{i_1 \cdots i_p}}_{j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p}.
\]

\end{ex}

\begin{sol}

Taking the Hodge dual of $\omega$,
\begin{align*}
    \star \omega &= \frac{1}{p!} \text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \omega_{i_1 \cdots i_p} e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n} \\
        &= \frac{1}{p!} \text{sign}(i_1, \ldots, i_p, i_{p + 1}, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \omega_{i_1 \cdots i_p} e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n}.
\end{align*}
We're free to rename $i_{p + 1}, \ldots, i_n$ to $j_1, \ldots, j_{n - p}$ and, if we use the Levi--Civita symbol,
\begin{align*}
    \star \omega &= \frac{1}{p!} \epsilon(i_1) \cdots \epsilon(i_p) \epsilon_{i_1 \cdots i_p j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}} \\
        &= \frac{1}{p!} \epsilon(i_1) \cdots \epsilon(i_p) \delta^{i_1 k_1} \cdots \delta^{i_p k_p} \epsilon_{k_1 \cdots k_p j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}} \\
        &= \frac{1}{p!} g^{i_1 k_1} \cdots g^{i_p k_p} \epsilon_{k_1 \cdots k_p j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}} \\
        &= \frac{1}{p!} {\epsilon^{i_1 \cdots i_p}}_{j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}}
\end{align*}
so if $\star \omega$ in terms of components is
\[
    \star \omega = {(\star \omega)}_{j_1 \cdots j_{n - p}} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}},
\]
then
\[
    {(\star \omega)}_{j_1 \cdots j_{n - p}} = \frac{1}{p!} {\epsilon^{i_1 \cdots i_p}}_{j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p}.
\]

\end{sol}

\section{The Second Pair of Equations}\label{sec:secondpairmaxwellequations}

\begin{ex}

Check this result.

\end{ex}

\begin{sol}

The claim is that on Minkowski space, the second pair of Maxwell equations,
\[
    \nabla \cdot \vec{E} = \rho, \qquad
    \nabla \times \vec{B} - \frac{\partial \vec{E}}{\partial t} = \vec{\jmath},
\]
can be rewritten as
\[
    \star_S \, d_S \star_S E = \rho, \qquad
    -\partial_t E + \star_S \, d_S \star_S B = j
\]
where $\star_S$ denotes the Hodge star operator on space, that is, $\mathbb{R}^3$ with its usual Euclidean metric.

Since $E = E_x dx + E_y dy + E_z dz$ is a 1-form, we have from solution~\ref{sol:divergence1form} that
\begin{align*}
    \star_S \, d_S \star_S E &= \partial_x E_x + \partial_y E_y + \partial_z E_z \\
                             &= \nabla \cdot \vec{E} \\
                             &= \rho.
\end{align*}

Consider now the Hodge dual in space of the 2-form $B$,
\begin{align*}
    \star_S B &= \star_S (B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy) \\
              &= B_x dx + B_y dy + B_z dz.
\end{align*}
From solution~\ref{sol:curl1form}, we get
\begin{align*}
    \star_S \, d_S \star_S B &= \star_S \, d_S (B_x dx + B_y dy + B_z dz) \\
        &= \left(\partial_y B_z - \partial_z B_y\right) dx
            + \left(\partial_z B_x - \partial_x B_z\right) dy
            + \left(\partial_x B_y - \partial_y B_x\right) dz \\
        &= {(\nabla \times \vec{B})}_i dx^i.
\end{align*}
Since we're in Euclidean space, we can turn vector fields into 1-forms easily. As in exercise~\ref{ex:raisingindex},
\begin{gather*}
    g(\star_S \, d_S \star_S B, \cdot)
        = \delta^{ij} {(\nabla \times \vec{B})}_i \partial_j
        = \nabla \times \vec{B}, \\
    g(-\partial_t E, \cdot) = -\delta^{ij} \partial_t E_i \partial_j = -\partial_t \vec{E}
\end{gather*}
and $g(j, \cdot) = \vec{\jmath}$,
so $-\partial_t E + \star_S \, d_S \star_S B = j$ is component-by-component equivalent to the last Maxwell equation.

\end{sol}

\begin{ex}

Check the calculations above.

\end{ex}

\begin{sol}\label{sol:secondpair}

Assume that $M = \mathbb{R} \times S$ is an oriented semi-Riemannian manifold where $S$ is space and let the current be given by $J = j - \rho \, dt$. Suppose $\dim(S) = 3$ and the metric is static and of the form $g = -dt^2 + ^3 g$ where $^3 g$ is a Riemannian metric on $S$. We want to show that
\[
    \star d \star F = J
\]
is equivalent to the second pair of Maxwell equations.

Taking the Hodge dual of the electromagnetic 2-form,
\[
    \star F = \star B + \star (E \wedge dt)
\]
and looking at the electric and magnetic terms separately, we get
\begin{align*}
    \star (E \wedge dt) &= \star (E_x dx \wedge dt + E_y dy \wedge dt + E_z dz \wedge dt) \\
        &= E_x dy \wedge dz + E_y dz \wedge dx + E_z dx \wedge dy \\
        &= \star_S E
\end{align*}
and
\begin{align*}
    \SwapAboveDisplaySkip
    \star B &= \star (B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy) \\
            &= B_x dt \wedge dx + B_y dt \wedge dy + B_x dt \wedge dz \\
            &= - B_x dx \wedge dt - B_y dy \wedge dt - B_z dz \wedge dt \\
            &= - \star_S B \wedge dt
\end{align*}
so
\[
    \star F = \star_S E - \star_S B \wedge dt.
\]
The exterior derivative of this is then
\[
    d \star F = d \star_S E - d (\star_S B \wedge dt)
\]
and we again look at the electric and magnetic terms separately to get
\begin{align*}
     d (\star_S B \wedge dt) &= dt \wedge \partial_t \star_S B \wedge dt + d_S \star_S B \wedge dt \\
        &= d_S \star_S B \wedge dt.
\end{align*}
and
\begin{align*}
    \SwapAboveDisplaySkip
    d \star_S E &= dt \wedge \partial_t \star_S E + d_S \star_S E \\
        &= \partial_t \star_S E \wedge dt + d_S \star_S E \\
        &= \star_S \partial_t E \wedge dt + d_S \star_S E
\end{align*}
by making use of the result from exercise~\ref{ex:pformspaceandtime} and reversing the exterior product without a sign change since $\star_S E$ is a 2-form, so
\[
    d \star F = \star_S \partial_t E \wedge dt + d_S \star_S E - d_S \star_S B \wedge dt.
\]
Applying the Hodge star to each term, for $\star_S \partial_t E \wedge dt$ we get
\begin{align*}
    \star (\star_S \partial_t E \wedge dt) &= \star \bigl(
            \partial_t E_x dy \wedge dz \wedge dt \\
            &\phantom{= \star \,\, } + \partial_t E_y dz \wedge dx \wedge dt \\
            &\phantom{= \star \,\, } + \partial_t E_z dx \wedge dy \wedge dt \bigr) \\
        &= - \partial_t E,
\end{align*}
for the 3-form on space $d_S \star_S E$ we get
\[
    \star d_S \star_S E = - \star_S d_S \star_S E \wedge dt
\]
and for the 3-form on spacetime $d_S \star_S B \wedge dt$ we get
\[
    \star d_S \star_S B \wedge dt = - \star_S d_S \star_S B.
\]
Combining,
\[
    \star d \star F = - \partial_t E - \star_S d_S \star_S E \wedge dt + \star_S d_S \star_S B.
\]
But $\star d \star F = J$, so
\[
    - \partial_t E - \star_S d_S \star_S E \wedge dt + \star_S d_S \star_S B = j - \rho \, dt
\]
and equating coefficients gives us
\[
    \star_S d_S \star_S E = \rho, \qquad
    - \partial_t E + \star_S d_S \star_S B = j.
\]

\end{sol}

\begin{ex}

Show this is true if we take
\[
    F_\pm = \frac{1}{2} (F \pm \star F).
\]

\end{ex}

\begin{sol}

On a 4-dimensional Riemannian manifold $M$, we say $F \in \Omega^2(M)$ is \emph{self-dual} if $\star F = F$ and \emph{anti-self-dual} if $\star F = - F$.
Since $\star^2 = 1$ it is not surprising that the Hodge star operator has eigenvalues $\pm 1$. That is, we can write any $F \in \Omega^2(M)$ as a sum of self-dual and anti-self-dual parts
\[
    F = F_+ + F_-, \qquad
    \star F_\pm = \pm F_\pm.
\]
Take $F_\pm$ as above. Then
\[
    F_+ + F_- = \frac{1}{2}(F + \star F + F - \star F) = F
\]
and
\begin{align*}
    \SwapAboveDisplaySkip
    \star F_\pm &= \frac{1}{2} (\star F \pm \star^2 F) \\
                &= \frac{1}{2} (\pm F + \star F) \\
                &= \pm \frac{1}{2} (F \pm \star F) \\
                &= \pm F_\pm.
\end{align*}

\end{sol}

\begin{ex}

Show that this result is true.

\end{ex}

\begin{sol}

In the Lorentzian case things are not quite as nice, since $\star^2 = -1$ implies its eigenvalues are $\pm i$. This means that we should really consider complex-valued differential forms on $M$. If we do that, we can write any $F \in \Omega^2(M)$ as
\[
    F = F_+ + F_-, \qquad
    \star F_\pm = \pm i F_\pm.
\]
Try
\[
    F_\pm = \frac{1}{2} (F \mp \star i F).
\]
Then
\[
    F_+ + F_- = \frac{1}{2} (F - \star i F + F + \star i F) = F
\]
and
\begin{align*}
    \SwapAboveDisplaySkip
    \star F_\pm &= \frac{1}{2}(\star F \mp \star^2 i F) \\
                &= \frac{1}{2}(\star F \pm i F) \\
                &= \frac{1}{2}(\pm i F + \star F) \\
                &= \frac{i}{2}(\pm F - \star i F) \\
                &= \pm \frac{i}{2}(F \mp \star i F) \\
                &= \pm i F_\pm.
\end{align*}

\end{sol}

\begin{ex}\label{ex:ebselfdual}

Show that these equations are equivalent, and both hold if at every time $t$ we have
\begin{gather*}
    E = E_1 dx^1 + E_2 dx^2 + E_3 dx^3, \\
    B = -i (E_1 dx^2 \wedge dx^3 + \text{cyclic permutations}).
\end{gather*}

\end{ex}

\begin{sol}

The electromagnetic 2-form $F = B + E \wedge dt$ has Hodge dual
\[
    \star F = \star_S E - \star_S B \wedge dt,
\]
so $F$ will be self-dual if
\[
    \star_S E = iB, \qquad \star_S B = -iE.
\]
These two equations are equivalent, as taking the Hodge dual of the first yields
\[
    \star_S^2 E = \star_S iB,
\]
but $\star_S^2 E = E$, implying $\star_S iB = E$, which requires that $\star_S B = -iE$.

For $E$ and $B$ as given,
\begin{align*}
    \star_S E &= \star_S (E_1 dx^1 + E_2 dx^2 + E_3 dx^3) \\
        &= E_1 dx^2 \wedge dx^3 + E_2 dx^3 \wedge dx^1 + E_3 dx^1 \wedge dx^2 \\
        &= iB
\end{align*}
and, although already implied,
\begin{align*}
    \star_S B &= -i \star_S (E_1 dx^2 \wedge dx^3 + \text{cyclic permutations}) \\
              &= -i (E_1 dx^1 + E_2 dx^2 + E_3 dx^3) \\
              &= -iE.
\end{align*}

\end{sol}

\begin{ex}\label{ex:planewavemaxwelleq2}

Check the above result.

\end{ex}

\begin{sol}

We are assuming $F$ is self-dual and and that $E$ is a \emph{plane wave} of the form
\[
    E(x) = \mathbf{E} e^{i k_\mu x^\mu}
\]
where $\mathbf{E} = \mathbf{E}_j dx^j$ is a constant complex-valued 1-form on $\mathbb{R}^3$
and $k \in {\Vect(\mathbb{R}^4)}^*$ is the fixed \emph{energy-momentum} covector.
By self-duality, we have
\[
    B(x) = \mathbf{B} e^{i k_\mu x^\mu}
\]
where $\mathbf{B} = -i \star_S \mathbf{E}$.
Let us write $^3 k$ for $k_j dx^j$, the \emph{momentum} of the plane wave.
Then\footnote{Note the $i$, missing in the text.}
\[
    d_S e^{i k_\mu x^\mu} = ie^{i k_\mu x^\mu} \, ^3 k.
\]
The second Maxwell equation, $\partial_t B + d_S E = 0$, turns into
\[
    \partial_t \mathbf{B} e^{i k_\mu x^\mu} + d_S \mathbf{E} e^{i k_\mu x^\mu} = 0.
\]
But
\[
    \partial_t \mathbf{B} e^{i k_\mu x^\mu} = -ik_0 \mathbf{B} e^{i k_\mu x^\mu}
\]
since we're on a Lorentzian manifold and
\begin{align*}
    d_S \mathbf{E} e^{i k_\mu x^\mu} &= {(-1)}^1 \mathbf{E} \wedge d_S e^{i k_\mu x^\mu} \\
        &= -\mathbf{E} \wedge {}^3 k \, i e^{i k_\mu x^\mu} \\
        &= i e^{i k_\mu x^\mu} \, ^3 k \wedge \mathbf{E}
\end{align*}
since both $\mathbf{E}$ and $^3 k$ are 1-forms.
This gives us
\begin{align*}
    -ik_0 \mathbf{B} e^{i k_\mu x^\mu} + i e^{i k_\mu x^\mu} \, ^3 k \wedge \mathbf{E} &= 0 \\
    -k_0 \mathbf{B} + {}^3 k \wedge \mathbf{E} &= 0 \\
    ^3 k \wedge \mathbf{E} &= k_0 \mathbf{B}.
\end{align*}

\end{sol}

\begin{ex}

Show [that] this equation implies $k_\mu k^\mu = 0$. Thus the energy-momentum of light is light-like!

\end{ex}

\begin{sol}

% It feels like there should be a more elegant geometrical way to do this,
% without breaking out the components.

Using the result from exercise~\ref{ex:planewavemaxwelleq2} and the relationship between $E$ and $B$ when $F$ is self-dual from exercise~\ref{ex:ebselfdual}, we get
\[
    ^3 k \wedge \mathbf{E} = k_0 \mathbf{B} = -ik_0 \star_S \mathbf{E}
\]
and, rearranging,
\[
    ik_0 \star_S \mathbf{E} + {}^3 k \wedge \mathbf{E} = 0.
\]
In terms of components,
\begin{align*}
    ik_0 \star_S \mathbf{E} + {}^3 k \wedge \mathbf{E}
        &= ik_0 (\mathbf{E}_x dy \wedge dz
                 + \mathbf{E}_y dz \wedge dx
                 + \mathbf{E}_z dx \wedge dy) \\
        &\mathrel{\phantom{=}}{} + k_i \mathbf{E}_j dx^i \wedge dx^j \\
        &= ik_0 (\mathbf{E}_x dy \wedge dz
                 + \mathbf{E}_y dz \wedge dx
                 + \mathbf{E}_z dx \wedge dy) \\
        &\mathrel{\phantom{=}}{} + (k_x \mathbf{E}_y - k_y \mathbf{E}_x) \, dx \wedge dy \\
        &\mathrel{\phantom{=}}{} + (k_y \mathbf{E}_z - k_z \mathbf{E}_y) \, dy \wedge dz \\
        &\mathrel{\phantom{=}}{} + (k_z \mathbf{E}_x - k_x \mathbf{E}_z) \, dz \wedge dx.
\end{align*}
Equating coefficients, we get the homogeneous system
\begin{align*}
    ik_0 \mathbf{E}_x + k_z \mathbf{E}_y - k_y \mathbf{E}_z &= 0, \\
    -k_z \mathbf{E}_x + ik_0 \mathbf{E}_y + k_x \mathbf{E}_z &= 0, \\
    k_y \mathbf{E}_x - k_x \mathbf{E}_y + ik_0 \mathbf{E}_z &= 0
\end{align*}
which is equivalent to $K_{ij} \mathbf{E}_j = 0$ for the skew-Hermitian matrix
\[
    K = \begin{pmatrix*}[r]
            ik_0 & k_z & -k_y \\
            -k_z & ik_0 & k_x \\
            k_y & -k_x & ik_0
        \end{pmatrix*}
\]
with determinant
\begin{align*}
    \det(K) &= ik_0 (ik_0 \cdot ik_0 + k_x^2) - k_z(-ik_0 k_z - k_x k_y) - k_y (k_x k_z - ik_0 k_y) \\
            %&= -i k_0^3 + ik_0 k_x^2 + ik_0 k_z^2 + k_z k_x k_y - k_y k_x k_z + ik_0 k_y^2 \\
            &= -i k_0^3 + ik_0 k_x^2 + ik_0 k_y^2 + ik_0 k_z^2.
\end{align*}
Since we require our electric field to be non-trivial, $\det(K) = 0$ which implies
\[
    -k_0^2 + k_x^2 + k_y^2 + k_z^2 = 0.
\]
Therefore $k_\mu k^\mu = 0$ and thus the energy-momentum of light is light-like.

\end{sol}

\begin{ex}

Check the above result.

\end{ex}

\begin{sol}

A simple self-dual solution to the vacuum Maxwell equations is
\[
    k = dt - dx, \qquad \mathbf{E} = dy - i \, dz.
\]
This holds since
\begin{align*}
    -i k_0 \star_S \mathbf{E} &= -i k_0 (\star_S dy - i \star_S dz) \\
    &= -i k_0 (dz \wedge dx - i\, dx \wedge dy) \\
    &= -i \, dz \wedge dx - dx \wedge dy,
\end{align*}
so
\[
    ^3 k \wedge \mathbf{E} = -dx \wedge dy + i \, dx \wedge dz = -ik_0 \star_S \mathbf{E}
\]
as required.

From $\star_S E = iB$,
\begin{align*}
    \SwapAboveDisplaySkip
    \mathbf{B} &= -i \star_S \mathbf{E} \\
               &= -i \, dz \wedge dx - dx \wedge dy.
\end{align*}
Since $k_\mu x^\mu = t - x$, this gives us
\begin{align*}
    E(x) &= (dy - i\, dz) e^{i (t - x)}, \\
    B(x) &= (-i \, dz \wedge dx - dx \wedge dy) e^{i (t - x)}
\end{align*}
or, in old-fashioned language,
\[
    \vec{E} = \bigl( 0, e^{i(t - x)}, -ie^{i(t - x)} \bigr), \qquad
    \vec{B} = \bigl( 0, -i e^{i(t - x)}, -e^{i(t - x)} \bigr).
\]

Write
\begin{align*}
    \SwapAboveDisplaySkip
    \vec{\mathcal{E}} &= \vec{E} + i \vec{B} \\
                      &= \bigl( 0, 2 e^{i(t - x)}, -2 i e^{i(t - x)} \bigr)
\end{align*}
which, recall from exercise~\ref{ex:i1}, lets us express the vacuum equations as
\[
    \nabla \cdot \vec{\mathcal{E}} = 0, \qquad
    \nabla \times \vec{\mathcal{E}} = i \frac{\partial \vec{\mathcal{E}}}{\partial t}.
\]
To show that our circularly-polarised plane waves are solutions, check
\[
    \nabla \cdot \vec{\mathcal{E}} = 2 \partial_y e^{i(t - x)}
                                      -2 i \partial_z e^{i(t - x)}
                                   = 0
\]
and
\begin{align*}
    \nabla \times \vec{\mathcal{E}} &= (\partial_y \mathcal{E}_z - \partial_z \mathcal{E}_y) \vec{\imath}
            + (\partial_z \mathcal{E}_x - \partial_x \mathcal{E}_z) \vec{\jmath}
            + (\partial_x \mathcal{E}_y - \partial_y \mathcal{E}_x) \vec{k} \\
        &= \bigl( 0, -2 e^{i(t - x)}, 2 i e^{i(t - x)} \bigr) \\
        &= - \vec{\mathcal{E}} \\
        &= i \partial_t \vec{\mathcal{E}}
\end{align*}
as $\partial_t \vec{\mathcal{E}} = i \vec{\mathcal{E}}$.

\end{sol}

\begin{ex}

Prove that all self-dual and anti-self-dual plane wave solutions are left and right circularly polarized, respectively.

\end{ex}

\begin{sol}

When $F$ is self-dual, Maxwell's vacuum equations for plane waves reduce to $B \wedge {}^3 k = 0$ and $^3 k \wedge E = -ik_0 \star_S E$. From the former, we also get by self-duality that $\innerp{E}{^3 k} = 0$.

Consider plane waves moving, without loss of generality, in the $x$-direction, so
\[
    k = k_0 dt - k_1 dx, \qquad
    \mathbf{E} = \mathbf{E}_2 dy + \mathbf{E}_3 dz.
\]
Then
\begin{align*}
    ^3 k \wedge \mathbf{E} &= -k_1 dx \wedge (\mathbf{E}_2 dy + \mathbf{E}_3 dz) \\
        &= -k_1 \mathbf{E}_2 dx \wedge dy - k_1 \mathbf{E}_3 dx \wedge dz
\end{align*}
and
\[
    \star_S \mathbf{E} = \mathbf{E}_2 dz \wedge dx + \mathbf{E}_3 dx \wedge dy,
\]
so $^3 k \wedge \mathbf{E} = -ik_0 \star_S \mathbf{E}$ requires
\[
    \begin{pmatrix*}[r]
        -k_1  & ik_0 \\
        -ik_0 & -k_1
    \end{pmatrix*}
    \begin{pmatrix}
        \mathbf{E}_2 \\ \mathbf{E}_3
    \end{pmatrix}
    = 0.
\]
For non-trivial solutions, $k_1^2 - k_2^2 = 0$ or $k_0 = \pm k_1$. Assuming without loss of generality that $k_0 = k_1$ (forward propagation), $\mathbf{E}_3 = -i \mathbf{E}_2$ and so, letting $\mathbf{E}_2 \equiv \mathbf{E}_0$ (since we have run out of ways of writing the letter ``E''),
\[
    \mathbf{E} = \mathbf{E}_0 (dy -i \, dz), \qquad
    k = k_0 (dt - dx).
\]
Using the self-dual relationship $B = -i \star_S E$, in old-fashioned language we get
\[
    \vec{E} = \mathbf{E}_0 \bigl( 0, e^{ik_0 (t - x)}, -ie^{ik_0 (t - x)} \bigr), \quad
    \vec{B} = \mathbf{E}_0 \bigl( 0, -ie^{ik_0 (t - x)}, -e^{ik_0 (t - x)} \bigr)
\]
and taking the real solutions only,
\begin{align*}
    \vec{E} &= \mathbf{E}_0 (0, \cos(k_0 (t - x)), \sin(k_0 (t - x))), \\
    \vec{B} &= \mathbf{E}_0 (0, \sin(k_0 (t - x)), -\cos(k_0 (t - x)))
\end{align*}
so all self-dual plane wave solutions to the vacuum equations are left circularly polarized.

When $F$ is anti-self-dual,
\[
    \star_S E - \star_S B \wedge dt = -iB - iE \wedge dt
\]
giving
\[
    \star_S E = -i B, \qquad
    \star_S B = iE,
\]
so
\begin{align*}
    \SwapAboveDisplaySkip
    ^3k \wedge E &= k_0 B \\
                 &= ik_0 \star_S E
\end{align*}
and, just as in the self-dual case, $B \wedge {}^3 k = 0$ implies that $\innerp{E}{^3k} = 0$.

Consider again plane waves moving, without loss of generality, in the $x$-direction, so
\[
    k = k_0 dt - k_1 dx, \qquad
    \mathbf{E} = \mathbf{E}_2 dy + \mathbf{E}_3 dz.
\]
Then $^3k \wedge \mathbf{E} = ik_0 \star_S \mathbf{E}$ requires
\[
    \begin{pmatrix*}[r]
        -k_1  & -ik_0 \\
        ik_0 & -k_1
    \end{pmatrix*}
    \begin{pmatrix}
        \mathbf{E}_2 \\ \mathbf{E}_3
    \end{pmatrix}
    = 0.
\]
Take $k_0 = k_1$ (forward propagation) to get $\mathbf{E}_2 = -i \mathbf{E}_3$ and so, letting $\mathbf{E}_3 \equiv \mathbf{E}_0$ this time,
\[
    \mathbf{E} = \mathbf{E}_0 (-i \, dy + dz), \qquad
    k = k_0 (dt - dx).
\]
Using the anti-self-dual relationship $B = i \star_S E$, in old-fashioned language we get
\[
    \vec{E} = \mathbf{E}_0 \bigl( 0, -ie^{ik_0 (t - x)}, e^{ik_0 (t - x)} \bigr), \quad
    \vec{B} = \mathbf{E}_0 \bigl( 0, e^{ik_0 (t - x)}, ie^{ik_0 (t - x)} \bigr)
\]
and taking the real solutions only,
\begin{align*}
    \vec{E} &= \mathbf{E}_0 (0, \sin(k_0 (t - x)), \cos(k_0 (t - x))), \\
    \vec{B} &= \mathbf{E}_0 (0, \cos(k_0 (t - x)), -\sin(k_0 (t - x)))
\end{align*}
so all anti-self-dual plane wave solutions to the vacuum equations are right circularly polarized.

\end{sol}

\begin{ex}

Let $P: \mathbb{R}^4 \to \mathbb{R}^4$ be parity transformation, that is,
\[
    P(t, x, y, z) = (t, -x, -y, -z).
\]
Show that if $F$ is a self-dual solution of Maxwell's equations, the pullback $P^* F$ is an anti-self-dual solution, and vice versa.

\end{ex}

\begin{sol}

From solution~\ref{sol:parity1form2form},
\[
    P^* E = -E, \qquad
    P^* B = B.
\]
The pullback of $F$ is therefore
\begin{align*}
    P^* F &= P^* B + P^* (E \wedge dt) \\
          &= B - E \wedge dt.
\end{align*}
Taking the Hodge dual and reusing some calculations from solution~\ref{sol:secondpair},
\begin{align*}
    \star(P^* F) &= \star B - \star(E \wedge dt) \\
                 &= -\star_S E - \star_S B \wedge dt.
\end{align*}
If $F$ is self-dual, $\star_S E = iB$, $\star_S B = -i E$ and
\begin{align*}
    \star(P^* F) &= -iB + iE \wedge dt \\
                 &= -i P^* F.
\end{align*}
Since $P^* P^* F = F$, we automatically get the corollary that if $F$ is anti-self-dual then $P^* F$ is self-dual.

\end{sol}

\chapter{De Rham Theory in Electromagnetism}

\begin{epigraph}
    I was at first almost frightened when I saw such mathematical force made to bear upon the subject, and then wondered to see that the subject stood it so well.
\end{epigraph}

\section{Closed and Exact 1-forms}

\begin{ex}

Show that this 1-form $E$ is closed. Show that $\int_{\gamma_0} E = -\pi$ and $\int_{\gamma_1} E = \pi$.

\end{ex}

\begin{sol}\label{sol:closedbutpathdependent}

The 1-form in question is defined on $\mathbb{R}^2 - \{0\}$ as
\[
    E = \frac{x \, dy - y \, dx}{x^2 + y^2}.
\]
The paths $\gamma_0, \gamma_1 : [0, 1] \to S^1 \subset \mathbb{R}^2$ describe the upper and lower half circle of radius 1 centered at the origin with $\gamma_0(0) = \gamma_1(0) = (-1, 0)$ and $\gamma_0(1) = \gamma_1(1) = (1, 0)$.

Denote $r^2 = x^2 + y^2$. The differential of $E$ is
\begin{align*}
    dE &= d \left(\frac{x}{r^2}\right) \wedge dy
          - d \left(\frac{y}{r^2} \right)\wedge dx \\
       &= \left(\frac{\partial}{\partial x} \frac{x}{r^2} dx
                + \frac{\partial}{\partial y} \frac{x}{r^2} dy\right) \wedge dy
          - \left(\frac{\partial}{\partial x} \frac{y}{r^2} dx
                  + \frac{\partial}{\partial y} \frac{y}{r^2} dy\right) \wedge dx \\
       &= \frac{\partial}{\partial x} \frac{x}{r^2} dx \wedge dy
          - \frac{\partial}{\partial y} \frac{y}{r^2} dy \wedge dx \\
       &= \frac{y^2 - x^2}{r^4} dx \wedge dy - \frac{x^2 - y^2}{r^4} dy \wedge dx \\
       &= \frac{y^2 - x^2}{r^4} dx \wedge dy - \frac{y^2 - x^2}{r^4} dx \wedge dy \\
       &= 0,
\end{align*}
so this 1-form is closed.

Note that similar to exercise~\ref{ex:liebracketpolarcoordinates}, $dx = \cos(\theta) \, dr - r \sin(\theta) \, d\theta$ and $dy = \sin(\theta) \, dr + r \cos(\theta) \, d\theta$ so $E = d \theta$. It's tempting to then say $dE = 0$ by $d^2 = 0$, but $d\theta$ is not exact since $\theta$ is not a well-defined 0-form, so the result doesn't follow.

We can parameterise our paths as
\begin{align*}
    \gamma_0: t &\mapsto (\cos(\pi(1 - t)), \sin(\pi(1 - t))), \\
    \gamma_1: t &\mapsto (\cos(\pi(1 + t)), \sin(\pi(1 + t))),
\end{align*}
so
\begin{align*}
    \SwapAboveDisplaySkip
    \gamma_0'(t) &= (\pi \sin(\pi(1 - t)), -\pi \cos(\pi(1 - t))), \\
    \gamma_1'(t) &= (-\pi \sin(\pi(1 + t)), \pi \cos(\pi(1 + t))).
\end{align*}

Integrating along $\gamma_0$,
\begin{align*}
    \int_{\gamma_0} E &= \int_0^1 E_{\gamma_0(t)}(\gamma_0'(t)) \, dt \\
        &= \int_0^1 \frac{-\pi \cos \bigl( \pi(1 - t) \bigr) \cos \bigl( \pi(1 - t) \bigr)
                          - \pi \sin \bigl( \pi(1 - t) \bigr) \sin \bigl( \pi(1 - t) \bigr)}
                         {{\cos \bigl( \pi(1 - t) \bigr)}^2 + {\sin \bigl( \pi(1 - t) \bigr)}^2} dt \\
        &= -\pi \int_0^1 dt \\
        &= -\pi,
\end{align*}
and along $\gamma_1$,
\begin{align*}
    \int_{\gamma_1} E &= \int_0^1 E_{\gamma_1(t)}(\gamma_1'(t)) \, dt \\
        &= \int_0^1 \frac{\pi \cos \bigl( \pi(1 + t) \bigr) \cos \bigl( \pi(1 + t) \bigr)
                          + \pi \sin \bigl( \pi(1 + t) \bigr) \sin \bigl( \pi(1 + t) \bigr)}
                         {{\cos \bigl( \pi(1 - t) \bigr)}^2 + {\sin \bigl( \pi(1 - t) \bigr)}^2} dt \\
        &= \pi \int_0^1 dt \\
        &= \pi.
\end{align*}
We could have skipped the second integral by making a symmetry argument that
\[
    \int_{\gamma_0} E = -\int_{\gamma_1} E.
\]
Or even better, by using $E = d\theta$ we drop parameterisation and skip both integrals as
\[
    \int_{\gamma_0} E = \int_\pi^0 d\theta = -\pi, \qquad
    \int_{\gamma_1} E = \int_\pi^{2\pi} d\theta = \pi.
\]

\end{sol}

\begin{ex}

Show that $\mathbb{R}^n$ is simply connected by exhibiting an explicit formula for a homotopy between any two paths between arbitrary points $p, q \in \mathbb{R}^n$.

\end{ex}

\begin{sol}

We say that a connected manifold is \emph{simply connected} if any two paths between two points $p$, $q$ are homotopic.

Let $\gamma_0, \gamma_1: [0, 1] \to \mathbb{R}^n$ with
\[
    \gamma_0(0) = \gamma_1(0) = p, \qquad
    \gamma_0(1) = \gamma_1(1) = q
\]
and consider
\begin{align*}
    \SwapAboveDisplaySkip
    \gamma &: [0, 1] \times [0, 1] \to \mathbb{R}^n, \\
           &: (s, t) \mapsto (1 - s)\gamma_0(t) + s \gamma_1(t).
\end{align*}
$\gamma$ is a homotopy between $\gamma_0$, $\gamma_1$ for arbitrary $p$, $q$ and therefore $\mathbb{R}^n$ is simply connected.

\end{sol}

\begin{ex}\label{ex:exactifintegraloverloopiszero}

Show that a 1-form $E$ is exact if and only if $\int_\gamma E = 0$ for all loops $\gamma$. (Hint: if $\omega$ is not exact, show that there are two smooth paths $\gamma$, $\gamma'$ from some point $x \in M$ to some point $y \in M$ such that $\int_\gamma \omega \neq \int_{\gamma'} \omega$. Use these paths to form a loop, perhaps only piecewise smooth.)

\end{ex}

\begin{sol}

Let $E = -d\phi$ be an exact 1-form and $\gamma: [0, 1] \to M$ a loop based at $p \in M$. Then
\begin{align*}
    \oint_\gamma E &= - \oint_\gamma d\phi \\
                   &= - \int_0^1 d\phi \bigl( \gamma'(t) \bigr) \, dt \\
                   &= - \int_0^1 \gamma'(t)(\phi) \, dt \\
                   &= - \int_0^1 \frac{d}{dt} \phi \bigl( \gamma(t) \bigr) \, dt \\
                   &= - \phi(p) + \phi(p) \\
                   &= 0.
\end{align*}

Conversely, let $E$ be not exact. On a simply connected manifold, every closed form is exact, so if $dE = 0$ then our manifold is not simply connected, implying the existence of non-homotopic smooth paths $\gamma_0$, $\gamma_1$ from $x$ to $y$ such that
\[
    \int_{\gamma_0} E \neq \int_{\gamma_1} E.
\]
We can therefore construct a piecewise-smooth loop $\tilde{\gamma}$ that traverses $\gamma_0$ forward and then $\gamma_1$ in reverse with
\[
    \oint_{\tilde{\gamma}} E = \int_{\gamma_0} E - \int_{\gamma_1} E \neq 0.
\]

\end{sol}

\begin{ex}

For any manifold $M$, show [that] the manifold $S^1 \times M$ is not simply connected by finding a 1-form on it that is closed but not exact.

\end{ex}

\begin{sol}\label{sol:s1productnotsimplyconnected}

Working in a chart with local coordinates $(\theta, x^1, \ldots, x^n)$, consider the 1-form $\omega = d\theta$ and let $\gamma$ be the loop traversing $S^1$ positively.
We know from solution~\ref{sol:closedbutpathdependent} that $d \omega = 0$, so $\omega$ is closed, and
\[
    \oint_\gamma \omega = 2 \pi,
\]
so, by exercise~\ref{ex:exactifintegraloverloopiszero}, $\omega$ is not exact.
The existence of a 1-form that is closed but not exact implies that $S^1 \times M$ is not simply connected.

\end{sol}

\section{Stokes' Theorem}

\begin{ex}\label{ex:ndisk}

Let the \emph{$n$-disk} $D^n$ be defined as
\[
    D^n = \setbuilder*{(x_1, \ldots, x_n) \given x_1^2 + \cdots + x_n^2 \leq 1}.
\]
Show that $D^n$ is an $n$-manifold with boundary in an obvious sort of way.

\end{ex}

\begin{sol}\label{sol:ndisk}

We need to show that $D^n$ is equipped with charts of the form $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ or $\varphi_\alpha: U_\alpha \to \mathbb{H}^n$, where $U_\alpha$ are open sets covering $D^n$ and $\mathbb{H}^n = \setbuilder*{x \in \mathbb{R}^n \given x^n \geq 0}$ is the closed half-space, such that the transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}$ are smooth where defined.

Let $\pi_i: \mathbb{R}^{n} \to \mathbb{R}^{n - 1}$, defined as
\[
    \pi_i: x \mapsto (x^1, \ldots, x^{i - 1}, x^{i + 1}, \ldots, x^{n}),
\]
be the projection that drops the $i$\textsuperscript{th} coordinate, $i \neq n$.

Recall the inverse stereographic projection from solution~\ref{sol:stereographicprojection} with $\alpha = 1$, which we denote as
\[
    \sigma_+^{-1}: x \mapsto \frac{1}{r^2 + 1} (2x^1, \ldots, 2x^n, r^2 - 1)
\]
where $r^2 = x_1^2 + \cdots + x_n^2$.

Consider the composition $\varphi_+ = \pi_i \circ \sigma_+^{-1}$,
\[
    \varphi_+: x \mapsto \frac{1}{r^2 + 1} (2x^1, \ldots, 2x^{i - 1}, 2x^{i + 1}, \ldots, 2x^n, r^2 - 1),
\]
and notice that $\varphi_+: D^n \to \mathbb{H}^n$. Indeed, on the boundary of $D^n$,
\[
    \lim_{r^2 \to 1} \varphi_+(x) = (x^1, \ldots, x^{i - 1}, x^{i + 1}, \ldots, x^n, 0)
\]
which is in $\partial \mathbb{H}^n$.

We can similarly construct $\varphi_-(x) = -\varphi_+(-x)$ corresponding to $\alpha = -1$. Then obviously the transition functions are smooth where they are defined.

\end{sol}

\begin{ex}

Check that the definition of tangent vectors in Chapter~\ref{ch:vectorfields} really does imply that the tangent space at a point on the boundary of an $n$-dimensional manifold with boundary is an $n$-dimensional vector space.

\end{ex}

\begin{sol}

We say that a function on $\mathbb{H}^n$ is smooth if it extends to a smooth function on the manifold $\setbuilder*{\mathbb{R}^n \given x^n > -\epsilon}$ for some $\epsilon > 0$.

We say that a function $f: M \to \mathbb{R}$ is smooth if for any chart $\varphi_\alpha$, $f \circ \varphi_\alpha^{-1}$ is smooth as a function on $\mathbb{R}^n$ or $\mathbb{H}^n$.

Let $p \in \partial M$. Then a tangent vector at $p$, $v_p: C^\infty(M) \to \mathbb{R}$, exists since $f$ is smooth up to and including the boundary by our extension of the definition of smoothness. Therefore $T_p M$ is an $n$-dimensional vector space as usual.

\end{sol}

\begin{ex}

For the mathematically inclined reader: prove that $\int_M \omega$ is independent of the choice of charts and partition of unity.

\end{ex}

\begin{sol}

% See "An Introduction to Manifolds" 2e by Loring W. Tu, 23.4. p265

Let $\dim(M) = n$, $\omega \in \Omega^n(M)$ and $\{\varphi_\alpha\}$ be an oriented atlas on $M$.

For some charts $\varphi$ and $\psi$ on an open set $U$, ${(\varphi^{-1})}^* \omega$ and ${(\psi^{-1})}^* \omega$ are $n$-forms on $\varphi(U)$ and $\psi(U)$, respectively. We can therefore construct an orientation-preserving diffeomorphism $\varphi \circ \psi^{-1}: \psi(U) \to \varphi(U)$.

Then
\begin{align*}
    \SwapAboveDisplaySkip
    \int_U \omega &= \int_{\varphi(U)} {(\varphi^{-1})}^* \omega \\
                  &= \int_{\psi(U)} {(\varphi \circ \psi^{-1})}^* {(\varphi^{-1})}^* \omega \\
                  &= \int_{\psi(U)} {(\psi^{-1})}^* \omega
\end{align*}
and therefore the integral of $\omega$ on $M$ is independent of the choice of charts.

For oriented atlases $\{(\varphi_\alpha, U_\alpha)\}$ and $\{(\varphi'_\beta, V_\beta)\}$, we have partitions of unity $\{f_\alpha\}$ and $\{f'_\beta\}$, say. Then
\[
    \omega = \sum_\alpha f_\alpha \omega = \sum_\beta f'_\beta \omega
\]
and
\begin{align*}
    \SwapAboveDisplaySkip
    \int_M \omega &= \sum_\alpha \int_{U_\alpha} f_\alpha \omega \\
                  &= \sum_\alpha \sum_\beta \int_{U_\alpha} f_\alpha f'_\beta \omega \\
                  &= \sum_\alpha \sum_\beta \int_{V_\beta} f_\alpha f'_\beta \omega \\
                  &= \sum_\beta \int_{V_\beta} f'_\beta \omega.
\end{align*}
In terms of local coordinates, under charts $\varphi_\alpha$ and $\varphi'_\beta$ we may write
\[
    f_\alpha \omega = g_\alpha dx^1 \wedge \cdots \wedge dx^n, \qquad
    f'_\beta \omega = g'_\beta dx'^1 \wedge \cdots \wedge dx'^n.
\]
But
\[
    g_\alpha dx^1 \wedge \cdots \wedge dx^n = g_\alpha \det(T) dx'^1 \wedge \cdots \wedge dx'^n
\]
so $g'_\beta = g_\alpha \det(T)$ on overlapping charts, with the Jacobian $T$ as per exercise~\ref{ex:transform1form}.
Then
\begin{align*}
    \int_M \omega &= \sum_\alpha \int_{U_\alpha} f_\alpha \omega \\
        &= \sum_\alpha \int_{\varphi_\alpha(U_\alpha)} g_\alpha dx^1 \wedge \cdots \wedge dx^n \\
        &= \sum_\beta \int_{\varphi'_\beta(V_\beta)} g'_\beta dx'^1 \wedge \cdots \wedge dx'^n \\
        &= \sum_\beta \int_{V_\beta} f'_\beta \omega.
\end{align*}
Therefore the integral of $\omega$ on $M$ is independent of the partition of unity.

\end{sol}

\begin{ex}

Show that $\partial D^n = S^{n - 1}$, where the $n$-disk $D^n$ is defined as in exercise~\ref{ex:ndisk}.

\end{ex}

\begin{sol}

The boundary of $M$ is the set of points $p \in M$ such that some chart $\varphi_\alpha: U_\alpha \to \mathbb{H}^n$ maps $p$ to a point in $\partial\mathbb{H}^n$.
We've already seen from solution~\ref{sol:ndisk} that when $r^2 \to 1$, $\varphi_\alpha(p) \in \partial\mathbb{H}^n$. This corresponds to
\[
    \partial D^n = \setbuilder*{(x_1, \ldots, \ldots, x_n) \given x_1^2 + \cdots + x_n^2 = 1} = S^{n - 1}.
\]

\end{sol}

\begin{ex}

Let $M = [0, 1]$. Show that Stokes' theorem in this case is equivalent to the fundamental theorem of calculus:
\[
    \int_0^1 f'(x) \, dx = f(1) - f(0).
\]

\end{ex}

\begin{sol}

Stokes' theorem states that for $M$ an oriented $n$-manifold with boundary and $\omega \in \Omega^{n - 1}(M)$, where either $M$ is compact or $\omega$ has compact support,
\begin{nowidthtags}
\[
    \int_M d\omega = \int_{\partial M} \omega. \tag{Stokes' theorem}\label{eq:stokestheorem}
\]
\end{nowidthtags}

Let $\omega = f$ be a 0-form, so $d\omega = df = f' dx$. Then
\[
    \int_M d\omega = \int_{[0, 1]} df = \int_0^1 f' dx
\]
and so, by Stokes' theorem,
\begin{align*}
\int_0^1 f'(x) \, dx &= \int_{\partial [0, 1]} f(x) \\
                     &= f(1) - f(0).
\end{align*}
The boundary $\partial [0, 1] = {\{0\}}^-\! \cup {\{1\}}^+$ where the sign denotes orientation.

While the integral over $[0, 1]$ has the Lebesgue measure on $\mathbb{R}$, it induces on its boundary the signed counting measure. Hence on the boundary integral, the boundary has non-zero measure.

\end{sol}

\begin{ex}

Let $M = \rinterval{0}{\infty}$ which is not compact.
Show that without the assumption that $f$ vanishes outside a compact set, Stokes' theorem may not apply.
(Hint\footnote{The original hint uses the wrong boundary.}: in this case Stokes' theorem says $\int_0^\infty f'(x) \, dx = -f(0)$.)

\end{ex}

\begin{sol}

Let $f$ be a 0-form on $M$.
The boundary of $M$ is $\partial M = {\{0\}}^-$ so, by \ref{eq:stokestheorem}, % chktex 2
\[
    \int_0^\infty f'(x) \, dx = \int_{{\{0\}}^-} f(x) = -f(0).
\]
But a standard Riemann integral of $f$ over $M$ gives
\begin{align*}
    \int_0^\infty f'(x) \, dx &= \lim_{b \to \infty} \int_0^b f'(x) \, dx \\
                              &= \lim_{b \to \infty} f(b) - f(0),
\end{align*}
which disagrees with Stokes' theorem unless $\displaystyle \lim_{x \to \infty} f(x) = 0$.

\end{sol}

\begin{ex}

Show that any submanifold is a manifold in its own right in a natural way.

\end{ex}

\begin{sol}\label{sol:submanifold}

Given a subset $S$ of an $n$-manifold $M$, we say that $S$ is a $k$-dimensional \emph{submanifold} of $M$ if for each point $p \in S$ there is an open set $U_\alpha$ of $M$ containing $p$ and a chart $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ such that $S \cap U_\alpha = \varphi_\alpha^{-1} (\mathbb{R}^k)$.

Consider the induced topology on $S$, so open sets are of the form $V_\alpha = S \cap U_\alpha$.
The collection $\{V_\alpha\}$ covers $S$ since it is not possible to find a point $p \in S$ such that $p \notin U_\alpha$ for any $\alpha$, as $\{U_\alpha\}$ covers $M$.

We can construct maps on $S$ to $\mathbb{R}^k$ by taking the restriction of $\varphi_\alpha$ to $M$ and projecting.
This gives us charts
\[
    \psi_\alpha: V_\alpha \to \mathbb{R}^k, \qquad
    \psi_\alpha = \pi \circ \varphi_\alpha,
\]
where $\pi: \mathbb{R}^n \to \mathbb{R}^k$ is a projection.
The collection $\{\psi_\alpha\}$ forms an atlas for $S$.

The transition functions $\psi_\alpha \circ \psi_\beta^{-1}: \mathbb{R}^k \to \mathbb{R}^k$ are smooth where they are defined as each $\psi_\alpha$ inherits the same smoothness properties as those of $\varphi_\alpha$.

Therefore $S$ is a manifold under the induced topology.

\end{sol}

\begin{ex}

Show that $S^{n - 1}$ is a compact submanifold of $\mathbb{R}^n$.

\end{ex}

\begin{sol}

$S^{n - 1}$ is a submanifold of $\mathbb{R}^n$ under stereographic projection as in solutions~\ref{sol:stereographicprojection},~\ref{sol:ndisk}, which gives us an atlas and smooth transition functions.

$S^{n - 1}$ is bounded since $\norm{p} = 1$ for all $p \in S^{n - 1}$.

Let $f: \mathbb{R}^n \to \mathbb{R}$ be given by $f(x_1, \ldots, x_n) = x_1^2 + \cdots + x_n^2$. Since $f$ is continuous, its inverse will map closed sets to closed sets. $f^{-1}\bigl(\{1\}\bigr) = S^{n - 1}$, so $S^{n - 1}$ is closed.

Since $S^{n - 1} \subset \mathbb{R}^n$ is closed and bounded, by the Heine--Borel theorem $S^{n - 1}$ is compact.

\end{sol}

\begin{ex}

Show that any open subset of a manifold is a submanifold.

\end{ex}

\begin{sol}

Recall from exercise~\ref{ex:opensubsetismanifold} that if $M$ is a manifold and $U$ an open subset of $M$ then $U$ with its induced topology is a manifold.

For each point $p \in U$, there is an open set $U_\alpha$ of $M$ containing $p$ and a chart $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ such that $U \cap U_\alpha = \varphi_\alpha^{-1}(\mathbb{R}^k)$, so $U$ is a submanifold of $M$ with charts $\pi \circ \varphi_\alpha$ restricted to $U$, where $\pi$ is a projection as in solution~\ref{sol:submanifold}.

\end{sol}

\begin{ex}

Show that if $S$ is a $k$-dimensional submanifold with boundary of $M$, then $S$ is a manifold with boundary in a natural way.
Moreover, show that $\partial S$ is a $(k - 1)$-dimensional submanifold of $M$.

\end{ex}

\begin{sol}

Take solution~\ref{sol:submanifold} and replace $\mathbb{R}$ with $\mathbb{H}$ and the result that $S$ is a manifold with boundary follows immediately.

We know that $\partial S$ is a manifold of dimension $k - 1$. To see that it is a submanifold of $S$, we note that for each point $p \in \partial S$ there is an open set $U_\alpha$ of $S$ containing $p$ and a chart $\varphi_\alpha: U_\alpha \to \mathbb{H}^k$ such that $\partial S \cap U_\alpha = \varphi_\alpha^{-1}(\mathbb{H}^{k - 1})$. Since $\partial S$ is a submanifold of $S$ and $S$ a submanifold with boundary of $M$, $\partial S$ is a submanifold of $M$.

\end{sol}

\begin{ex}

Show that $D^n$ is a submanifold of $\mathbb{R}^n$ in this sense.

\end{ex}

\begin{sol}

For interior points $p \in D^n \setminus \partial D^n$, we have for an open set $U$ in $\mathbb{R}^n$ that $U_\pm \cap U = \varphi_\pm^{-1}(\mathbb{R}^n)$ for $U_\pm$ an open set of $D_n$ and $\varphi_\pm$ the corresponding chart, as in solution~\ref{sol:ndisk}.

For boundary points $p \in \partial D^n$, we similarly have $U_\pm \cap U = \varphi_\pm^{-1}(\mathbb{H}^n)$.

Therefore $D^n$ is a submanifold of $\mathbb{R}^n$.

\end{sol}

\begin{ex}

Suppose that $S \subset \mathbb{R}^2$ is a 2-dimensional compact orientable submanifold with boundary.
Work out what Stokes' theorem says when applied to a 1-form on $S$.
This is sometimes called Green's theorem.

\end{ex}

\begin{sol}

Let $\omega = \omega_x dx + \omega_y dy$ be a 1-form on $S$. Taking the exterior derivative,
\begin{align*}
    d\omega &= \partial_x \omega_y dx \wedge dy + \partial_y \omega_x dy \wedge dx \\
            &= (\partial_x \omega_y - \partial_y \omega_x) \, dx \wedge dy.
\end{align*}
Therefore, by \ref{eq:stokestheorem}, % chktex 2
\[
    \int_{\partial S} (\omega_x dx + \omega_y dy) =
        \int_S (\partial_x \omega_y - \partial_y \omega_x) \, dx \wedge dy.
\]

\end{sol}

\begin{ex}

Suppose that $S \subset \mathbb{R}^3$ is a 2-dimensional compact orientable submanifold with boundary.
Show Stokes' theorem applied to $S$ boils down to the classic Stokes' theorem.

\end{ex}

\begin{sol}\label{sol:classicstokestheorem}

Let $\omega = \omega_x dx + \omega_y dy + \omega_z dz$ be a 1-form on $\mathbb{R}^3$, so the exterior derivative, as in solution~\ref{sol:curl1form}, is
\begin{align*}
    d\omega &= (\partial_y \omega_z - \partial_z \omega_y) \, dy \wedge dz \\
        &\mathrel{\phantom{=}}{} + (\partial_z \omega_x - \partial_x \omega_z) \, dz \wedge dx \\
        &\mathrel{\phantom{=}}{} + (\partial_x \omega_y - \partial_y \omega_x) \, dx \wedge dy.
\end{align*}

%Let $\{e^1, e^2\}$ be an oriented orthonormal basis on $\Omega^1(S)$. Then the ``area'' form on $S$, which is not necessarily exact, is given by
%\begin{align*}
%    dA_S &= e^1 \wedge e^2 \\
%         &= n_1 dy \wedge dz + n_2 dz \wedge dx + n_3 dx \wedge dy \\
%         &= \star n
%\end{align*}
%for some unit 1-form $n = n_i dx^i$. This uniquely determines the unit vector $n^i \partial_i$ normal to $S$.

Let $F = F^i \partial_i$ be the vector field dual to $\omega$, so $F^i = g^{ij} \omega_j = \omega_i$ since we're in $\mathbb{R}^3$. Then in old-fashioned vector calculus,
\[
    \int_S d\omega = \int_S (\nabla \times \vec{F}) \cdot d\vec{A}
\]
where $d\vec{A} = (dy \wedge dz, dz \wedge dx, dx \wedge dy)$ is the oriented area element
and
\[
    \int_{\partial S} \omega = \int_{\partial S} F_i \, dx^i
                             = \int_{\partial S} \vec{F} \cdot d\vec{s}
\]
with line element $d\vec{s} = (dx, dy, dz)$.
Therefore, by \ref{eq:stokestheorem}, % chktex 2
\[
    \int_S (\nabla \times \vec{F}) \cdot d\vec{A} = \int_{\partial S} \vec{F} \cdot d\vec{s}.
\]

\end{sol}

\begin{ex}

Suppose that $S \subset \mathbb{R}^3$ is a 3-dimensional compact orientable submanifold with boundary.
Show Stokes' theorem applied to $S$ is equivalent to Gau\ss' theorem, also known as the divergence theorem.

\end{ex}

\begin{sol}

Let $\omega = \omega_x dx + \omega_y dy + \omega_z dz$ be a 1-form on $\mathbb{R}^3$. By solution~\ref{sol:divergence1form},
\[
    \star \omega = \omega_x dy \wedge dz + \omega_y dz \wedge dx + \omega_z dx \wedge dy
\]
and
\[
    d \star \omega = (\partial_x \omega_x + \partial_y \omega_y + \partial_z \omega_z) \, dx \wedge dy \wedge dz.
\]
Again, let $F$ be the vector dual to $\omega$. Then
\[
    \int_S d \star \omega = \int_S \nabla \cdot \vec{F} \, dV
\]
where $dV$ is the volume form and
\[
    \int_{\partial S} \star \omega = \int_{\partial S} \vec{F} \cdot d\vec{A}
\]
where $d\vec{A}$ is as in solution~\ref{sol:classicstokestheorem}.
Therefore, by \ref{eq:stokestheorem}, % chktex 2
\[
    \int_S \nabla \cdot \vec{F} \, dV = \int_{\partial S} \vec{F} \cdot d\vec{A}.
\]

\end{sol}

\section{De Rham Cohomology}

\begin{epigraph}
    The boundary of a boundary is zero.
\end{epigraph}

\begin{ex}\label{ex:pullbackclosedandexact}

Show that the pullback of a closed form is closed
and the pullback of an exact form is exact.

\end{ex}

\begin{sol}

Recall from \S\ref{sec:cotangentvectors} that the exterior derivative is natural.

Let $\omega \in \Omega^p(M)$ be a closed form and $\phi: N \to M$. Then
\[
    d (\phi^* \omega) = \phi^* (d \omega) = 0
\]
since $\omega$ is closed.

If instead $\omega$ is exact, so $\omega = d\mu$ for some $\mu \in \Omega^{p - 1}(M)$, we get
\[
    \phi^* \omega = \phi^* (d \mu) = d (\phi^* \mu)
\]
which is exact.

\end{sol}

\begin{ex}

Show that given any map $\phi: M \to M'$ there is a linear map from $H^p(M')$ to $H^p(M)$ given by
\[
    [\omega] \mapsto [\phi^* \omega]
\]
where $\omega$ is any closed $p$-form on $M'$. Call this linear map
\[
    \phi^*: H^p(M') \to H^p(M).
\]
Show that if $\psi: M' \to M''$ is another map, then
\[
    {(\psi \phi)}^* = \phi^* \psi^*.
\]

\end{ex}

\begin{sol}\label{sol:pullbackcohomology}

Recall that for
\begin{align*}
    Z^p(M) &= \ker \bigl( d: \Omega^p(M) \to \Omega^{p + 1}(M) \bigr), \\
    Z^p(M) \supseteq B^p(M) &= \text{im} \bigl( d: \Omega^{p - 1}(M) \to \Omega^p(M) \bigr),
\end{align*}
the spaces of closed and exact $p$-forms on $M$ respectively, we define the \emph{$p$\textsuperscript{th} de~Rham cohomology group of $M$} as
\[
    H^p(M) = \faktor{Z^p(M)}{B^p(M)}.
\]
Let $\omega$ and $\omega'$ be cohomologous. Then naturally the pullback will preserve the cohomology, by exercise~\ref{ex:pullbackclosedandexact}. Explicitly,
\begin{align*}
    \phi^*[\omega] &= [\phi^* \omega] \\
                   &= \bigl[ \phi^*(\omega' + d\mu) \bigr] \\
                   &= \bigl[ \phi^* \omega' + \phi^* (d\mu) \bigr] \\
                   &= \bigl[ \phi^* \omega' + d(\phi^*\mu) \bigr] \\
                   &= [\phi^* \omega'] \\
                   &= \phi^* [\omega'].
\end{align*}
Introducing another linear map $\psi: M' \to M''$,
\[
    {(\psi \circ \phi)}^*[\omega] = \bigl[ {(\psi \circ \phi)}^* \omega \bigr]
                                  = [\phi^* \psi^* \omega]
                                  = \phi^* \psi^* [\omega]
\]
by exercise~\ref{ex:dualidentity}, so ${(\psi \phi)}^* = \phi^* \psi^*$ when acting on cohomology classes in $H^p(M'')$.

\end{sol}

\section{Gauge Freedom}

\emph{Nothing to do.}

\section{The Bohm--Aharonov Effect}

\begin{ex}

Do this. (Hint: show that $\star dz = r \, dr \wedge d\theta$.)

\end{ex}

\begin{sol}\label{sol:cylindricaldualcurrent}

We have cylindrical coordinates $z, r, \theta$ on $\mathbb{R}^3$, with corresponding 1-forms $dz$ defined everywhere, $dr$ defined away from $r = 0$ and $d\theta$ the closed but not exact 1-form from solution~\ref{sol:closedbutpathdependent}.

Recall that
\begin{align*}
    \SwapAboveDisplaySkip
    dx &= \cos(\theta) \, dr - r \sin(\theta) \, d\theta, \\
    dy &= \sin(\theta) \, dr + r \cos(\theta) \, d\theta.
\end{align*}
Taking the Hodge dual of $dz$,
\begin{align*}
    \star dz &= dx \wedge dy \\
             &= \bigl( \cos(\theta) \, dr -r \sin(\theta) \, d\theta \bigr)
                       \wedge (\sin(\theta) \, dr + r \cos(\theta) \, d\theta \bigr) \\
             &= r \, {\cos(\theta)}^2 dr \wedge d\theta + r \, {\sin(\theta)}^2 dr \wedge d\theta \\
             &= r \, dr \wedge d\theta.
\end{align*}
Suppose the current is cylindrically symmetric and flows in the $z$-direction, so that $j = f(r) \, dz$. Then away from the $z$-axis,
\[
    \star j = \star f(r) \, dz = f(r) r \, dr \wedge d\theta.
\]

\end{sol}

\begin{ex}

Show that $\star d\theta = \frac{1}{r} \, dz \wedge dr$.

\end{ex}

\begin{sol}

Taking the Hodge dual of $d\theta$,
\begin{align*}
    \star d\theta &= \star \left( \frac{x \, dy - y \, dx}{x^2 + y^2} \right) \\
                  &= \frac{x \,dz \wedge dx - y \, dy \wedge dz}{r^2} \\
                  &= \frac{1}{r} \bigl( \cos(\theta) \, dz \wedge dx - \sin(\theta) \, dy \wedge dz \bigr).
\end{align*}
But, from solution~\ref{sol:cylindricaldualcurrent},
\begin{align*}
    \cos(\theta) \, dz \wedge dx &= \cos(\theta) \, dz \wedge \bigl( \cos(\theta) \, dr -r \sin(\theta) \, d\theta \bigr) \\
        &= {\cos(\theta)}^2 dz \wedge dr - r \cos(\theta) \sin(\theta) \, dz \wedge d\theta
\end{align*}
and
\begin{align*}
    \SwapAboveDisplaySkip
    \sin(\theta) \, dy \wedge dz &= \sin(\theta) \bigl( r \cos(\theta) \, d\theta + \sin(\theta) \, dr \bigr) \wedge dz \\
        %&= r \cos(\theta) \sin(\theta) \, d\theta \wedge dz + {\sin(\theta)}^2 dr \wedge dz \\
        &= - r \cos(\theta) \sin(\theta) \, dz \wedge d\theta - {\sin(\theta)}^2 dz \wedge dr,
\end{align*}
so
\begin{align*}
    \SwapAboveDisplaySkip
    \star d\theta &= \frac{1}{r} \bigl( {\cos(\theta)}^2 dz \wedge dr + {\sin(\theta)}^2 dz \wedge dr \bigr) \\
                  &= \frac{1}{r} \, dz \wedge dr.
\end{align*}

\end{sol}

\begin{ex}

Check that $d \star B = \star j$ holds if and only if $g'(r) = r f(r)$.

\end{ex}

\begin{sol}

We have that $\star B = g(r) \, d\theta$, so
\begin{align*}
    d \star B &= dg(r) \, d\theta \\
              &= g'(r) \, dr \wedge d\theta
\end{align*}
since $d\theta$ is closed.
From solution~\ref{sol:cylindricaldualcurrent}, $\star j = f(r) r \, dr \wedge d\theta$, so if $d \star B = \star j$, we require $g'(r) = rf(r)$.

\end{sol}

\section{Wormholes}

\begin{ex}

Work out the details. (Hint: define a map $p: S^1 \times S^{n - 1} \to S^1$ corresponding to projection onto the first factor, and let the 1-form $\omega$ on $S^1 \times S^{n - 1}$ be the pullback of $d\theta$ by $p$.)

\end{ex}

\begin{sol}

Let $d\theta \in \Omega^1(S^1)$ be the classic closed and not exact 1-form we've seen already. Using the projection
\begin{align*}
    p &: S^1 \times S^{n - 1} \to S^1, \\
      &: \bigl( \theta_1, (\theta_2, \ldots, \theta_n) \bigr) \mapsto \theta_1,
\end{align*}
we can define a 1-form on the torus as $\omega = p^* d\theta \in \Omega^1(S^1 \times S^{n - 1})$.

By solution~\ref{sol:pullbackcohomology}, $\omega$ is closed and not exact since $p^*$ is a linear map from $H^1(S^1)$ to $H^1(S^1 \times S^{n - 1})$.

We can also show this without leveraging cohomology, since we know that $\omega$ is closed by exercise~\ref{ex:pullbackclosedandexact}. Then the result that $\omega$ is not exact follows directly from solution~\ref{sol:s1productnotsimplyconnected} with $M = S^{n - 1}$, as
\[
    \oint_{S^1} \omega \neq 0.
\]

\end{sol}

\begin{ex}\label{ex:vacuumelectrostaticwormhole}

In the space $\mathbb{R} \times S^2$ with the metric $g$ given above, let $E$ be the 1-form
\[
    E = e(r) \, dr.
\]
Show that $dE = 0$ holds no matter what the function $e(r)$ is, and show that $d \star E = 0$ holds when
\[
    e(r) = \frac{q}{4 \pi {f(r)}^2}.
\]

\end{ex}

\begin{sol}

Our metric on $\mathbb{R} \times S^2$ is $g = dr^2 + {f(r)}^2 \bigl( d\phi^2 + {\sin(\phi)}^2 d\theta^2 \bigr)$ where $f$ is positive for all $r$ and $f(r) \to r$ when $|r|$ is sufficiently large.\footnote{We approach $r$, not $r^2$, since the latter is not Euclidean.}

We want our 1-form to satisfy the vacuum electrostatic equations
\[
    dE = 0, \qquad
    d \star E = 0.
\]
$E$ is closed since
\[
    dE = e'(r) \, dr \wedge dr = 0.
\]
The volume form is
\begin{align*}
    \text{vol} &= \sqrt{|\det(g)|} \, dr \wedge d\theta \wedge d\phi \\
               &= {f(r)}^2 \sin(\phi) \, dr \wedge d\theta \wedge d\phi.
\end{align*}
By definition of the Hodge star, $dr \wedge \star dr = \innerp{dr}{dr} \text{vol} = \text{vol}$.
Denoting $\star dr = \alpha \, d\theta \wedge d\phi$ where $\alpha$ is a normalisation factor,
\[
    dr \wedge \star dr = \alpha \, dr \wedge d\theta \wedge d\phi = \text{vol},
\]
fixing $\alpha$ and giving us $\star dr = {f(r)}^2 \sin(\phi) \, d\theta \wedge d\phi$, so
\begin{align*}
    d \star E &= d \bigl( e(r) \star dr \bigr) \\
              &= d \bigl( e(r) {f(r)}^2 \sin(\phi) \, d\theta \wedge d\phi \bigr) \\
              &= \partial_r \bigl( e(r) {f(r)}^2 \bigr) \cdot \sin(\phi) \, dr \wedge d\theta \wedge d\phi.
\end{align*}
Then for $d \star E = 0$, we require $e(r) {f(r)}^2$ to be constant in $r$.
Let this constant be $\frac{q}{4 \pi}$, say, for some arbitrary $q$. Then
\[
    e(r) = \frac{q}{4 \pi {f(r)}^2}
\]
and $d \star E = 0$.

\end{sol}

\begin{ex}

Find a function $\phi$ with $E = -d\phi$.

\end{ex}

\begin{sol}

Denote the radial path from $0$ to $r$ by $\gamma$. Then a scalar potential is (up to a constant in $r$)
\[
    \phi(r) = -\int_\gamma E = -\frac{q}{4\pi}\int_0^r \frac{ds}{{f(s)}^2}.
\]

\end{sol}

\begin{ex}\label{ex:chargewithoutcharge}

Let $S^2$ denote any of the 2-spheres of the form $\{r\} \times S^2 \subset \mathbb{R} \times S^2$, equipped with the above volume form. Show that
\[
    \int_{S^2} \star E = q.
\]

\end{ex}

\begin{sol}

Our (positively oriented) volume form on $S^2$ is $r^2 \sin(\theta) \, d\theta \wedge d\phi$.
Taking the Hodge dual, we get
\begin{align*}
    \star E &= \frac{q}{4 \pi {f(r)}^2} \star dr \\
            &= \frac{q}{4 \pi} \sin(\phi) \, d\theta \wedge d\phi,
\end{align*}
so
\begin{align*}
    \SwapAboveDisplaySkip
    \int_{S^2} \star E &= \int_{S^2} \frac{q}{4 \pi} \sin(\phi) \, d\theta \wedge d\phi \\
        &= \int_{S^2} \frac{q}{4 \pi r^2} \text{vol} \\
        &= \frac{q}{4 \pi r^2} \int_{S^2} \text{vol} \\
        &= q,
\end{align*}
charge without charge.

\end{sol}

\begin{ex}

With this clue, work out a careful answer to the riddle.

\end{ex}

\begin{sol}

The riddle is: why does the integral of $\star E$ over \emph{any} 2-sphere of constant $r$ give $q$, when we expect to measure charge $q$ over  one mouth of the wormhole and $-q$ over the other?

Label each mouth ``positive'' and ``negative'', where we orient ourselves such that if starting at $r \ll 0$ and travelling in the positive $r$ direction, we are entering the negative mouth of the wormhole and exiting the positive mouth, and \emph{vice versa}.

In exercise~\ref{ex:chargewithoutcharge} we assumed our 2-sphere had positive radius.
We need to consider inverted 2-spheres to measure the charge over the negative mouth.

Using the negatively oriented volume form $-r^2 \sin(\theta) \, d\theta \wedge d\phi$ gives us
\[
    \int_{S^2} \star E = -q
\]
and resolves the riddle.

\end{sol}

\begin{ex}

Describe how this result generalises to spaces of other dimensions.

\end{ex}

\begin{sol}

By Maxwell, $d \star E = \rho$, so $\star E$ closed means the electric charge density $\rho = 0$.

In general, for an $n$-dimensional manifold $M$, closed 1-form $E$ and $(n - 1)$-submanifold $S \subset M$, if
\[
    \int_S \star E \neq 0
\]
then the $(n-1)$-form $\star E$ is closed but not exact.
The existence of closed but not exact $(n - 1)$-forms implies the de~Rham cohomology $H^{n - 1}(M)$ is non-empty.

\end{sol}

\begin{ex}\label{ex:closedbutnotexact2form}

Show using Cartesian coordinates that $\omega$ is closed on $\mathbb{R}^3 - \{0\}$.

\end{ex}

\begin{sol}\label{sol:closedbutnotexact2form}

The 2-form $\omega$ is given by
\[
    \omega = \frac{x \, dy \wedge dz + y \, dz \wedge dx + z \, dx \wedge dy}{{(x^2 + y^2 + x^2)}^\frac{3}{2}}.
\]
Splitting into three terms, take the derivative of the first,
\begin{align*}
    d \frac{x \, dy \wedge dz}{{(x^2 + y^2 + x^2)}^\frac{3}{2}}
        &= \partial_x \frac{x}{{(x^2 + y^2 + x^2)}^\frac{3}{2}} \, dx \wedge dy \wedge dz \\
        &= \frac{{(x^2 + y^2 + z^2)}^\frac{3}{2} - 3x^2 \sqrt{x^2 + y^2 + z^2}}{{(x^2 + y^2 + z^2)}^3} \, dx \wedge dy \wedge dz \\
        &= \frac{x^2 + y^2 + z^2 - 3x^2}{{(x^2 + y^2 + z^2)}^\frac{3}{2}} \, dx \wedge dy \wedge dz.
\end{align*}
The $y$ and $z$ terms are similar, by symmetry,
%\begin{align*}
%    d \frac{y \, dz \wedge dx}{{(x^2 + y^2 + x^2)}^\frac{3}{2}}
%        &= \frac{x^2 + y^2 + z^2 - 3y^2}{{(x^2 + y^2 + z^2)}^\frac{3}{2}} \, dx \wedge dy \wedge dz, \\
%    d \frac{z \, dx \wedge dy}{{(x^2 + y^2 + x^2)}^\frac{3}{2}}
%        &= \frac{x^2 + y^2 + z^2 - 3z^2}{{(x^2 + y^2 + z^2)}^\frac{3}{2}} \, dx \wedge dy \wedge dz,
%\end{align*}
so
\[
    d\omega = \frac{3(x^2 + y^2 + z^2) - 3x^2 - 3y^2 - 3z^2}{{(x^2 + y^2 + z^2)}^\frac{3}{2}} \, dx \wedge dy \wedge dz = 0.
\]

\end{sol}

\begin{ex}

Generalise these examples and find an $(n-1)$-form in $\mathbb{R}^n - \{0\}$ that is closed but not exact.
Conclude that $H^{n - 1} \bigl( \mathbb{R}^{n} - \{0\} \bigr)$ is nonzero.

\end{ex}

\begin{sol}

We want to generalise the 1-form $d\theta$ of solution~\ref{sol:closedbutpathdependent} and 2-form $\omega$ of exercise~\ref{ex:closedbutnotexact2form} to a closed but not exact $(n-1)$-form.

The form will obviously be
\[
    \omega = \frac{\sum_i x_i dx^1 \wedge \cdots \wedge dx^{i - 1} \wedge dx^{i + 1} \wedge \cdots \wedge dx^n}
                  {{(x_1^2 + \cdots + x_n^2)}^\frac{n}{2}}.
\]
Taking the exterior derivative of the first term,
\begin{align*}
    d \, \frac{x_1 dx^2 \wedge \cdots \wedge dx^n}{{(x_1^2 + \cdots + x_n^2)}^\frac{n}{2}}
        &= \partial_1 \frac{x_1}{{(x_1^2 + \cdots + x_n^2)}^\frac{n}{2}} \, dx^1 \wedge \cdots \wedge dx^n \\
        ={}& \frac{{(x_1^2 + \cdots + x_n^2)}^\frac{n}{2} - n x_1^2 {(x_1^2 + \cdots + x_n^2)}^{\frac{n}{2} - 1}}
                  {{(x_1^2 + \cdots + x_n^2)}^n} \, dx^1 \wedge \cdots \wedge dx^n \\
        ={}& \frac{x_1^2 + \cdots + x_n^2 - n x_1^2}
                  {{(x_1^2 + \cdots + x_n^2)}^{\frac{n}{2} + 1}} \, dx^1 \wedge \cdots \wedge dx^n.
\end{align*}
By symmetry, the $x_2, \ldots, x_n$ terms are similar and therefore, as in solution~\ref{sol:closedbutnotexact2form}, $d\omega = 0$.

Let $S \subset \mathbb{R}^n - \{0\}$ be an $(n - 1)$-submanifold. Then
\[
    \int_S \omega \neq 0
\]
since it is not possible to deform $S$ due to the puncture at the origin, so by \ref{eq:stokestheorem} $\omega$ is not exact. % chktex 2

By the existence of a closed but not exact $(n - 1)$-form, $H^{n - 1} \bigl( \mathbb{R}^n - \{0\} \bigr)$ is non-empty.

\end{sol}

\section{Monopoles}

\begin{ex}

Check this. (Hint: show that $B = \frac{m}{4 \pi} \sin(\phi) \, d\theta \wedge d\phi$.)

\end{ex}

\begin{sol}

The vacuum magnetostatic equations are
\[
    dB = 0, \qquad
    d \star B = 0.
\]
On $\mathbb{R} \times S^2$ with metric $g$ as in exercise~\ref{ex:vacuumelectrostaticwormhole}, we can find a closed but not exact magnetic 2-form by duality. Using $B = \star E$,
\begin{align*}
    B &= \star \frac{m \, dr}{4 \pi {f(r)}^2} \\
      &= \frac{m}{4 \pi} \sin(\phi) \, d\phi \wedge d\theta
\end{align*}
and on integrating over any 2-sphere,
\begin{align*}
    \int_{S^2} B &= \int_{S^2} \frac{m}{4 \pi} \sin(\phi) \, d\phi \wedge d\theta \\
        &= \frac{m}{4 \pi r^2} \int_{S^2} \text{vol} \\
        &= m,
\end{align*}
the magnetic charge.

\end{sol}

\part{Gauge Fields}

\chapter{Symmetry}

\begin{epigraph}
    Symmetry dictates interactions.
\end{epigraph}

\section{Lie Groups}\label{sec:liegroups}

\begin{ex}

Show that $\SO(3, 1)$ contains the Lorentz transformation mixing up the $t$ and $x$ coordinates:
\[
    \begin{pmatrix}
        \phantom{-}\cosh(\phi) & -\sinh(\phi)           & 0 & 0 \\
        -\sinh(\phi)           & \phantom{-}\cosh(\phi) & 0 & 0 \\
        0                      & 0                      & 1 & 0 \\
        0                      & 0                      & 0 & 1
    \end{pmatrix}
\]
as well as the Lorentz transformations mixing up $t$ and $y$, or $t$ and $z$ coordinates.

\end{ex}

\begin{sol}\label{sol:so31lorentztransformations}

Let $\Lambda$ be the Lorentz transformation with rapidity $\phi$ mixing up $t$ and $x$ given above.
Then for some vector $v = v^\mu \partial_\mu$ on $\mathbb{R}^4$, the components of $\Lambda v$ transform as
\[
    {\Lambda^\mu}_\nu v^\nu = \begin{pmatrix}
        \phantom{-} \cosh(\phi) v^0 - \sinh(\phi) v^1 \\
        -\sinh(\phi) v^0 + \cosh(\phi) v^1 \\
        \;\;\;\; v^2 \\
        \;\;\;\; v^3
    \end{pmatrix}.
\]
On Minkowski space with metric $\eta$ as in exercise~\ref{ex:minkowskimetric}, the inner product of two vectors $v$, $w$ transformed under $\Lambda$ is
\begin{align*}
    \eta(\Lambda v, \Lambda w)
        &\mathrel{=}{} - \bigl(\cosh(\phi) v^0 - \sinh(\phi) v^1 \bigr)
            \bigl( \cosh(\phi) w^0 - \sinh(\phi) w^1 \bigr) \\
        &\mathrel{\phantom{=}}{} + \bigl(-\sinh(\phi) v^0 + \cosh(\phi) v^1 \bigr)
                                   \bigl(-\sinh(\phi) w^0 + \cosh(\phi) w^1 \bigr) \\
        &\mathrel{\phantom{=}}{} + v^2 w^2 + v^3 w^3 \\
        &\mathrel{=}{} - {\cosh(\phi)}^2 v^0 w^0 + \cosh(\phi) \sinh(\phi) v^0 w^1 \\
        &\mathrel{\phantom{=}}{} + \sinh(\phi) \cosh(\phi) v^1 w^0 - {\sinh(\phi)}^2 v^1 w^1 \\
        &\mathrel{\phantom{=}}{} + {\sinh(\phi)}^2 v^0 w^0 - \sinh(\phi) \cosh(\phi) v^0 w^1 \\
        &\mathrel{\phantom{=}}{} - \cosh(\phi) \sinh(\phi) v^1 w^0 + {\cosh(\phi)}^2 v^1 w^1 \\
        &\mathrel{\phantom{=}}{} + v^2 w^2 + v^3 w^3 \\
        &\mathrel{=}{} - \bigl( {\cosh(\phi)}^2 - {\sinh(\phi)}^2 \bigr) v^0 w^0
                       + \bigl( -{\sinh(\phi)}^2 + {\cosh(\phi)}^2) v^1 w^1 \\
        &\mathrel{\phantom{=}}{} + \bigl( \cosh(\phi) \sinh(\phi) - \sinh(\phi) \cosh(\phi) \bigr) v^0 w^1 \\
        &\mathrel{\phantom{=}}{} + \bigl( \sinh(\phi) \cosh(\phi) - \cosh(\phi) \sinh(\phi) \bigr) v^1 w^0 \\
        &\mathrel{\phantom{=}}{} + v^2 w^2 + v^3 w^3 \\
        &= - v^0 w^0 + v^1 w^1 + v^2 w^2 + v^3 w^3 \\
        &= \eta(v, w)
\end{align*}
so $\Lambda$ preserves the inner product and is therefore in $\O(3, 1)$.
Taking the determinant gives
\[
    \det(\Lambda) = {\cosh(\phi)}^2 - {\sinh(\phi)}^2 = 1
\]
so $\Lambda \in \SO(3, 1)$.

The Lorentz transformations with rapidity $\phi$ mixing up the $t$ and $y$ and $t$ and $z$ coordinates are
\[
    \begin{pmatrix}
        \phantom{-}\cosh(\phi) & 0 & -\sinh(\phi)           & 0 \\
        0                      & 1 & 0                      & 0 \\
        -\sinh(\phi)           & 0 & \phantom{-}\cosh(\phi) & 0 \\
        0                      & 0 & 0                      & 1
    \end{pmatrix}, \qquad
    \begin{pmatrix}
        \phantom{-}\cosh(\phi) & 0 & 0 & -\sinh(\phi) \\
        0                      & 1 & 0 & 0 \\
        0                      & 0 & 1 & 0 \\
        -\sinh(\phi)           & 0 & 0 & \phantom{-}\cosh(\phi)
    \end{pmatrix},
\]
respectively.
By similar calculations, these two boosts preserve the inner product and have determinant $1$, so are also in $\SO(3, 1)$.

\end{sol}

\begin{ex}

Show that $\SO(3, 1)$ contains neither parity,
\[
    P: (t, x, y, z) \mapsto (t, -x, -y, -z),
\]
nor \emph{time-reversal},
\[
    T: (t, x, y, z) \mapsto (-t, x, y, z),
\]
but that these lie in $\O(3, 1)$. Show that the product $PT$ lies in $\SO(3, 1)$.

\end{ex}

\begin{sol}

We can represent these transformations as
\[
    P = \begin{tightmatrix}\begin{pmatrix*}[r]
            1 & 0  & 0  & 0 \\
            0 & -1 & 0  & 0 \\
            0 & 0  & -1 & 0 \\
            0 & 0  & 0  & -1
        \end{pmatrix*}\end{tightmatrix}, \qquad
    T = \begin{pmatrix*}[r]
            -1 & 0  & 0  & 0 \\
            0  & 1  & 0  & 0 \\
            0  & 0  & 1  & 0 \\
            0  & 0  & 0  & 1
        \end{pmatrix*}.
\]
For vectors $v$, $w$,
\begin{align*}
    \eta(Pv, Pw) &= -v^0 w^0 + v^1 w^1 + v^2 w^2 + v^3 w^3 = \eta(v, w), \\
    \eta(Tv, Tw) &= -v^0 w^0 + v^1 w^1 + v^2 w^2 + v^3 w^3 = \eta(v, w)
\end{align*}
so $P$ and $T$ are in $\O(3, 1)$, which implies $PT \in \O(3, 1)$.

$\det(P) = \det(T) = -1$ so $P$ and $T$ are not in $\SO(3, 1)$. But
\[
    \det(PT) = \det(P) \det(T) = 1
\]
so the product $PT \in \SO(3, 1)$.

\end{sol}

\begin{ex}

Show that $\SL(n, \mathbb{R})$, $\SL(n, \mathbb{C})$, $\O(p, q)$, $\SO(p, q)$, $\U(n)$ and $\SU(n)$ are really matrix groups,
that is, that they are closed under matrix multiplication, inverses, and contain the identity matrix.

\end{ex}

\begin{sol}

Let $u$, $v$ be vectors on $\mathbb{C}^n$ with some metric $g$ and $A$, $B$ be matrices in some group $G$.

\begin{itemize}

    \item For $G$ one of $\O(p, q)$, $\U(n)$,
    \begin{dsfalign}
        \innerp*{(AB) v}{(AB) w} &= \innerp*{A (Bv)}{A (Bw)} \\
                                 &= \innerp*{Bv}{Bw} \\
                                 &= \innerp*{v}{w}
    \end{dsfalign}
    so $AB \in G$, implying $G$ is closed under multiplication.
    The same holds for $\SO(p, q)$ and $\SU(n)$ but we additionally require $\det(AB) = 1$, which is true as $\det(AB) = \det(A) \det(B)$.

    This secondary requirement also applies to $\SL(n, \mathbb{R})$ and $\SL(n, \mathbb{C})$, so both of these groups are closed as well.

    \item For $G$ one of the orthogonal or unitary groups, $A \in G$ is a rotation about some axis by some angle $\theta$, say.
    Then a matrix $A^{-1}$ rotating by $-\theta$ will satisfy $AA^{-1} = A^{-1}A = \id$.
    For $A$ unitary, $A^{-1} = A^\dagger$, the conjugate transpose, and for $A$ orthogonal this reduces to the transpose.

    For $G$ any of $\SL(n, \mathbb{R})$, $\SL(n, \mathbb{C})$, $\SO(p, q)$, $\SU(n)$, $A$ is invertible since $\det(A) = 1$.
    The inverse $A^{-1} \in G$ since $\det(A^{-1}) = {\det(A)}^{-1} = 1$.

    \item The standard $n \times n$ identity matrix satisfies
    \[
        \innerp*{\id \, u}{\id \, v} = \innerp*{u}{v}, \qquad \det(\id) = 1
    \]
    so the identity is in $\SL(n, \mathbb{R})$, $\SL(n, \mathbb{C})$, $\O(p, q)$, $\SO(p, q)$, $\U(n)$ and $\SU(n)$.

\end{itemize}

\end{sol}

\begin{ex}

Show that the groups $\GL(n, \mathbb{R})$, $\GL(n, \mathbb{C})$, $\SL(n, \mathbb{R})$, $\SL(n, \mathbb{C})$, $\O(p, q)$, $\SO(p, q)$, $\U(n)$ and $\SU(n)$ are Lie groups.
(Hint: the hardest part is to show that they are submanifolds of the space of matrices.)

\end{ex}

\begin{sol}

Let $A$, $B$ be matrices in $\GL(n, \mathbb{C})$. The product map acts elementwise as ${(ab)}_{ij} = a_{ik} b_{kj}$,
which is smooth since the product is a polynomial of elements of $A$ and $B$.
Inversion by Cramer's rule,
\[
    A \mapsto A^{-1} = \frac{\text{adj}(A)}{\det(A)},
\]
is also smooth since entries of $\text{adj}(A)$ are polynomials of entries of $A$.

Let $M(n, \mathbb{C})$ be the space of $n \times n$ matrices over $\mathbb{C}$.
This is trivially a smooth $2n^2$-manifold since it is homeomorphic to $\mathbb{R}^{2n^2}$.
The map $\det : M(n, \mathbb{C}) \to \mathbb{C}$ is smooth, so $\GL(n, \mathbb{C}) = \det^{-1}(\mathbb{C} \setminus \{0\})$ is an open subset of $M(n, \mathbb{C})$ and therefore a submanifold (via solution~\ref{sol:submanifold}), so $\GL(n, \mathbb{C})$ is a Lie group.

$\GL(n, \mathbb{R})$ is a Lie group, analogously.

Closed subgroups of Lie groups are Lie groups, so the classical groups $\SL(n, \mathbb{C})$, $\SL(n, \mathbb{R})$, $\O(p, q)$, $\SO(p, q)$, $\U(n)$ and $\SU(n)$ are Lie groups.

\end{sol}

\begin{ex}

Given a Lie group $G$, define its \emph{identity component} $G_0$ to be the connected component containing the identity element.
Show that the identity component of any Lie group is a subgroup, and a Lie group in its own right.

\end{ex}

\begin{sol}

Let $g, h \in G_0$.

Since $G$ is a Lie group, the product map
\begin{align*}
    \mu &: G_0 \times G_0 \to G, \\
        &: (g, h) \mapsto gh
\end{align*}
is continuous so, since $G_0 \times G_0$ is connected, the image $\mu(G_0 \times G_0)$ is connected. $\mu(\id, \id) = \id$, so $gh \in G_0$ and therefore $G_0$ is closed.

Similarly, consider the inversion map $g \mapsto g^{-1}$ which is also by definition continuous, so its image is connected. Since $\id = \id^{-1}$, this connected component is the identity component.

$G_0$ is therefore a subgroup of $G$. Smooth product and inverse operations imply it is a Lie group.

\end{sol}

\begin{ex}

Show that every element of $\O(3)$ is either a rotation about some axis or a rotation about some axis followed by a reflection through some plane.
Show that the former class of elements are all in the identity component of $\O(3)$, while the latter are not.
Conclude that the identity component of $\O(3)$ is $\SO(3)$.

\end{ex}

\begin{sol}

Let $Q \in \O(3)$. Since $QQ^T = \id$, $\det(QQ^T) = 1$, so $\det{Q} = \pm 1$.

Let $R \in \O(3)$ be a rotation. This is smoothly parameterised by the angle $\theta$ and when $\theta = 0$, $R = \id$.
Therefore $\det(R) = 1$ and $R$ is in the identity component.
Therefore $R \in SO(3) \subset \O(3)$.

Let $P \in \O(3)$ be a reflection, which is not orientation-preserving, so $\det(P) = -1$.
The composition $RP \in \O(3)$ which also has $\det(RP) = -1$.
Since reflections are not continuous transformations and since the identity cannot be of the form $RP$, this is a disconnected component of $\O(3)$.

\end{sol}

\begin{ex}

Show that there is no path from the identity to the element $PT$ in $\SO(3, 1)$.
Show that $\SO(3, 1)$ has two connected components.
The identity component is written $\SO_0(3, 1)$;
we warn the reader that sometimes this group is called the Lorentz group.
We prefer to call it the \emph{connected Lorentz group}.

\end{ex}

\begin{sol}

From solution~\ref{sol:so31lorentztransformations},
\[
    \eta_{\mu\nu} {\Lambda^\mu}_\rho {\Lambda^\nu}_\sigma v^\rho w^\sigma
        = \eta_{\mu\nu} v^\mu w^\nu
        = \eta_{\rho \sigma} v^\rho w^\sigma,
\]
so the general Lorentz group $\O(3, 1)$ is characterised by
\[
    \eta_{\mu\nu} {\Lambda^\mu}_\rho {\Lambda^\nu}_\sigma = \eta_{\rho\sigma}.
\]
Looking only at the time component,
\[
    \eta_{\mu\nu} {\Lambda^\mu}_0 {\Lambda^\nu}_0
        = -{\Lambda^0}_0 {\Lambda^0}_0 + {\Lambda^i}_0 {\Lambda^i}_0
\]
and, equating with $\eta_{00} = -1$,
\[
    {({\Lambda^0}_0)}^2 = 1 + {({\Lambda^i}_0)}^2 \geq 1,
\]
implying either ${\Lambda^0}_0 \geq 1$ or ${\Lambda^0}_0 \leq 1$.
Therefore there is no smooth path between transformations with ${\Lambda^0}_0$ of different sign, so they must lie in disjoint connected components.

Transformations with ${\Lambda^0}_0 \geq 1$ preserve the direction of time.
Since $\delta_0^0 = 1$ (the identity preserves the direction of time), the group of proper orthochronous Lorentz transformations is the identity component, $\SO_0(3, 1)$.

The Klein four-group $V_4 = \{\id, P, T, PT\}$ is a discrete subgroup of $\O(3, 1)$. The transformation $PT \in \SO(3, 1)$ has ${(PT)}^0_0 = -1$, so $PT$ is not path-connected to the identity component.
We therefore have four disjoint connected components of the Lorentz group,
\begin{align*}
    \SO_0(3, 1) &= \setbuilder*{\Lambda \in \O(3, 1) \given \det(\Lambda) = 1, {\Lambda^0}_0 \geq 1}, \\
    \SO(3, 1) \setminus \SO_0(3, 1)
        &= \setbuilder*{\Lambda \in \O(3, 1)  \given {\det(\Lambda) = 1, \Lambda^0}_0 \leq 1}, \\
    \O_0(3, 1) \setminus \SO_0(3, 1)
        &= \setbuilder*{\Lambda \in \O(3, 1)  \given {\det(\Lambda) = -1, \Lambda^0}_0 \geq 1}, \\
    \O(3, 1) \setminus \bigl( \O_0(3, 1) \cup \SO(3, 1) \bigr)
        &= \setbuilder*{\Lambda \in \O(3, 1)  \given {\det(\Lambda) = -1, \Lambda^0}_0 \leq 1},
\end{align*}
the proper orthochronous, proper non-orthochronous, improper orthochronous and improper non-orthochronous transformations, respectively.
These are related by elements of $V_4$.
\[
    \begin{tikzcd}[row sep=7ex, column sep={8.8em, between origins},
                   every arrow/.append style=leftrightarrow]
        & \SO_0(3, 1) \dlar[bend right=30, swap]{T}
                      \drar[bend left=30]{P} & \\
        \O(3, 1) \setminus \bigl( \O_0(3, 1) \cup \SO(3, 1) \bigr)
                \drar[bend right=30, swap]{P}
                \ar{rr}
            & & \O_0(3, 1) \setminus \SO_0(3, 1)
                \dlar[bend left=30]{T} \\
        & \SO(3, 1) \setminus \SO_0(3, 1) \ar[crossing over, bend right=8, labels=description]{uu}{PT} &
    \end{tikzcd}
\]

\end{sol}

\begin{ex}

Show that if $\rho: G \to H$ is a homomorphism of groups, then
\[
    \rho(1) = 1
\]
and
\[
    \rho(g^{-1}) = {\rho(g)}^{-1}.
\]
(Hint: first prove that a group only has one element with the properties of the identity element, and for each group element $g$ there is only one element with the properties of $g^{-1}$.)

\end{ex}

\begin{sol}

Let $e, f$ be identity elements of $G$. Then $e = ef = f$, so the identity is unique.

Let $fg = gf = hg = gh = 1$. Then $fgf = f = fgh = h$ so the inverse is unique.

Given two groups $G$ and $H$, we say a function $\rho: G \to H$ is a \emph{homomorphism} if $\rho(gh) = \rho(g) \rho(h)$.
\[
    \rho(g) = \rho(\id_G g) = \rho(\id_G) \rho(g)
\]
so $\rho(\id_G) = \id_H$.
\[
    \id_H = \rho(\id_G) = \rho(g^{-1} g) = \rho(g^{-1}) \rho(g)
\]
so $\rho(g^{-1}) = {\rho(g)}^{-1}$.

\end{sol}

\begin{ex}

A $1 \times 1$ matrix is just a number, so show that
\[
    \U(1) = \setbuilder{e^{i\theta} \given \theta \in \mathbb{R}}.
\]
In physics, an element of $\U(1)$ is called a \emph{phase}.
Show that $\U(1)$ is isomorphic to $\SO(2)$, with an isomorphism being given by\footnote{The direction is conventional, but we use positive rotations here to make the isomorphism more direct.}
\[
    \settightmatrix
    \rho(e^{i\theta}) =
        \begin{pmatrix*}[r]
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix*}.
\]
(Hint: rotations of the 2-dimensional real vector space $\mathbb{R}^2$ are the same as rotations of the complex plane $\mathbb{C}$.)

\end{ex}

\begin{sol}

$\rho: \U(1) \to \SO(2)$ is a homomorphism by
\begin{align*}
    \rho(e^{i\theta_1}) \rho(e^{i\theta_2})
        &=
        \begin{tightmatrix}\begin{pmatrix*}[r]
            \cos(\theta_1) & -\sin(\theta_1) \\
            \sin(\theta_1) & \cos(\theta_1)
        \end{pmatrix*}\end{tightmatrix}
        \begin{tightmatrix}\begin{pmatrix*}[r]
            \cos(\theta_2) & -\sin(\theta_2) \\
            \sin(\theta_2) & \cos(\theta_2)
        \end{pmatrix*}\end{tightmatrix} \\
        &=
        \begin{tightmatrix}\begin{pmatrix*}[r]
            \cos(\theta_1 + \theta_2) & -\sin(\theta_1 + \theta_2) \\
            \sin(\theta_1 + \theta_2) & \cos(\theta_1 + \theta_2)
        \end{pmatrix*}\end{tightmatrix} \\
        &= \rho(e^{i(\theta_1 + \theta_2)}) \\
        &= \rho(e^{i\theta_1}e^{i\theta_2}).
\end{align*}
For some $z \in \mathbb{C}$ with $z = x + iy$, we have
\[
    e^{i\theta} z = x \cos(\theta) - y \sin(\theta) + i \bigl( x \sin(\theta) + y \cos(\theta) \bigr)
\]
and for some vector $v \in \Vect(\mathbb{R}^2)$ with components $(x, y)$,
\[
    \rho(e^{i\theta}) \begin{pmatrix} x \\ y \end{pmatrix}
        = \begin{pmatrix*}[r]
            x \cos(\theta) - y \sin(\theta) \\
            x \sin(\theta) + y \cos(\theta)
        \end{pmatrix*}.
\]
We can equate the real and imaginary components of $e^{i\theta} z$ with the $x$ and $y$ components of $\rho(e^{i\theta}) v$.
Since every element of $\SO(2)$ (represented as matrices) is of the form $\rho(e^{i\theta})$, $\rho$ is surjective.
Since $\rho$ takes every element of $U(1)$ to a distinct element of $\SU(2)$, $\rho$ is injective.

Thus $\rho$ is a homomorphic bijection and therefore $\U(1) \cong \SO(2)$.

\end{sol}

\begin{ex}

Given groups $G$ and $H$, let $G \times H$ denote the set of ordered pairs $(g, h)$ with $g \in G$, $h \in H$.
Show that $G \times H$ becomes a group with product
\[
    (g, h) (g', h') = (gg', hh'),
\]
identity element
\[
    1 = (1, 1)
\]
and inverse
\[
    {(g, h)}^{-1} = (g^{-1}, h^{-1}).
\]
The group $G \times H$ is called the \emph{direct product} or \emph{direct sum} of $G$ and $H$, depending on who you talk to.
(When called the direct sum, it is written $G \oplus H$.)
Show that if $G$ and $H$ are Lie groups, so is $G \times H$.
Show that $G \times H$ is abelian if and only if $G$ and $H$ are abelian.

\end{ex}

\begin{sol}

$G \times H$ is obviously a group.

Let $G$, $H$ be Lie groups.
Then $G \times H$ is a Lie group since it is a manifold with the product topology as per solution~\ref{sol:producttopology}.

Let $G$, $H$ be abelian. Then $G \times H$ is abelian since
\[
    (g, h) (g', h') = (gg', hh') = (g'g, h'h) = (g', h')(g, h).
\]
Suppose $G$ is not abelian. Then
\[
    (g, 1) (g', h') = (gg', h') \neq (g'g, h') = (g' h')(g, 1)
\]
and equivalently if $H$ is not abelian, so $G \times H$ is abelian if and only if $G$ and $H$ are abelian.

\end{sol}

\begin{ex}\label{ex:directsumrepresentation}

Show that [the] direct sum of representations is really a representation.

\end{ex}

\begin{sol}

A representation of $G$ on $V$ is a homomorphism $\rho: G \to \GL(V)$.

Let $G$ be a group and let $\rho$ be a representation of $G$ on $V$ and $\rho'$ be a representation of $G$ on $V'$.
Let $\rho \oplus \rho'$, the \emph{direct sum} of the representations $\rho$ and $\rho'$, be the representation of $G$ on the direct sum $V \oplus V'$ given by
\[
    (\rho \oplus \rho')(g)(v, v') = \bigl( \rho(g) v, \rho'(g) v' \bigr)
\]
for all $v \in V$, $v' \in V'$.

Let $g, h \in G$. Then
\begin{align*}
    (\rho \oplus \rho')(gh) &= \bigl( \rho(gh), \rho'(gh) \bigr) \\
        &= \bigl( \rho(g) \rho(h), \rho'(g) \rho'(h) \bigr) \\
        &= \bigl( \rho(g), \rho'(g) \bigr) \bigl( \rho(h), \rho'(h) \bigr) \\
        &= (\rho \oplus \rho')(g) \cdot (\rho \oplus \rho')(h)
\end{align*}
so $\rho \oplus \rho': G \to \GL(V \oplus V')$ is a homomorphism and therefore $\rho \oplus \rho'$ is really a representation of $G$ on $V \oplus V'$.

\end{sol}

\begin{ex}

Prove that the above is true.

\end{ex}

\begin{sol}

Let $V$, $V'$ be vector spaces with bases $\{e_i\}$, $\{e'_j\}$, respectively.
The \emph{tensor product} $V \otimes V'$ is the vector space whose basis is given by $\{e_i \otimes e'_j\}$.
Given $v = v^i e_i \in V$ and $v\ = v'^j e'_j \in V'$, we define the tensor product
\[
    v \otimes v' = v^i v'^j e_i \otimes e'_j.
\]
The \emph{universal property}: given any bilinear function $f: V \times V' \to W$ for some other vector space $W$, there is a unique linear function $F: V \otimes V' \to W$ such that $f(v, v') = F(v \otimes v')$.
\[
    \begin{tikzcd}[column sep=small]
        V \times V' \rar{} \drar[swap]{f} & V \otimes V' \dar{F} \\
                                          & W
    \end{tikzcd}
\]
$f$ is bilinear, so
\[
    f(v, v') = f(v^i e_i, v'^j e'_j) = v^i v'^j f(e_i, e'_j).
\]
Setting $f(v, v') = F(v \otimes v')$,
\[
    F(v \otimes v') = v^i v'^j f(e_i, e'_j) = v^i v'^j F( e_i \otimes e'_j)
\]
so $F$ is linear and unique, satisfying our universal property.

\end{sol}

\begin{ex}

Show that this is well-defined and indeed a representation.

\end{ex}

\begin{sol}

Suppose that $\rho$ is a representation of $G$ on $V$ and $\rho'$ is a representation of $G$ on $V'$.
Then the \emph{tensor product} $\rho \otimes \rho'$ of the representations $\rho$ and $\rho'$ is the representation of $G$ on $V \otimes V'$ given by
\[
    (\rho \otimes \rho')(g)(v \otimes v') = \rho(g) v \otimes \rho'(g) v'.
\]
This is well-defined since it follows that
\[
    \rho(g)v \otimes \rho'(g)v' = v^i v'^j \rho(g)e_i \otimes \rho'(g)e'_j.
\]
Let $g, h \in G$. Then, similarly to exercise~\ref{ex:directsumrepresentation},
\begin{align*}
    (\rho \otimes \rho')(gh) &= \rho(gh) \otimes \rho'(gh) \\
        &= \bigl( \rho(g) \rho(h) \bigr) \otimes \bigl( \rho'(g) \rho'(h) \bigr) \\
        &= (\rho \otimes \rho')(g) \cdot (\rho \otimes \rho')(h)
\end{align*}
so $\rho \otimes \rho': G \to \GL(V \otimes V')$ is a homomorphism and therefore a representation of $G$ on $V \otimes V'$.

\end{sol}

\begin{ex}

Given two representations $\rho$ and $\rho'$ of $G$, show that $\rho$ and $\rho'$ are both subrepresentations of $\rho \oplus \rho'$.

\end{ex}

\begin{sol}

Suppose $\rho$ is a representation of $G$ on $V$ and suppose that $V'$ is an \emph{invariant} subspace of $V$, i.e.\ if $v \in V'$ then $\rho(g)v \in V'$ for all $g \in G$.
A \emph{subrepresentation} of $\rho$ is a representation $\rho'$ of $G$ on $V'$ satisfying $\rho'(g)v = \rho(g)v$ for all $v \in V'$.

Consider the invariant subspace $V \oplus \{0\} \subseteq V \oplus V'$.
\begin{align*}
    \rho(g) (v, 0) &= \left( \rho(g)v, 0 \right) \\
                   &= \left( \rho(g)v, \rho'(g) 0 \right) \\
                   &= (\rho \oplus \rho')(g) (v, 0)
\end{align*}
so $\rho$ is a subrepresentation of $\rho \oplus \rho'$.
By symmetry, $\rho'$ with invariant subspace $\{0\} \oplus V'$ is also a subrepresentation.

\end{sol}

\begin{ex}

Check that this is indeed a representation.

\end{ex}

\begin{sol}

For any $n \in \mathbb{Z}$, $\U(1)$ has a representation $\rho_n$ on $\mathbb{C}$ given by
\[
    \rho_n(e^{i\theta}) v = e^{in\theta} v.
\]
To see that this is a representation, we need to show that $\rho_n: \mathbb{C} \to \GL(\mathbb{C})$ is a homomorphism for all $n$.
For $\theta_1, \theta_2 \in \mathbb{R}$,
\begin{align*}
    \rho_n(e^{i\theta_1} \cdot e^{i\theta_2}) &= \rho_n(e^{i(\theta_1 + \theta_2)}) \\
        &= e^{in(\theta_1 + \theta_2)} \\
        &= e^{in\theta_1} \cdot e^{in\theta_2} \\
        &= \rho_n(e^{i\theta_1}) \rho_n(e^{i\theta_2})
\end{align*}
so $\rho_n$ is a group homomorphism. Since $\rho_n(g)$ is an invertible linear transformation on $\mathbb{C}$ for all $g \in \U(1)$, $\rho_n$ is a representation of $\U(1)$ on $\mathbb{C}$.

\end{sol}

\begin{ex}

Show that any complex 1-dimensional representation of $\U(1)$ is equivalent to one of the representations $\rho_n$.

\end{ex}

\begin{sol}

Note that since $\rho$ is a homomorphism, we require $\rho(1) = \rho(e^{i0}) = 1$.
Any complex 1-dimensional representation of $\U(1)$ will be a rescaling of $\theta$,
\[
    \rho_\alpha(e^{i\theta}) v = e^{i\alpha\theta}v, \quad \alpha \in \mathbb{R}.
\]
This is not of the form $\rho_n$ for $\alpha \not\in \mathbb{Z}$, but is equivalent by the bijection $\rho_\alpha^{-1}: e^{i\theta} \mapsto e^{i \frac{\theta}{\alpha}}$.

\end{sol}

\begin{ex}

Show that the tensor product of the representations $\rho_n$ and $\rho_m$ is equivalent to the representation $\rho_{n + m}$.

\end{ex}

\begin{sol}

By bilinearity,
\begin{align*}
    (\rho_n \otimes \rho_m) (e^{i\theta}) (v \otimes v') &= \rho_n(e^{i\theta}) v \otimes \rho_m(e^{i\theta}) v' \\
        &= e^{in\theta} v \otimes e^{im\theta} v' \\
        &= e^{in\theta} e^{im\theta} (v \otimes v') \\
        &= e^{i(n + m)\theta} (v \otimes v') \\
        &= \rho_{n + m}(e^{i\theta}) (v \otimes v'),
\end{align*}
so $\rho_n \otimes \rho_m$ is equivalent to $\rho_{n + m}$.

\end{sol}

\begin{ex}\label{ex:paulimatrices}

Show that any $2 \times 2$ matrix may be uniquely expressed as a linear combination of Pauli matrices $\sigma_0, \ldots, \sigma_3$ with complex coefficients, and that the matrix is hermitian if and only if these coefficients are real.
Show that the matrix is \emph{traceless} if and only if the coefficient of $\sigma_0$ vanishes.

\end{ex}

\begin{sol}

The Pauli matrices are
\[
    \sigma_0 = \begin{pmatrix}
            1 & 0 \\
            0 & 1
        \end{pmatrix}, \qquad
    \sigma_1 = \begin{pmatrix}
            0 & 1 \\
            1 & 0
        \end{pmatrix}, \qquad
    \tightmatrix
    \sigma_2 = \begin{pmatrix*}[r]
            0 & -i \\
            i &  0
        \end{pmatrix*}, \qquad
    \sigma_3 = \begin{pmatrix*}[r]
            1 &  0 \\
            0 & -1
        \end{pmatrix*}.
\]
A linear combination of Pauli matrices with complex coefficients $c_\mu$ looks like
\[
    \sum c_\mu \sigma_\mu =
        \begin{pmatrix*}[l]
            c_0 + c_3  & c_1 - ic_2 \\
            c_1 + ic_2 & c_0 - c_3
        \end{pmatrix*}
    = \begin{pmatrix}
        z_{11} & z_{12} \\
        z_{21} & z_{22}
    \end{pmatrix},
\]
which relates
\[
    c_0 = \frac{z_{11} + z_{22}}{2}, \quad
    c_1 = \frac{z_{12} + z_{21}}{2}, \quad
    c_2 = \frac{z_{21} - z_{12}}{2i}, \quad
    c_3 = \frac{z_{11} - z_{22}}{2}.
\]
If each $z_{ij} = 0$, each $c_\mu = 0$, so the Pauli matrices are linearly independent.
From above it is clear that they span $M(2, \mathbb{C})$ but, more directly, $\dim \bigl( M(2, \mathbb{C}) \bigr) = 4$ so linear independence implies they form a basis.

A matrix is hermitian if it is its own conjugate transpose, so here we would have
\[
        \begin{pmatrix*}[l]
            c_0 + c_3  & c_1 - ic_2 \\
            c_1 + ic_2 & c_0 - c_3
        \end{pmatrix*}
        =
        \begin{pmatrix*}[l]
            c_0^* + c_3^*  & c_1^* - ic_2^* \\
            c_1^* + ic_2^* & c_0^* - c_3^*
        \end{pmatrix*}
\]
which implies $c_\mu \in \mathbb{R}$.

Taking the trace,
\[
    \tr \begin{pmatrix*}[l]
                c_0 + c_3  & c_1 - ic_2 \\
                c_1 + ic_2 & c_0 - c_3
              \end{pmatrix*}
        = 2 c_0
\]
so the matrix is traceless if and only if $c_0 = 0$.

\end{sol}

\begin{ex}\label{ex:sigmaisigmajisisigmak}

For $i = 1, 2, 3$, show that
\[
    \sigma_i^2 = 1
\]
and show that if $(i, j, k)$ is a cyclic permutation of $(1, 2, 3)$ then
\[
    \sigma_i \sigma_j = -\sigma_j \sigma_i = \sqrt{-1} \sigma_k.
\]
\end{ex}

\begin{sol}

The result $\sigma_i^2 = 1$ follows from direct computation.

Taking cyclic products $\sigma_i \sigma_j$, we get
\[
    \sigma_1 \sigma_2
    =
    \begin{pmatrix}
        0 & 1 \\
        1 & 0
    \end{pmatrix}
    \tightmatrix
    \begin{pmatrix*}[r]
        0 & -i \\
        i &  0
    \end{pmatrix*}
    =
    \begin{pmatrix*}[r]
        i &  0 \\
        0 & -i
    \end{pmatrix*}
    = i\sigma_3
\]
and, similarly, $\sigma_2 \sigma_3 = i\sigma_1$ and $\sigma_3 \sigma_1 = i \sigma_2$, so
\[
    \sigma_i \sigma_j = i\sigma_k, \qquad
    \sigma_j \underbrace{\sigma_k \sigma_k}_{\sigma_0} \sigma_i
        = i \sigma_i \cdot i \sigma_j \\
        = -\sigma_i \sigma_j.
\]

\end{sol}

\begin{ex}\label{ex:su2isunit3sphere}

Show that the determinant of the $2 \times 2$ matrix $a + bI + cJ + dK$ is $a^2  + b^2 + c^2 + d^2$.
Show that if $a, b, c, d$ are real and $a^2 + b^2 + c^2 + d^2 = 1$, this matrix is unitary.
Conclude that $\SU(2)$ is the unit sphere in $\mathbb{H}$.

\end{ex}

\begin{sol}

We have quaternions
\[
    I = -i\sigma_1, \qquad
    J = -i\sigma_2, \qquad
    K = -i\sigma_3.
\]
The matrix $U = a + bI + cJ + dK$ is
\begin{align*}
    U &= a \sigma_0 - ib\sigma_1 -ic \sigma_2 -id \sigma_3 \\
    &= \begin{pmatrix}
        a & 0 \\
        0 & a
    \end{pmatrix}
    -
    \begin{pmatrix}
        0 & ib \\
        ib & 0
    \end{pmatrix}
    -
    \begin{pmatrix}
        \,\, 0 & c \\
        -c &  0
    \end{pmatrix}
    -
    \begin{tightmatrix}\begin{pmatrix}
        id & \,\, 0 \\
        0 & -id
    \end{pmatrix}\end{tightmatrix} \\
    &=
    \begin{pmatrix*}[r]
        a - id  & -ib - c \\
        -ib + c & a + id
    \end{pmatrix*},
\end{align*}
so
\[
    \det(U) = a^2 + b^2 + c^2 + d^2.
\]
Imposing $\det(U) = 1$, we get the inverse and conjugate transpose
\[
    U^{-1} =
        \begin{pmatrix}
            a + id & ib + c \\
            ib - c & a - id
        \end{pmatrix}, \qquad
    U^\dagger =
        \begin{pmatrix}
            a^* + id^* & ib^* + c^* \\
            ib^* -c^* & a^* - id^*
        \end{pmatrix}
\]
so if $a, b, c, d \in \mathbb{R}$, $U^{-1} = U^\dagger$ is unitary.
Since we required that $\det(U) = 1$, $U \in \SU(2)$.

$\det(U) = 1$ describes $S^3$, so $\SU(2)$ is the unit 3-sphere in $\mathbb{H}$ since each point as a quaternion lies on $S^3$.

\end{sol}

\begin{ex}

Show that the spin-0 representation of $\SU(2)$ is equivalent to the \emph{trivial} representation in which every element of the group acts on $\mathbb{C}$ as the identity.

\end{ex}

\begin{sol}

Let $\mathcal{H}_j$ be the space of homogeneous polynomials of degree $2j$ on $\mathbb{C}^2$.
For a vector $(x, y) \in \mathbb{C}^2$, $\mathcal{H}_j$ has the monomial basis $\{x^p y^q\}$ with $p + q = 2j$.

For any $g \in \SU(2)$, let $U_j(g)$ be the linear transformation of $\mathcal{H}_j$ given by
\[
    \bigl(U_j(g) f\bigr) v = f(g^{-1} v)
\]
for all $f \in \mathcal{H}_j$, $v \in \mathbb{C}^2$.

$\dim(\mathcal{H}_j) = 2j + 1$, so the spin-0 representation is 1-dimensional.
The basis for $\mathcal{H}_0$ is $\{1\}$, so any $f \in \mathcal{H}_0$ is of the form $f(x, y) = f_0$ where $f_0$ is constant.
\begin{align*}
    \bigl( U_0(g) f \bigr) v &= f(g^{-1} v) \\
                             &= f_0
\end{align*}
so $U_0(g) f = f$, implying $U_0(g)$ is the identity for all $g \in \SU(2)$.

\end{sol}

\begin{ex}\label{ex:spinhalffundamentalrepresentation}

Show that the spin-$\frac{1}{2}$ representation of $\SU(2)$ is equivalent [to] the fundamental representation, in which every element $g \in \SU(2)$ acts on $\mathbb{C}^2$ by matrix multiplication.

\end{ex}

\begin{sol}

The basis for $\mathcal{H}_\frac{1}{2}$ is $\{x, y\}$, so
\[
    f(x, y) = f_1 x + f_2 y =
    \begin{pmatrix}
        f_1 & f_2
    \end{pmatrix}
    \begin{pmatrix}
        x \\ \, y \,
    \end{pmatrix}
\]
Denote $f = \begin{psmallmatrix} f_1 \\ f_2 \end{psmallmatrix}$, so $f(x, y) \equiv \innerp{f}{v}$.
Then, since $g^{-1} = g^\dagger$,
\begin{align*}
    \bigl( U_\frac{1}{2}(g) f \bigr) v &= \innerp{f}{g^\dagger v} \\
                                       &= \innerp{gf}{v}.
\end{align*}

\end{sol}

\begin{ex}

Show that for any representation $\rho$ of a group $G$ on a vector space $V$ there is a \emph{dual} or \emph{contragredient} representation $\rho^*$ of $G$ on $V^*$, given by
\[
    \bigl( \rho^*(g) f \bigr) (v) = f \bigl( \rho(g^{-1}) v \bigr)
\]
for all $v \in V$, $f \in V^*$.
Show that all the representations $U_j$ of $\SU(2)$ are equivalent to their duals.

\end{ex}

\begin{sol}

$\rho^*$ is a homomorphism since
\[
    \bigl( \rho^*(\id) f \bigr) v = f \bigl(\rho(\id) v \bigr) = f(v),
\]
i.e.\ $\rho^*$ preserves the identity, and
\begin{align*}
    \bigl( \rho^*(gh) f \bigr) (v) &= f \bigl( \rho(h^{-1} g^{-1}) v \bigr) \\
        &= f \bigl( \rho(h^{-1}) \rho(g^{-1}) v \bigr) \\
        &= \bigl( \rho^*(h) f \bigr) \rho(g^{-1}) v \\
        &= \bigl( \rho^*(g) \rho^*(h) f \bigr) v
\end{align*}
so $\rho^*(gh) = \rho^*(g) \rho^*(h)$ and $\rho^*$ is a representation of $G$ on $\GL(V^*)$.

For $U_j$ a representation of $\SU(2)$,
\begin{align*}
    \bigl( U_j^*(g) f \bigr) (v) &= f \bigl( U_j(g^{-1}) v \bigr) \\
                                 &= f \bigl( (U_j(g^{-1}) \id) v \bigr) \\
                                 &= f \bigl( g v \bigr)
\end{align*}
so representations of $\SU(2)$ are equivalent to their duals (isomorphic via the adjoint).

\end{sol}

\begin{ex}\label{ex:commutativescalarmultipleidentity}

Show that if $S$ is a $2 \times 2$ matrix commuting with all $2 \times 2$ traceless hermitian matrices, $S$ is a scalar multiple of the identity matrix.
(One approach is to suppose $S$ commutes with the Pauli matrices $\sigma_1, \sigma_2, \sigma_3$ and derive equations its matrix entries must satisfy.)
% Erratum: the Pauli matrices are listed as \sigma_1, \sigma_2, \sigma_2 in the book.

\end{ex}

\begin{sol}

Let
\[
    S = \begin{pmatrix}
            s_{11} & s_{12} \\
            s_{21} & s_{22}
        \end{pmatrix}
\]
commute with the Pauli matrices.
\begin{gather*}
    S \sigma_1 =
        \begin{pmatrix}
            s_{11} & s_{12} \\
            s_{21} & s_{22}
        \end{pmatrix}
        \begin{pmatrix}
            0 & 1 \\
            1 & 0
        \end{pmatrix}
        =
        \begin{pmatrix}
            s_{12} & s_{11} \\
            s_{22} & s_{21}
        \end{pmatrix}, \\
    \sigma_1 S =
        \begin{pmatrix}
            0 & 1 \\
            1 & 0
        \end{pmatrix}
        \begin{pmatrix}
            s_{11} & s_{12} \\
            s_{21} & s_{22}
        \end{pmatrix}
        =
        \begin{pmatrix}
            s_{21} & s_{22} \\
            s_{11} & s_{12}
        \end{pmatrix},
\end{gather*}
so $s_{11} = s_{22}$ and $s_{12} = s_{21}$.
\begin{gather*}
    S \sigma_3 =
        \begin{pmatrix}
            s_{11} & s_{12} \\
            s_{12} & s_{11}
        \end{pmatrix}
        \begin{tightmatrix}\begin{pmatrix*}[r]
            1 &  0 \\
            0 & -1
        \end{pmatrix*}\end{tightmatrix}
        =
        \begin{tightmatrix}\begin{pmatrix*}[r]
            s_{11} & -s_{12} \\
            s_{12} & -s_{11}
        \end{pmatrix*}\end{tightmatrix}, \\
    \sigma_3 S =
        \begin{tightmatrix}\begin{pmatrix*}[r]
            1 &  0 \\
            0 & -1
        \end{pmatrix*}\end{tightmatrix}
        \begin{pmatrix}
            s_{11} & s_{12} \\
            s_{12} & s_{11}
        \end{pmatrix}
        =
        \begin{tightmatrix}\begin{pmatrix*}[r]
            s_{11}  & s_{12} \\
            -s_{12} & -s_{11}
        \end{pmatrix*}\end{tightmatrix},
\end{gather*}
so $s_{12} = -s_{12} = 0$. This gives us
\[
    S = \begin{pmatrix}
            s_{11} & 0 \\
            0      & s_{11}
        \end{pmatrix}
        = s_{11} \cdot \id.
\]

\end{sol}

\begin{ex}

Using the fact that $\GL(3, \mathbb{R})$ is a subgroup of $\GL(3, \mathbb{C})$, we can think of $\rho$ as a homomorphism from $\SU(2)$ to $\GL(3, \mathbb{C})$, or in other words, a representation of $\SU(2)$ on $\mathbb{C}^3$.
Show that this is equivalent to the spin-1 representation of $\SU(2)$.

\end{ex}

\begin{sol}

With $T = T^i\sigma_i$, we can identify the space of $2 \times 2$ hermitian matrices with $\mathbb{R}^3 \subset \mathbb{C}^3$.
The homomorphism $\rho: \SU(2) \to \GL(3, \mathbb{C})$ is given by
\[
    \rho(g) T = g T g^{-1}
\]
and is a representation of $\SU(2)$ on $\mathbb{C}^3$.

In the spin-1 representation, we have polynomials of the form
\begin{align*}
    f(x, y) &= f_{11} x^2 + (f_{12} + f_{21}) xy + f_{22} y^2 \\
            &= \begin{pmatrix}
                    x & y
               \end{pmatrix}
               \begin{pmatrix}
                    f_{11} & f_{12} \\
                    f_{21} & f_{22}
               \end{pmatrix}
               \begin{pmatrix}
                    x \\ \, y \,
               \end{pmatrix} \\
            &= v^* T v.
\end{align*}
Then
\begin{align*}
    \SwapAboveDisplaySkip
    \bigl(U_1(g) f \bigr) v &= f(g^{-1} v) \\
                            &= {(g^{-1} v)}^* T (g^{-1} v) \\
                            &= v^* g T g^{-1} v \\
                            &= (gfg^{-1}) (v)
\end{align*}
where $gfg^{-1}: v \mapsto v^* gTg^{-1} v$,
so $U_1(g) f = gfg^{-1}$ and therefore the spin-1 representation $U_1$ of $\SU(2)$ is equivalent to the representation $\rho$ above.

\end{sol}

\begin{ex}

Show that the cocycle automatically satisfies the \emph{cocycle condition}
\[
    e^{i\theta(g, h)} e^{i\theta(gh, k)} = e^{i\theta(g, hk)} e^{i\theta(h, k)}.
\]

\end{ex}

\begin{sol}

For projective unitary representations,
\[
    \rho(g) \rho(h) = e^{i\theta(g, h)} \rho(gh).
\]
For $g, h, k$,
\begin{align*}
    \SwapAboveDisplaySkip
    \rho(g) \rho(h) \rho(k) &= e^{i\theta(g, h)} \rho(gh) \rho(k) \\
                            &= e^{i\theta(g, h)} e^{i\theta(gh, k)} \rho(ghk)
\end{align*}
and
\begin{align*}
    \SwapAboveDisplaySkip
    \rho(g) \rho(h) \rho(k) &= \rho(g) e^{i\theta(h, k)} \rho(hk) \\
                            &= e^{i\theta(g, hk)} e^{i\theta(h, k)} \rho(ghk)
\end{align*}
so equating gives $e^{i\theta(g, h)} e^{i\theta(gh, k)} = e^{i\theta(g, hk)} e^{i\theta(h, k)}$.

\end{sol}

\begin{ex}

Show this.
(Hint: show that if the cocycle were inessential we would have $U_j(-1) = 1$, which is not true for $j$ a half-integer.)

\end{ex}

\begin{sol}

In general, we have $\rho(gh) = e^{i\theta(g, h)} \rho(g) \rho(h)$.

We have the double cover $\rho: \SU(2) \to \SO(3)$.
Let $U_j$ be the spin-$j$ representation of $\SU(2)$.
For each $h \in \SO(3)$, pick $g \in SU(2)$ such that $\rho(g) = h$ and define the projective unitary representation of $\SO(3)$ as $V_j(h) = U_j(g)$.
Since both $g$ and $-g$ cover $h$, the choice is arbitrary up to the sign.

But
\[
    U_j(g) = \begin{dcases*}
                \phantom{-} U_j(-g) & \text{(bosonic)}, \\
                -U_j(-g)            & \text{(fermionic)}
             \end{dcases*}
\]
so, unlike the bosonic case, $V_j$ is not indepenedent of the choice of $g$.
Then
\[
    V_j(hh') = U_j(\pm gg') = \pm U_j(g) U_j(g') = \pm V_j(h) V_j(h')
\]
so $V_j$ is a projective representation with cocycle $\pm1$.

If the cocycle is inessential, there exists $h, h'$ such that $\theta(h, h') = 0$.
This implies $V_j(hh') = V_j(h) V_j(h')$ necessarily, but picking $-g$,
\begin{align*}
    V_j(hh') &= U_j(-gg') \\
             &= U_j(-1) U_j(g) U_j(g') \\
             &= U_j(-1) V_j(h) V_j(h')
\end{align*}
so we require $U_j(-1) = 1$, which is not true for fermions and therefore the cocycle is essential.

\end{sol}

\begin{ex}\label{ex:minkowskispacepaulimatrices}

Suppose that $x \in \mathbb{R}^4$. Show that $x^\mu x_\mu$ as computed using the Minkowski metric
\[
    x^\mu x_\mu = -x_0^2 + x_1^2 + x_2^2 + x_3^2
\]
is equal to minus the determinant of the matrix $x^\mu \sigma_\mu$ (which is to be understood using the Einstein summation convention).

\end{ex}

\begin{sol}

By direct calculation,
\begin{align*}
    x^\mu \sigma_\mu &= \begin{pmatrix*}[l]
                            x^0 & 0 \\
                            0   & x^0
                        \end{pmatrix*}
                        + \begin{pmatrix*}[l]
                            0   & x^1 \\
                            x^1 & 0
                          \end{pmatrix*}
                        + \begin{tightmatrix}\begin{pmatrix}
                            0     & -i x^2 \\
                            i x^2 &  \phantom{-} 0
                          \end{pmatrix}\end{tightmatrix}
                        + \begin{tightmatrix}\begin{pmatrix*}[l]
                            x^3    & \phantom{-} 0 \\
                            \, 0   & -x^3
                          \end{pmatrix*}\end{tightmatrix} \\
        &= \begin{pmatrix*}[l]
            x^0 + x^3 & x^1 - ix^2 \\
            x^1 + ix^2 & x^0 - x^3
          \end{pmatrix*}
\end{align*}
so
\begin{align*}
    \SwapAboveDisplaySkip
    -\det(x^\mu \sigma_\mu) &= - (x^0 + x^3) (x^0 - x^3) + (x^1 - ix^2)(x^1 + ix^2) \\
                            &= - {(x^0)}^2 + {(x^1)}^2 + {(x^2)}^2 + {(x^3)}^2 \\
                            &= x^\mu x_\mu.
\end{align*}

\end{sol}

\begin{ex}\label{ex:slrepresentation}

Let $M$ denote the space of $2 \times 2$ hermitian complex matrices, a 4-dimensional real vector space with basis given by the Pauli matrices $\sigma_\mu$.
Let $\rho$ be the representation of $\SL(2, \mathbb{C})$ on $M$ by
\[
    \rho(g) T = g T g^{-1}.
\]
Using the identification [of] $M$ with Minkowski space given by
\begin{align*}
    \mathbb{R}^4 &\to M \\
    x            &\mapsto x^\mu \sigma_\mu,
\end{align*}
show using \hyperref[ex:minkowskispacepaulimatrices]{the previous exercise} that $\rho$ preserves the Minkowski metric and hence defines a homomorphism
\[
    \rho: \SL(2, \mathbb{C}) \to \O(3, 1).
\]

\end{ex}

\begin{sol}

From exercise~\ref{ex:paulimatrices}, any $T \in M$ can be written as $T = T^\mu \sigma_\mu$ with $T^\mu$ real.
From exercise~\ref{ex:minkowskispacepaulimatrices}, we therefore have $\det(T) = -T^\mu T_\mu$.

As
\[
    \det\bigl( \rho(g) T \bigr) = \det(g T g^{-1}) = \det(T) = -T^\mu T_\mu,
\]
$\rho$ preserves the Minkowski metric on $M$ and therefore $\rho: \SL(2, \mathbb{C}) \to \O(3, 1)$ is a homomorphism.

\end{sol}

\begin{ex}

Show that the range of $\rho: \SL(2, \mathbb{C}) \to \O(3, 1)$ lies in $\SO_0(3, 1)$.

\end{ex}

\begin{sol}

Consider $\id_M = \sigma_0$. Then $\rho: \sigma_0 \mapsto \id \in \SO_0(3, 1)$.
Since $\SL(2, \mathbb{C})$ is connected and $\rho$ is continuous, $\rho$ must map every element of $\SL(2, \mathbb{C})$ to the connected component of $\O(3, 1)$,
\[
    \rho: \SL(2, \mathbb{C}) \to \SO_0(3, 1).
\]

\end{sol}

\begin{ex}

Show that $\rho$ is two-to-one. In fact, $\rho$ is also onto, so $\SL(2, \mathbb{C})$ is a double cover of the connected Lorentz group $\SO_0(3, 1)$.

\end{ex}

\begin{sol}

Note that $\rho$ is at least two-to-one, since
\[
    \rho(-g)T = (-g) T {(-g)}^{-1} = gTg^{-1} = \rho(g) T
\]
implies $\rho(-g) = \rho(g)$. Suppose $\rho(g) = \rho(h)$; then
\[
    \rho(gh^{-1}) = \rho(g) {\rho(h)}^{-1} = 1
\]
which requires that $gh^{-1}$ commutes with all matrices $T \in M$, which, per exercise~\ref{ex:commutativescalarmultipleidentity}, implies that $gh^{-1}$ is a scalar multiple of the identity.
The only scalar multiples of the identity in $\SO_0(3, 1)$ are $\pm \id$, so $h = \pm g$ and $\rho$ is two-to-one.
Since $\rho$ is also surjective, it is a double cover of $\SO_0(3, 1)$.

\end{sol}

\begin{ex}

Investigate the finite-dimensional representations of $\SL(2, \mathbb{C})$ and $\SO(3, 1)$, copying the techniques used above for $\SU(2)$ and $\SO(3)$.

\end{ex}

\begin{sol}

Since $\O(3, 1) \subset \GL(4, \mathbb{R})$, the homomorphism $\rho$ from exercise~\ref{ex:slrepresentation} is a representation of $\SL(2, \mathbb{C})$ on $\mathbb{R}^4$.

Similar to the $\SU(2) \to \SO(3)$ case where we construct a representation of $\SO(3)$ using its double cover, for each $h \in \SO(3, 1)$, pick $g \in \SL(2, \mathbb{C})$ with $\rho(g) = h$ and define the projective representation as $Q_j(h) = P_j(g)$ where $P_j$ is the spin-$j$ representation of $\SL(2, \mathbb{C})$.
Proceed analogously to get the spin-$j$ projective representations of $\SO(3, 1)$.

\end{sol}

\section{Lie Algebras}

\begin{ex}

For analysts: show that this sum converges.

\end{ex}

\begin{sol}

The exponential of a square matrix $T$ is
\begin{align*}
    \exp(T) &= 1 + T + \frac{T^2}{2!} + \frac{T^3}{3!} + \cdots \\
            &= \sum_{n = 0}^\infty \frac{T^n}{n!}.
\end{align*}
Since the space of square matrices is a vector space, we can take any submultiplicative matrix norm. As
\[
    \frac{\norm{T^n}}{n!} \leq \frac{\norm{T}^n}{n!},
\]
we get
\[
    \sum_{n = 0}^\infty \frac{\norm{T^n}}{n!} \leq \sum_{n = 0}^\infty \frac{\norm{T}^n}{n!} = e^{\norm{T}}
\]
so the series $\sum \frac{\norm{T^n}}{n!}$ converges and therefore, by normal convergence, $\exp(T)$ does too.

\end{sol}

\begin{ex}

Show that the matrix describing a counterclockwise rotation of angle $t$ about the unit vector $n = (n^x, n^y, n^z) \in \mathbb{R}^3$ is given by
\[
    \exp \bigl( t(n^x \! J_x + n^y \! J_y + n^z \! J_z) \bigr).
\]

\end{ex}

\begin{sol}

The matrices
\[
    J_x =
        \begin{pmatrix*}[r]
            0 & 0 & 0 \\
            0 & 0 & \m 1 \\
            0 & 1 & 0 \\
        \end{pmatrix*}, \quad
    J_y =
        \begin{pmatrix*}[r]
            0  & 0 & 1 \\
            0  & 0 & 0 \\
            -1 & 0 & 0 \\
        \end{pmatrix*}, \quad
    J_z =
        \begin{pmatrix*}[r]
            0 & \m 1 & 0 \\
            1 & 0 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix*}
\]
form a basis for $\mathfrak{so}(3)$.

Denote
\begin{align*}
    \SwapAboveDisplaySkip
    N &= n^x \! J_x + n^y \! J_y + n^z \! J_z \\
      &= \begin{tightmatrix}\begin{pmatrix*}[r]
            0 \phantom{^z} & -n^z  & n^y \\
            n^z            & 0 \phantom{^x} & -n^x \\
            -n^y           & n^x            & 0 \phantom{^x}
         \end{pmatrix*}\end{tightmatrix}.
\end{align*}
The characteristic polynomial is
\begin{align*}
    p_N(\lambda) &= \det(N - \lambda \cdot \id) \\
        %&= -\lambda \bigl(\lambda^2 + {(n^x)}^2 \bigr)
        %   + n^z(-\lambda n^z - n^x n^y) + n^y(n^x n^z -\lambda n^y) \\
        &= -\lambda^3 - \lambda
\end{align*}
and by the Cayley--Hamilton theorem, $p_N(N) = 0$ implies $N^3 = -N$, so $N^4 = -N^2$ and so on.
Therefore we only need to calculate
\begin{align*}
    N^2 &= \begin{tightmatrix}\begin{pmatrix}
        -{(n^y)}^2 - {(n^z)}^2 & n^x n^y & n^x n^z \\
        n^x n^y & -{(n^z)}^2 - {(n^x)}^2 & n^y n^z \\
        n^x n^z & n^y n^z                & -{(n^x)}^2 - {(n^y)}^2
    \end{pmatrix}\end{tightmatrix} \\
       &= \begin{pmatrix}
        {(n^x)}^2 & n^x n^y   & n^x n^z \\
        n^x n^y   & {(n^y)}^2 & n^y n^z \\
        n^x n^z   & n^y n^z   & {(n^z)}^2
    \end{pmatrix}
    - \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
\end{align*}
as ${(n^i)}^2 = 1 - {(n^j)}^2 - {(n^k)}^2$.

Exponentiating,
\begin{align*}
    \exp (tN)
        &= \id + tN + \frac{t^2}{2!} N^2 + \frac{t^3}{3!} N^3
                     + \frac{t^4}{4!} N^4 + \cdots \\
        &= \id + tN + \frac{t^2}{2!} N^2 - \frac{t^3}{3!} N
                     - \frac{t^4}{4!} N^2 + \cdots \\
        &= \id + \left( t - \frac{t^3}{3!} + \cdots \right) N
              + \left( \frac{t^2}{2!} - \frac{t^4}{4!} + \cdots \right) N^2 \\
        &= \id + \sin(t) N + \bigl(1 - \cos(t) \bigr) N^2
\end{align*}
which reproduces the matrix form of Rodrigues' rotation formula for a rotation about $n$ by an angle $t$.

%If we denote $N^2 = N \otimes N - \id$,
%\[
%    \exp(tN) = \id \cdot \cos(t) + \sin(t) N + \bigl(1 - \cos(t) \bigr) N \otimes N
%\]
%which is the standard vector form.

\end{sol}

\begin{ex}

Check this!

\end{ex}

\begin{sol}

The claim is that if we consider the difference
\[
    \exp(s J_x) \exp(t J_y) - \exp(t J_y) \exp(s J_x)
\]
and expand it as a power series in $s$ and $t$, keeping only the lowest-order terms, we obtain $st(J_x J_y - J_y J_x) + \text{higher order terms}$.
\begin{align*}
    &\exp(s J_x) \exp(t J_y) - \exp(t J_y) \exp(s J_x) \\
    &\qquad\begin{aligned}
        &= (\id + sJ_x + \cdots) (\id + tJ_y + \cdots)
           - (\id + tJ_y + \cdots) (\id + sJ_x + \cdots) \\
        &= (\id + sJ_x + tJ_y + st J_x J_y + \cdots)
           - (\id + sJ_x + tJ_y + st J_y J_x + \cdots) \\
        &= st(J_x J_y - J_y J_x) + \cdots.
    \end{aligned}
\end{align*}

\end{sol}

\begin{ex}\label{ex:so3basis}

Show that
\[
    J_x^2 = J_y^2 = J_z^2 = -1
\]
and
\[
    [J_x, J_y] = J_z, \quad
    [J_y, J_z] = J_x, \quad
    [J_z, J_x] = J_y.
\]
Note the resemblance to vector cross products and quaternions, but also the differences.

\end{ex}

\begin{sol}

By direct calculation,
\[
    J_x^2 = \begin{pmatrix*}[r]
            0 &    0 & 0 \\
            0 & \m 1 & 0 \\
            0 &    0 & \m 1
        \end{pmatrix*}, \quad
    J_y^2 = \begin{pmatrix*}[r]
            -1 & 0 & 0 \\
             0 & 0 & 0 \\
             0 & 0 & \m 1
        \end{pmatrix*}, \quad
    J_z^2 = \begin{pmatrix*}[r]
            -1 &    0 & 0 \\
             0 & \m 1 & 0 \\
             0 &    0 & 0
        \end{pmatrix*}
\]
so the first statement is false.

The commutators are
\begin{align*}
    [J_x, J_y] &=
        \begin{pmatrix}
            0 & 0 & 0 \\
            1 & 0 & 0 \\
            0 & 0 & 0
        \end{pmatrix}
        - \begin{pmatrix}
            0 & 1 & 0 \\
            0 & 0 & 0 \\
            0 & 0 & 0
        \end{pmatrix}
        = J_z, \\
    [J_y, J_z] &=
        \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 \\
            0 & 1 & 0
        \end{pmatrix}
        - \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 0 & 1 \\
            0 & 0 & 0
        \end{pmatrix}
        = J_x, \\
    [J_z, J_x] &=
        \begin{pmatrix}
            0 & 0 & 1 \\
            0 & 0 & 0 \\
            0 & 0 & 0
        \end{pmatrix}
        - \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 \\
            1 & 0 & 0
        \end{pmatrix}
        = J_y.
\end{align*}
For $\mathbb{R}^3$ with basis vectors $\{\vec{\imath}, \vec{\jmath}, \vec{k}\}$, the cross product satisfies
\[
    \vec{\imath} \times \vec{\jmath} = \vec{k}, \qquad
    \vec{\jmath} \times \vec{k} = \vec{\imath}, \qquad
    \vec{k} \times \vec{\imath} = \vec{\jmath},
\]
so $\mathbb{R}^3$ with the cross product as Lie bracket forms the Lie algebra $\mathfrak{so}(3)$.
More generally, since the Hodge star maps $\Lambda^2 V \to V$ for $V$ an orientable 3-dimensional inner product space, we get that $\Lambda^2 V$ is isomorphic to $\mathfrak{so}(3)$.

\end{sol}

\begin{ex}\label{ex:exptTidentities}

Suppose $T$ is any $n \times n$ complex matrix. Show that
\[
    \exp\bigl( (s + t) T \bigr) = \exp(sT) \exp(tT)
\]
by a power series calculation. (Hint: use the binomial theorem.)
Show that for a fixed $T$, $\exp(tT)$ is a smooth function from $t \in \mathbb{R}$ to the $n \times n$ matrices.
Show that $\exp(tT)$ is the identity when $t = 0$ and that
\[
    \frac{d}{dt} \exp(tT) \Big|_{t = 0} = T.
\]

\end{ex}

\begin{sol}

Expressing the exponential of $(s + t)T$ as a power series,
\begin{align*}
    \exp\bigl( (s + t) T \bigr) &= \sum_{n = 0}^\infty \frac{{(s + t)}^n T^n}{n!} \\
        &= \sum_{n = 0}^\infty \sum_{k = 0}^n \binom{n}{k} \frac{s^{n - k} t^k T^n}{n!} \\
        &= \sum_{n = 0}^\infty \sum_{k = 0}^n \frac{s^{n - k} t^k T^n}{k! (n - k)!} \\
        &= \sum_{n = 0}^\infty \sum_{k = 0}^n \frac{s^{n - k} T^{n - k}}{(n - k)!} \frac{t^k T^k}{k!} \\
        &= \sum_{n = 0}^\infty \frac{s^n T^n}{n!} \cdot \sum_{n = 0}^\infty \frac{t^n T^n}{n!} \\
        &= \exp(st) \exp(tT)
\end{align*}
by the Cauchy product formula.

For a fixed $T$, the function
\begin{align*}
    f_T: t \mapsto \exp(tT) &= \sum_{n = 0}^\infty \frac{{t^n}T^n}{n!} \\
        &= \id + tT + \frac{{(tT)}^2}{2} + \frac{{(tT)}^3}{3!} + \cdots
\end{align*}
is smooth since it is polynomial in $t$.

From the above expansion, we get that $\displaystyle \lim_{t \to 0} \exp(tT) = \id$.
Differentiating,
\begin{align*}
    \frac{d}{dt} \exp(tT) &= \frac{d}{dt} \sum_{n = 0}^\infty \frac{{t^n}T^n}{n!} \\
        &= \sum_{n = 1}^\infty \frac{t^{n - 1} T^n}{(n - 1)!} \\
        &= T \sum_{n = 1}^\infty \frac{t^{n - 1} T^{n - 1}}{(n - 1)!} \\
        &= T \exp(tT)
\end{align*}
so at $t = 0$,
\[
    \frac{d}{dt} \exp(tT) \Big|_{t = 0} = T.
\]

\end{sol}

\begin{ex}

Show that the Lie algebra $\mathfrak{gl}(n, \mathbb{C})$ of $\GL(n, \mathbb{C})$ consists of all $n \times n$ complex matrices.
Show that the Lie algebra $\mathfrak{gl}(n, \mathbb{R})$ of $\GL(n, \mathbb{R})$ consists of all $n \times n$ real matrices.

\end{ex}

\begin{sol}

Let $\gamma(t)$ be a path in $\GL(n, \mathbb{C})$ with $\gamma(0) = \id$.
We require only that $\det\bigl(\gamma(t)\bigr) \neq 0$.

Let $\gamma(t) = \exp(tT)$ so, from exercise~\ref{ex:exptTidentities}, $\gamma'(0) = T$.
By \hyperref[ex:detexptrace]{the next exercise}, our requirement is equivalent to $e^{t \tr(T)} \neq 0$.
This holds for any $n \times n$ complex matrix $T$, so $\mathfrak{gl}(n, \mathbb{C}) = M(n, \mathbb{C})$.

The same argument holds when restricting the field to $\mathbb{R}$, so $\mathfrak{gl}(n, \mathbb{R}) = M(n, \mathbb{R})$.

\end{sol}

\begin{ex}\label{ex:detexptrace}

Show that for any matrix $T$,
\[
    \det\bigl(\exp(T)\bigr) = e^{\tr(T)}.
\]
(Hint: first show it for diagonalizable matrices, then use the fact that these are dense in the space of all matrices.)
Use this to show that the Lie algebra $\mathfrak{sl}(n, \mathbb{C})$ of $\SL(n, \mathbb{C})$ consists of all $n \times n$ traceless complex matrices,
while the Lie algebra $\mathfrak{sl}(n, \mathbb{R})$ of $\SL(n, \mathbb{R})$ consists of all $n \times n$ traceless real matrices.

\end{ex}

\begin{sol}

Let $T$ be diagonalizable and write $T = SDS^{-1}$ with $D = \mathrm{diag}(\lambda_1, \ldots, \lambda_n)$.
\begin{align*}
    \exp(T) &= \exp(SDS^{-1}) \\
            %&= \sum_{n = 0}^\infty \frac{{(SDS^{-1})}^n}{n!} \\
            &= \sum_{n = 0}^\infty \frac{S D^n S^{-1}}{n!} \\
            &= S \exp(D) S^{-1}
\end{align*}
so
\begin{align*}
    \SwapAboveDisplaySkip
    \det\bigl(\exp(T)\bigr) &= \det\bigl(S \exp(D) S^{-1} \bigr) \\
        %&= \det(S) \det\bigl(\exp(D)\bigr) \det(S^{-1}) \\
        &= \det\bigl(\exp(D)\bigr) \\
        &= \prod_{i = 1}^n e^{\lambda_i} \\[-1\jot]
        &= \exp \biggl(\sum_{i = 1}^n \lambda_i \! \biggr) \\
        &= e^{\tr(D)} \\
        %&= e^{\tr(SS^{-1}D)} \\
        %&= e^{\tr(SDS^{-1})} \\
        &= e^{\tr(T)}.
\end{align*}
Since diagonalizable matrices are dense in the space of matrices, this holds for all $n \times n$ matrices.

Let $\gamma(t)$ be a path in $\SL(n, \mathbb{C})$ with $\gamma(0) = \id$. We require only that $\det\bigl(\gamma(t)\bigr) = 1$.
Let $\gamma(t) = \exp(tT)$ so, from exercise~\ref{ex:exptTidentities}, $\gamma'(0) = T$. Then our condition becomes $e^{t \tr(T)} = 1$ so $\tr(T) = 0$ and $\mathfrak{sl}(n, \mathbb{C})$ is all $n \times n$ traceless complex matrices.

The same argument holds when restricting the field to $\mathbb{R}$, so $\mathfrak{sl}(n, \mathbb{R})$ is all $n \times n$ traceless real matrices.

\end{sol}

\begin{ex}\label{ex:lorentzliealgebra}

Let $g$ be a metric of signature $(p, q)$ on $\mathbb{R}^n$, where $p + q = n$.
Show that the Lie algebra $\mathfrak{so}(p, q)$ of $\SO(p, q)$ consists of all real $n \times n$ matrices $T$ with
\[
    g(Tv, w) = -g(v, Tw)
\]
for all $v, w \in \mathbb{R}^n$.
Show that the dimension of $\mathfrak{so}(p, q)$, hence that of $\SO(p, q)$, is $\frac{n(n - 1)}{2}$.
Determine an explicit basis of the Lorentz Lie algebra, $\mathfrak{so}(3, 1)$.

\end{ex}

\begin{sol}\label{sol:lorentzliealgebra}

Let $\gamma(t)$ be a path in $\SO(p, q)$ with $\gamma(0) = \id$. Then for any $v, w \in \mathbb{R}^n$,
\[
    g\bigl(\gamma(t)v, \gamma(t) w\bigr) = g(v, w)
\]
for all $t$. Letting $\gamma'(0) = T$ and differentiating at $t = 0$,
\begin{align*}
    \frac{d}{dt} g\bigl(\gamma(t)v, \gamma(t) w\bigr) \Big|_{t = 0} \! &=
        \frac{d}{dt} \bigl(
                g_{\mu\nu} {{\gamma(t)}^\mu}_\rho v^\rho {{\gamma(t)}^\nu}_\sigma w^\sigma
            \bigr) \Big|_{t = 0} \\
        &= g_{\mu\nu} \bigl(
                {{\gamma(t)}^\mu}_\rho v^\rho {{\gamma'(t)}^\nu}_\sigma w^\sigma
                + {{\gamma'(t)}^\mu}_\rho v^\rho {{\gamma(t)}^\nu}_\sigma w^\sigma
            \bigr) \Big|_{t = 0} \\
        &= g_{\mu\nu} \bigl(
                \delta^\mu_\rho v^\rho {T^\nu}_\sigma w^\sigma
                + {T^\mu}_\rho v^\rho \delta^\nu_\sigma w^\sigma
           \bigr) \\
        &= g_{\mu\nu} \bigl(
                v^\mu {T^\nu}_\sigma w^\sigma + {T^\mu}_\rho v^\rho w^\nu
           \bigr) \\
        &= g(v, Tw) + g(Tv, w)
\end{align*}
so $g(v, Tw) + g(Tv, w) = 0$
and therefore $\mathfrak{so}(p, q)$ is the set of real $n \times n$ matrices $T$ satisfying $g(Tv, w) = -g(v, Tw)$.
Thus, elements of $\mathfrak{so}(p, q)$ are traceless and either symmetric or skew-symmetric, satisfying $T_{\mu\nu} = \pm T_{\nu\mu}$ where the sign is negative if $\mu, \nu < q$ (0-indexed), otherwise positive.

The dimension of this space is $\frac{n(n - 1)}{2}$ and,
since the dimension of the tangent space is equal to the dimension of the manifold,
$\dim\bigl(\SO(p, q)\bigr) = \frac{n(n - 1)}{2}$ as well.

As a result, we expect $\mathfrak{so}(3, 1)$ to be a 6-dimensional vector space.
A natural basis will be three spatial rotations and three Lorentz boosts.
The spatial rotations can be constructed from the familiar basis of $\mathfrak{so}(3)$ as
\[
    J_x =
        \begin{pmatrix*}[r]
            0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 0 & 0 & \m 1 \\
            0 & 0 & 1 & 0 \\
        \end{pmatrix*}, \quad
    J_y =
        \begin{pmatrix*}[r]
            0 &    0 & 0 & 0 \\
            0 &    0 & 0 & 1 \\
            0 &    0 & 0 & 0 \\
            0 & \m 1 & 0 & 0 \\
        \end{pmatrix*}, \quad
    J_z =
        \begin{pmatrix*}[r]
            0 & 0 & 0    & 0 \\
            0 & 0 & \m 1 & 0 \\
            0 & 1 & 0    & 0 \\
            0 & 0 & 0    & 0 \\
        \end{pmatrix*}.
\]
For the Lorentz boosts as in solution~\ref{sol:so31lorentztransformations}, let $\gamma_i(\zeta)$ be a path in $\SO(3, 1)$ about $jk$ parameterised by rapidity.
Denoting $\gamma_i'(0) = K_i$,
\begin{align*}
    \gamma_x(\zeta) &=
        \begin{pmatrix}
            \phantom{-}\cosh(\zeta) & \m\sinh(\zeta)           & 0 & 0 \\
            -\sinh(\zeta)           & \phantom{\m}\cosh(\zeta) & 0 & 0 \\
            0                       & 0                        & 1 & 0 \\
            0                       & 0                        & 0 & 1
        \end{pmatrix}, \quad
        K_x = \begin{pmatrix*}[r]
            0  & \m 1 & 0 & 0 \\
            -1 &    0 & 0 & 0 \\
            0  &    0 & 0 & 0 \\
            0  &    0 & 0 & 0
        \end{pmatrix*}, \displaybreak[0] \\
    \gamma_y(\zeta) &=
        \begin{pmatrix}
            \phantom{-}\cosh(\zeta) & 0 & \m\sinh(\zeta)           & 0 \\
            0                       & 1 & 0                        & 0 \\
            -\sinh(\zeta)           & 0 & \phantom{\m}\cosh(\zeta) & 0 \\
            0                       & 0 & 0                        & 1
        \end{pmatrix}, \quad
        K_y = \begin{pmatrix*}[r]
            0  & 0 & \m 1 & 0 \\
            0  & 0 & 0    & 0 \\
            -1 & 0 & 0    & 0 \\
            0  & 0 & 0    & 0
        \end{pmatrix*}, \displaybreak[0] \\
    \gamma_z(\zeta) &=
        \begin{pmatrix}
            \phantom{-}\cosh(\zeta) & 0 & 0 & \m\sinh(\zeta) \\
            0                       & 1 & 0 & 0 \\
            0                       & 0 & 1 & 0 \\
            -\sinh(\zeta)           & 0 & 0 & \phantom{\m}\cosh(\zeta)
        \end{pmatrix}, \quad
        K_z = \begin{pmatrix*}[r]
            0  & 0 & 0 & \m 1 \\
            0  & 0 & 0 & 0 \\
            0  & 0 & 0 & 0 \\
            -1 & 0 & 0 & 0
        \end{pmatrix*}.
\end{align*}
We can combine these into the skew-symmetric matrix of Lorentz generators
\[
    \tightmatrix
    M = \begin{pmatrix*}[r]
            0\phantom{_x} & K_x           & K_y           & K_z \\
            -K_x          & 0\phantom{_x} & J_z \,        & - J_y \, \\
            -K_y          & - J_z \,      & 0\phantom{_x} & J_x \, \\
            -K_z          & J_y \,        & - J_x \,      & 0\phantom{_x}
        \end{pmatrix*}
\]
where in this form we notice that each entry $M_{\alpha\beta}$ can be expressed in terms of the Minkowski metric $\eta$ as
\[
    {(M_{\alpha\beta})}_{\mu\nu}
        = \eta_{\alpha\mu} \eta_{\beta\nu} - \eta_{\beta\mu} \eta_{\alpha\nu}.
\]

\end{sol}

\begin{ex}\label{ex:suntracelessskewadjoint}

Show that the Lie algebra $\mathfrak{u}(n)$ of $\U(n)$ consists of all \emph{skew-adjoint} complex $n \times n$ matrices, that is, matrices $T$ with
\[
    T_{ij} = -\overline{T}_{ji}.
\]
In particular, show that $\mathfrak{u}(1)$ consists of the purely imaginary complex numbers:
\[
    \mathfrak{u}(1) = \setbuilder{ix \given x \in \mathbb{R}}.
\]
Show that the Lie algebra $\mathfrak{su}(n)$ of $\SU(n)$ consists of all traceless skew-adjoint complex $n \times n$ matrices.

\end{ex}

\begin{sol}

Let $\gamma(t)$ be a path in $\U(n)$ with $\gamma(0) = \id$.
Then for any $v, w \in \mathbb{C}^n$,
\[
    \innerp{\gamma(t) v}{\gamma(t) w}
        = {\bar{\gamma}(t)}_{ij}\bar{v}^j {\gamma(t)}_{ik} w^k = \bar{v}^i w^i.
\]
Let $\gamma'(0) = T$.
Differentiating and setting $t = 0$,
\[
    \bar{v}^i T_{ij} w^j + {\overline{T}}_{ij}\bar{v}^j w^i = 0
\]
so $T_{ij} = -\overline{T}_{ji}$.

For $z \in \mathfrak{u}(1)$, our condition reduces to $z = -\bar{z}$, so $\mathfrak{u}(1) = \setbuilder{ix \given x \in \mathbb{R}}$.

For $\mathfrak{su}(n)$, let $\gamma(t)$ be a path in $\SU(n)$ with $\gamma(0) = \id$ and let $\gamma'(0) = T$.
We require $\det\bigl(\gamma(t)\bigr) = 1$ which, by exercise~\ref{ex:detexptrace}, is equivalent to $\tr(T) = 0$.
Therefore $\mathfrak{su}(n)$ consists of all traceless skew-adjoint complex $n \times n$ matrices.

\end{sol}

\begin{ex}

Show this for $G$ a matrix Lie group by differentiating
\[
    \gamma(t) {\gamma(t)}^{-1} = \id
\]
with respect to $t$, using the product rule.

\end{ex}

\begin{sol}

Differentiating,
\begin{align*}
    \frac{d}{dt} \left(\gamma(t) {\gamma(t)}^{-1} \right) \Big|_{t = 0} \!
        &= \gamma(0) \frac{d}{dt} {\gamma(t)}^{-1} \Big|_{t = 0}
            + \frac{d}{dt} \gamma(t) \Big|_{t = 0} \cdot {\gamma(0)}^{-1} \\
        &= \frac{d}{dt} {\gamma(t)}^{-1} \Big|_{t = 0}
            + \frac{d}{dt} \gamma(t) \Big|_{t = 0}
\end{align*}
so
\[
    \frac{d}{dt} \gamma(t) \Big|_{t = 0} \!
        = -\frac{d}{dt} {\gamma(t)}^{-1} \Big|_{t = 0}.
\]

\end{sol}

\begin{ex}

If $G$ is a matrix Lie group and $\gamma$, $\eta$ are paths in $G$ with $\gamma(0) = \eta(0) = 1$, show that
\[
    \frac{d}{dt} \gamma(t) \eta(t) \Big|_{t = 0}
        = \frac{d}{dt} \gamma(t) \Big|_{t = 0}
          +\frac{d}{dt} \eta(t) \Big|_{t = 0}.
\]
Conclude that the differential of $\cdot : G \times G \to G$ and $(1, 1) \in G \times G$ is the addition map from $\mathfrak{g} \oplus \mathfrak{g}$ to $\mathfrak{g}$.

\end{ex}

\begin{sol}

By differentiating,
\begin{align*}
    \frac{d}{dt} \gamma(t) \eta(t) \Big|_{t = 0}
        &= \gamma'(0) \eta(0) + \gamma(0) \eta'(0) \\
        &= \gamma'(0) + \eta'(0)
\end{align*}
as required.
This implies that the derivative transforms the group operation on $G \times G$ into addition on $\mathfrak{g} \oplus \mathfrak{g}$.

\end{sol}

\begin{ex}

Check these. Note that in~\ref{sol:liebracketmatrix:linearity}, the term `scalars' means real numbers if $\mathfrak{g}$ is a real vector space, but complex numbers if $\mathfrak{g}$ is a complex vector space.

\end{ex}

\begin{sol}\label{sol:liebracketmatrix}

In the case of matrix Lie groups, where the Lie algebra $\mathfrak{g}$ consists of matrices and the Lie bracket is the commutator, it is easy to check the following identities:
\begin{enumerate}
    \item $[v, w] = -[w, v]$ for all $v, w \in \mathfrak{g}$,\label{sol:liebracketmatrix:anticommutativity}
    \item $[u, \alpha v + \beta w] = \alpha [u, v] + \beta [u, w]$
          for all $u, v, w \in \mathfrak{g}$ and scalars $\alpha$, $\beta$,\label{sol:liebracketmatrix:linearity}
    \item the \emph{Jacobi identity}: $\big[u, [v, w]\big] + \big[v, [w, u]\big] + \big[w, [u, v]\big] = 0$.\label{sol:liebracketmatrix:jacobiidentity}
\end{enumerate}

See solution~\ref{sol:liebracketvectorfields}, which is identical.

\end{sol}

\begin{ex}

Show that the Lie algebras $\mathfrak{su}(2)$ and $\mathfrak{so}(3)$ are isomorphic as follows.
First show that $\mathfrak{su}(2)$ has as a basis the quaternions $I$, $J$, $K$ or, in other words, the matrices $-i \sigma_1$, $-i \sigma_2$, $-i \sigma_3$.
Then show that the linear map $f: \mathfrak{su}(2) \to \mathfrak{so}(3)$ given by
\[
    - \frac{i}{2} \sigma_j \mapsto J_j
\]
is a Lie algebra homomorphism.

\end{ex}

\begin{sol}

From exercise~\ref{ex:su2isunit3sphere}, $\SU(2)$ is isomorphic to $S^3$ and therefore its tangent space is 3-dimensional.
From exercise~\ref{ex:suntracelessskewadjoint}, $\mathfrak{su}(2)$ consists of all traceless skew-adjoint complex $2 \times 2$ matrices.
From exercise~\ref{ex:paulimatrices}, the Pauli matrices are linearly independent and $\tr(c_i \sigma_i) = 0$ for any $c_i \in \mathbb{C}$.

Since the quaternions $I$, $J$, $K$ in matrix form are three linearly independent traceless skew-adjoint complex $2 \times 2$ matrices, they form a basis of $\mathfrak{su}(2)$.

A Lie algebra isomorphism is a bijective linear map $f : \mathfrak{g} \to \mathfrak{h}$ preserving the Lie bracket, i.e.\ mapping $[v, w] \mapsto [f(v), f(w)]$.
Recall from exercise~\ref{ex:sigmaisigmajisisigmak} that $[\sigma_i, \sigma_j] = 2i \epsilon_{ijk} \sigma_k$ and from exercise~\ref{ex:so3basis} that $[J_i, J_j] = \epsilon_{ijk} J_k$
and consider the obviously bijective map $f: - \frac{i}{2} \sigma_j \mapsto J_j$
from $\mathfrak{su}(2)$ to $\mathfrak{so}(3)$.
\begin{align*}
    f \bigl(\bigl[-\tfrac{i}{2} \sigma_i, -\tfrac{i}{2} \sigma_j \bigr] \bigr)
        &= f \bigl( -\tfrac{1}{4} [\sigma_i, \sigma_j] \bigr) \\
        &= f \bigl( -\tfrac{i}{2} \epsilon_{ijk} \sigma_k \bigr) \\
        &= \epsilon_{ijk}J_k \\
        &= [J_i, J_j] \\
        &= \bigl[ f \bigl(-\tfrac{i}{2} \sigma_i \bigr),
                  f \bigl(-\tfrac{i}{2} \sigma_j\bigr) \bigr],
\end{align*}
so $\mathfrak{su}(2)$ and $\mathfrak{so}(3)$ are isomorphic.

\end{sol}

\begin{ex}\label{ex:pushforwardliebracket}

Let $M$ be any manifold and $v, w \in \Vect(M)$. Let $\phi$ be a diffeomorphism of $M$. Show that
\[
    \phi_* [v, w] = [\phi_* v, \phi_* w].
\]
Conclude that if $v$, $w$ are two left-invariant vector fields on a Lie group, so is $[v, w]$.

\end{ex}

\begin{sol}

Recall from solution~\ref{sol:pushforwardvectorfield} the~\ref{eq:pushforwardvector} at a point,
\[
    \phi_* (v_p) (f) = (\phi_* v)(f) \bigl(\phi(p)\bigr).
\]

Applying $\phi_* [v, w]$ to some $f \in C^\infty(M)$ at $p \in M$,
\begin{align*}
    \phi_* {[v, w]}_p f &= {[v, w]}_p (\phi^* f) \\
        &= v \bigl(w (\phi^* f) \bigr) \bigl(\phi(p)\bigr)
           - w \bigl(v (\phi^* f) \bigr) \bigl(\phi(p)\bigr) \\
        &= v \bigl( (\phi_* w) (f) (\phi(p)) \bigr)
           - w \bigl( (\phi_* v) (f) (\phi(p)) \bigr) \\
        &= v \bigl( w(\phi^* f) (\phi(p)) \bigr)
           - w \bigl( v(\phi^* f) (\phi(p)) \bigr) \\
        &= v \bigl( w(\phi^* f) \circ \phi \bigr) (p)
           - w \bigl( v(\phi^* f) \circ \phi \bigr) (p) \\
        &= \phi_* v \bigl( w(\phi^* f) \bigr) (p)
           - \phi_* w \bigl( v(\phi^* f) \bigr) (p) \\
        &= \phi_* v \bigl( (\phi_* w) (f) \bigr) (p)
           - \phi_* w \bigl( (\phi_* v) (f) \bigr) (p) \\
        &= {[\phi_* v, \phi_* w]}_p f.
\end{align*}
If $v$, $w$ are left-invariant then $\phi_* [v, w] = [\phi_* v, \phi_* w] = [v, w]$, so $[v, w]$ is also left-invariant.

\end{sol}

\begin{ex}\label{ex:liealgebraflow}

Let $G$ be a matrix Lie group.
Let $v$ be a left-invariant vector field on $G$ and $v_1 \in \mathfrak{g}$ its value at the identity.
Let $\phi_t: G \to G$ be given by
\[
    \phi_t(g) = g \exp(tv_1).
\]
Show that $\phi_t$ is the flow generated by $v$, that is, that
\[
    \frac{d}{dt} \phi_t(g) \Big|_{t = 0} = v_g
\]
for all $g \in G$.

\end{ex}

\begin{sol}

Recall from exercise~\ref{ex:tangentvector} that for a manifold $M$, $f \in C^\infty(M)$ and a path $\gamma: \mathbb{R} \to M$,
\[
    \gamma'(t): f \mapsto \frac{d}{dt} f\bigl(\gamma(t)\bigr).
\]
Denoting $\gamma(t) = \exp(tv_{\id})$, we have
\[
    \gamma(0) = \id, \qquad
    \gamma'(0) = v_{\id}.
\]
Differentiating,
\begin{align*}
    \SwapAboveDisplaySkip
    \frac{d}{dt} \phi_t(g) \Big|_{t = 0} &= \frac{d}{dt} L_g\bigl(\exp(tv_{\id}) \bigr) \Big|_{t = 0} \\
        &= \gamma'(0) (L_g) \\
        &= v_{\id}(L_g) \\
        &= {(L_g)}_* v_{\id} \\
        &= v_g
\end{align*}
which, from \S\ref{sec:flowsandtheliebracket}, implies that $\phi_t$ is the flow generated by $v$.

\end{sol}

\begin{ex}

Let $G$ be a matrix Lie group and $\mathfrak{g}$ its Lie algebra.
Let $u_1$,~$v_1$ and $w_1 = [u_1, v_1]$ be elements of $\mathfrak{g}$ and let $u$, $v$ and $w$ be the corresponding left-invariant vector fields on $G$.
Show that $[u, v] = w$, so that $\mathfrak{g}$ and the left-invariant vector fields on $G$ are isomorphic as Lie algebras.
(Hint: use \hyperref[ex:liealgebraflow]{the previous exercise} and, if necessary, review the material on flows in \hyperref[ch:vectorfields]{Chapter~3} of Part~\ref{part:electromagnetism}.)

\end{ex}

\begin{sol}

Let
\[
    \gamma_{u_1}(t) = \exp(tu_1), \qquad
    \gamma_{v_1}(s) = \exp(sv_1),
\]
be paths in $G$. Let $\phi_t$, $\psi_s$ be flows generated by $u$ and $v$, respectively, i.e.
\[
    \phi_t(g) = g \gamma_{u_1}(t), \qquad
    \psi_s(g) = g \gamma_{v_1}(s).
\]
We have that
\[
    [u_1, v_1] = \frac{\partial^2}{\partial s\, \partial t}
        \bigl(
            \gamma_{u_1}(t) \gamma_{v_1}(s) - \gamma_{v_1}(s) \gamma_{u_1}(t)
        \bigr) \Big|_{s = t = 0}.
\]
Recall from exercise~\ref{ex:liebracketflows} the Lie bracket of vector fields in terms of their flows.
\begin{align*}
    {[u, v]}_g &= \frac{\partial^2}{\partial s\, \partial t}
            \bigl(
                \psi_s (\phi_t(g)) - \phi_t (\psi_s(g))
            \bigr) \Big|_{s = t = 0} \\
        &= \frac{\partial^2}{\partial s\, \partial t}
            \bigl(
                \psi_s (g \gamma_{u_1}(t)) - \phi_t (g\gamma_{v_1}(s))
            \bigr) \Big|_{s = t = 0} \\
        &= \frac{\partial^2}{\partial s\, \partial t}
            \bigl(
                g \gamma_{u_1}(t) \gamma_{v_1}(s) - g \gamma_{v_1}(s) \gamma_{u_1}(t)
            \bigr) \Big|_{s = t = 0} \\
\end{align*}
giving $[u_1, v_1] = {[u, v]}_1$.

Pushing forward $w_1$ by $L_g$,
\begin{align*}
    {(L_g)}_* w_1
        &= {(L_g)}_* [u_1, v_1] \\
        &= {(L_g)}_* {[u, v]}_1 \\
        &= {[u, v]}_g
\end{align*}
and since ${(L_g)}_* w_1 = w_g$, we get $w = [u, v]$.

\end{sol}

\begin{ex}

Show that this is a Lie algebra homomorphism.

\end{ex}

\begin{sol}

The claim is that every homomorphism $\rho: G \to H$ between Lie groups determines a corresponding homomorphism $d\rho: \mathfrak{g} \to \mathfrak{h}$ between their Lie algebras given by
\[
    d\rho = {(\rho)}_* : T_1 G \to T_1 H.
\]
By exercise~\ref{ex:pushforwardliebracket}, for $v, w \in \mathfrak{g}$,
\begin{align*}
    d\rho \bigl([v, w]\bigr) &= \rho_*\bigl([v, w]\bigr) \\
        &= [\rho_* v, \rho_* w] \\
        &= [d\rho (v), d\rho(w)]
\end{align*}
so $d\rho$ is a Lie algebra homomorphism.

\end{sol}

\begin{ex}

Do these calculations.

\end{ex}

\begin{sol}

Consider the two-to-one homomorphism $\rho: \SU(2) \to \SO(3)$ from \S\ref{sec:liegroups}, which determines the homomorphism $d\rho: \mathfrak{su}(2) \to \mathfrak{so}(3)$.

Our conventional basis for $\mathfrak{su}(2)$ is $\{-\frac{i}{2} \sigma_j\}$, so the path in $\SU(2)$ corresponding to $j = 3$ is
\begin{align*}
    g_t &= \exp\bigl(-\tfrac{i}{2} t \sigma_3\bigr) \\
        &= \tightmatrix\begin{pmatrix}
                e^{-\frac{it}{2}} & 0 \;\, \\
                0                 & e^{\frac{it}{2}}
            \end{pmatrix}.
\end{align*}
$\rho(g_t)$ is determined by its action on each
\[
    \rho(g_t) \sigma_j = g_t \sigma_j g_t^{-1}.
\]
For $\sigma_1$,
\begin{align*}
    \SwapAboveDisplaySkip
    \rho(g_t) \sigma_1 &= g_t \sigma_1 g_t^{-1} \\
        &= \begin{pmatrix}
                e^{-\frac{it}{2}} & 0 \;\, \\
                0                 & e^{\frac{it}{2}}
            \end{pmatrix}
            \begin{pmatrix}
                0 & 1 \\
                1 & 0
            \end{pmatrix}
            \begin{pmatrix}
                e^{\frac{it}{2}} & 0 \; \\
                0                & e^{- \frac{it}{2}}
            \end{pmatrix} \\
        &= \begin{pmatrix}
                0      & e^{-it} \\
                e^{it} & 0 \;
            \end{pmatrix} \\
        &= \cos(t) \sigma_1 + \sin(t) \sigma_2,
\end{align*}
for $\sigma_2$,
\begin{align*}
    \SwapAboveDisplaySkip
    \rho(g_t) \sigma_2 &= g_t \sigma_2 g_t^{-1} \\
        &= \begin{pmatrix}
                e^{-\frac{it}{2}} & 0 \;\, \\
                0                 & e^{\frac{it}{2}}
            \end{pmatrix}
            \begin{pmatrix*}[r]
                0 & \m i \\
                i &  0
            \end{pmatrix*}
            \begin{pmatrix}
                e^{\frac{it}{2}} & 0 \; \\
                0                & e^{- \frac{it}{2}}
            \end{pmatrix} \\
        &= \begin{pmatrix}
                0                 & \m ie^{-\frac{it}{2}} \\
                ie^{\frac{it}{2}} & 0
            \end{pmatrix}
            \begin{pmatrix}
                e^{\frac{it}{2}} & 0 \; \\
                0                & e^{-\frac{it}{2}}
            \end{pmatrix} \\
        &= \begin{pmatrix}
                0       & \m ie^{-it} \\
                ie^{it} & 0
            \end{pmatrix} \\
        &= -\sin(t) \sigma_1 + \cos(t) \sigma_2
\end{align*}
and for $\sigma_3$,
\begin{align*}
    \SwapAboveDisplaySkip
    \rho(g_t) \sigma_3 &= g_t \sigma_3 g_t^{-1} \\
        &= \begin{pmatrix}
                e^{-\frac{it}{2}} & 0 \,\,\, \\
                0                 & e^{\frac{it}{2}}
            \end{pmatrix}
            \begin{pmatrix*}[r]
                1 &  0 \\
                0 & \m 1
            \end{pmatrix*}
            \begin{pmatrix}
                e^{\frac{it}{2}} & \!\!0 \\
                0                & e^{- \frac{it}{2}}
            \end{pmatrix} \\
        &= \begin{pmatrix}
                e^{-\frac{it}{2}} & 0 \,\,\, \\
                0                 & \m e^{\frac{it}{2}}
            \end{pmatrix}
            \begin{pmatrix}
                e^{\frac{it}{2}} & \!\!0 \\
                0                & e^{- \frac{it}{2}}
            \end{pmatrix} \\
        &= \sigma_3.
\end{align*}
Note that if we instead used $\{\frac{i}{2} \sigma_j\}$ as our basis for $\mathfrak{su}(2)$, our result would have the sign flipped on each sine function, corresponding to a positive rotation about the $z$-axis by $t$.

\end{sol}

\begin{ex}

Show that $\rho \bigl( \exp(-\frac{i}{2} \sigma_1) \bigr)$ is a rotation of angle $t$ about the $x$-axis and $\rho \bigl( \exp(-\frac{i}{2} \sigma_2) \bigr)$ is a rotation of angle $t$ about the $y$-axis.

\end{ex}

\begin{sol}

The path in $\SU(2)$ corresponding to $j = 1$ is
\begin{align*}
    g_t &= \exp\bigl(-\tfrac{i}{2} t \sigma_1\bigr) \\
        &= \sum_{n = 0}^\infty \frac{{\bigl(-\frac{i}{2} t \sigma_1\bigr)}^n}{n!} \\
        &= \sigma_0 \cdot \left(
                \id + \frac{{\bigl(-\tfrac{i}{2} t\bigr)}^{2\!\!}}{2!}
                    + \frac{{\bigl(-\tfrac{i}{2} t\bigr)}^{4\!\!}}{4!}
                    + \cdots
            \right) \\*
        &\mathrel{\phantom{=}}{} + \sigma_1 \cdot \left(
                \frac{- \tfrac{i}{2} t}{1}
                + \frac{{\bigl(-\tfrac{i}{2} t\bigr)}^{3\!\!}}{3!}
                + \frac{{\bigl(-\tfrac{i}{2} t\bigr)}^{5\!\!}}{5!}
                + \cdots
            \right) \displaybreak[0] \\
        &= \sigma_0 \cdot \left(
                \id - \frac{{\bigl(\tfrac{t}{2}\bigr)}^{2\!\!}}{2!}
                    + \frac{{\bigl(\tfrac{t}{2}\bigr)}^{4\!\!}}{4!}
                    + \cdots
            \right) \\*
        &\mathrel{\phantom{=}}{} + i \sigma_1 \cdot \left(
                - \tfrac{t}{2}
                + \frac{{\bigl(\tfrac{t}{2}\bigr)}^{3\!\!}}{3!}
                - \frac{{\bigl(-\tfrac{t}{2}\bigr)}^{5\!\!}}{5!}
                + \cdots
            \right) \displaybreak[0] \\
        &= \cos(\tfrac{t}{2}) \sigma_0 - i \sin(\tfrac{t}{2}) \sigma_1 \displaybreak[0] \\
        &= \begin{pmatrix}
                \cos(\tfrac{t}{2})    & \m i \sin(\tfrac{t}{2}) \\
                -i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
            \end{pmatrix}.
\end{align*}
$\rho(g_t)$ is determined by its action on each $\rho(g_t) \sigma_j = g_t \sigma_j g_t^{-1}$.

For $\sigma_1$,
\begin{align*}
    \rho(g_t) \sigma_1 &= g_t \sigma_1 g_t^{-1} \\
        &= \begin{pmatrix}
                \cos(\tfrac{t}{2})    & \m i \sin(\tfrac{t}{2}) \\
                -i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
           \end{pmatrix}
           \begin{pmatrix}
                0 & 1 \\
                1 & 0
            \end{pmatrix}
            \begin{pmatrix}
                \cos(\tfrac{t}{2})   & i \sin(\tfrac{t}{2}) \\
                i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
           \end{pmatrix} \\
        &= \begin{pmatrix}
                -i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2}) \\
                \cos(\tfrac{t}{2})   & \m i \sin(\tfrac{t}{2})
           \end{pmatrix}
           \begin{pmatrix}
                \cos(\tfrac{t}{2})   & i \sin(\tfrac{t}{2}) \\
                i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
           \end{pmatrix} \\
        &= \sigma_1,
\end{align*}
for $\sigma_2$,
\begin{align*}
    \rho(g_t) \sigma_2 &= g_t \sigma_2 g_t^{-1} \\
        &= \begin{pmatrix}
                \cos(\tfrac{t}{2})    & \m i \sin(\tfrac{t}{2}) \\
                -i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
           \end{pmatrix}
            \begin{pmatrix*}[r]
                0 & \m i \\
                i &    0
            \end{pmatrix*}
            \begin{pmatrix}
                \cos(\tfrac{t}{2})   & i \sin(\tfrac{t}{2}) \\
                i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
           \end{pmatrix} \\
        &= \begin{pmatrix}
                \sin(\tfrac{t}{2})   & \m i \cos(\tfrac{t}{2}) \\
                i \cos(\tfrac{t}{2}) & \m \sin(\tfrac{t}{2})
            \end{pmatrix}
            \begin{pmatrix}
                \cos(\tfrac{t}{2})   & i \sin(\tfrac{t}{2}) \\
                i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
           \end{pmatrix} \\
        &= \begin{pmatrix}
                2 \cos(\tfrac{t}{2}) \sin(\tfrac{t}{2})
                    & i \bigl( {\sin(\tfrac{t}{2})}^2 - {\cos(\tfrac{t}{2})}^2 \bigr) \\
                i \bigl( {\cos(\tfrac{t}{2})}^2 - {\sin(\tfrac{t}{2})}^2 \bigr)
                    & -2 \cos(\tfrac{t}{2}) \sin(\tfrac{t}{2})
            \end{pmatrix} \\
        &= \begin{pmatrix}
                \sin(t)   & \m i \cos(t) \\
                i \cos(t) & -\sin(t)
            \end{pmatrix} \\
        &= \cos(t) \sigma_2 + \sin(t) \sigma_3
\end{align*}
and for $\sigma_3$,
\begin{align*}
    \rho(g_t) \sigma_3 &= g_t \sigma_3 g_t^{-1} \\
        &= \begin{pmatrix}
                \cos(\tfrac{t}{2})    & \m i \sin(\tfrac{t}{2}) \\
                -i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
           \end{pmatrix}
            \begin{pmatrix*}[r]
                1 &  0 \\
                0 & \m 1
            \end{pmatrix*}
            \begin{pmatrix}
                \cos(\tfrac{t}{2})   & i \sin(\tfrac{t}{2}) \\
                i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
           \end{pmatrix} \\
        &= \begin{pmatrix}
                \cos(\tfrac{t}{2})     & i \sin(\tfrac{t}{2}) \\
                - i \sin(\tfrac{t}{2}) & -\cos(\tfrac{t}{2})
            \end{pmatrix}
            \begin{pmatrix}
                \cos(\tfrac{t}{2})   & i \sin(\tfrac{t}{2}) \\
                i \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
           \end{pmatrix} \\
        &= \begin{pmatrix}
                {\cos(\tfrac{t}{2})}^2 - {\sin(\tfrac{t}{2})}^2
                    & 2 i \cos(\tfrac{t}{2}) \sin(\tfrac{t}{2}) \\
                -2 i \cos(\tfrac{t}{2}) \sin(\tfrac{t}{2})
                    & {\sin(\tfrac{t}{2})}^2 - {\cos(\tfrac{t}{2})}^2
            \end{pmatrix} \\
        &= \begin{pmatrix}
                \cos(t)     & i \sin(t) \\
                - i \sin(t) & -\cos(t)
            \end{pmatrix} \\
        &= - \sin(t) \sigma_2 + \cos(t) \sigma_3.
\end{align*}
Flipping the sign of $t$ to be consistent with our convention of rotating in a positive direction, in the space spanned by $\{\sigma_j\}$ we get
\[
    \rho(g_t) = \begin{pmatrix}
            1 & 0       & 0 \\
            0 & \cos(t) & \m \sin(t) \\
            0 & \sin(t) & \phantom{\m} \cos(t) \\
        \end{pmatrix}
\]
which describes a (positive) rotation about the $x$-axis by $t$.

The path in $\SU(2)$ corresponding to $j = 2$ is
\begin{align*}
    g_t &= \exp\bigl(-\tfrac{i}{2} t \sigma_2\bigr) \\
        &= \cos(\tfrac{t}{2}) \sigma_0 - i\sin(\tfrac{t}{2}) \sigma_2 \\
        &= \begin{pmatrix*}[r]
                \cos(\tfrac{t}{2}) & \m \sin(\tfrac{t}{2}) \\
                \sin(\tfrac{t}{2}) & \cos(\tfrac{t}{2})
            \end{pmatrix*}.
\end{align*}
By similar calculations and after flipping $t$, we get
\[
    \rho(g_t) = \begin{pmatrix}
            \phantom{-} \cos(t) & 0 & \sin(t) \\
            0 \!\!\!            & 1 & 0 \,\, \\
            - \sin(t)           & 0 & \cos(t) \\
        \end{pmatrix}
\]
which describes a (positive) rotation about the $y$-axis by $t$.

\end{sol}

\begin{ex}

Show that in the spin-$\tfrac{1}{2}$ representation of $\SU(2)$, the expected value of the angular momentum about the $z$-axis in the so-called \emph{spin-up state},
\[
    {\uparrow} = \begin{pmatrix}
            \,1\, \\ 0
        \end{pmatrix},
\]
is $\tfrac{1}{2}$, while in the \emph{spin-down state},
\[
    {\downarrow} = \begin{pmatrix}
            \,0\, \\ 1
        \end{pmatrix},
\]
it is $-\tfrac{1}{2}$.
Similarly, compute the expected value of the angular momentum about the $x$- and $y$-axes\footnote{We consider the $x$- and $y$-axes since we are already asked to compute the expected value about the $z$-axis.} in these states.

\end{ex}

\begin{sol}

The expected value of the $z$-component of the system's angular momentum about that axis is given by
\[
    \innerp{\psi}{dU\bigl(\tfrac{\sigma_z}{2}\bigr) \psi}
\]
where $dU$ is a representation of $\mathfrak{su}(2)$.
Recall from exercise~\ref{ex:spinhalffundamentalrepresentation} that the spin-$\frac{1}{2}$ representation of $\SU(2)$ is equivalent to the fundamental representation.

For the spin-up state,
\begin{align*}
    \innerp{\uparrow}{dU\bigl(\tfrac{\sigma_z}{2}\bigr) {\uparrow}}
        &= \innerp*{\begin{pmatrix}
                \,1\, \\ 0
            \end{pmatrix}}
            {\begin{pmatrix}
                \tfrac{1}{2} & \phantom{\m}0 \\
                0            & \m \tfrac{1}{2}
            \end{pmatrix}
            \begin{pmatrix}
                \,1\, \\ 0
            \end{pmatrix}} \\
    &= \innerp*{\begin{pmatrix}
                \,1\, \\ 0
            \end{pmatrix}}
            {\begin{pmatrix}
                \,\tfrac{1}{2}\, \\ 0
            \end{pmatrix}} \\
    &= \frac{1}{2}
\end{align*}
and for the spin-down state,
\begin{align*}
    \innerp{\downarrow}{dU\bigl(\tfrac{\sigma_z}{2}\bigr) {\downarrow}}
        &= \innerp*{\begin{pmatrix}
                \,0\, \\ 1
            \end{pmatrix}}
            {\begin{pmatrix}
                \tfrac{1}{2} & \phantom{\m}0 \\
                0            & \m \tfrac{1}{2}
            \end{pmatrix}
            \begin{pmatrix}
                \,0\, \\ 1
            \end{pmatrix}} \\
    &= \innerp*{\begin{pmatrix}
                \,0\, \\ 1
            \end{pmatrix}}
            {\begin{pmatrix}
                \,\, 0 \\ -\tfrac{1}{2}
            \end{pmatrix}} \\
    &= -\frac{1}{2}.
\end{align*}

For the $x$-axis,
\begin{align*}
    \innerp{\uparrow}{dU\bigl(\tfrac{\sigma_x}{2}\bigr) {\uparrow}}
        &= \innerp*{\begin{pmatrix}
                \,1\, \\ 0
            \end{pmatrix}}
            {\begin{pmatrix}
                0            & \tfrac{1}{2} \\
                \tfrac{1}{2} & 0
            \end{pmatrix}
            \begin{pmatrix}
                \,1\, \\ 0
            \end{pmatrix}} \\
        &= \innerp*{\begin{pmatrix}
                \,1\, \\ 0
            \end{pmatrix}}
            {\begin{pmatrix}
                \,0\, \\ \tfrac{1}{2}
            \end{pmatrix}} \\
        &= 0, \\
\innerp{\downarrow}{dU\bigl(\tfrac{\sigma_x}{2}\bigr) {\downarrow}}
        &= \innerp*{\begin{pmatrix}
                \,0\, \\ 1
            \end{pmatrix}}
            {\begin{pmatrix}
                0            & \tfrac{1}{2} \\
                \tfrac{1}{2} & 0
            \end{pmatrix}
            \begin{pmatrix}
                \,0\, \\ 1
            \end{pmatrix}} \\
        &= \innerp*{\begin{pmatrix}
                \,0\, \\ 1
            \end{pmatrix}}
            {\begin{pmatrix}
                \,\tfrac{1}{2}\, \\ 0
            \end{pmatrix}} \\
        &= 0
\end{align*}
and for the $y$-axis,
\begin{align*}
    \innerp{\uparrow}{dU\bigl(\tfrac{\sigma_y}{2}\bigr) {\uparrow}}
        &= \innerp*{\begin{pmatrix}
                \,1\, \\ 0
            \end{pmatrix}}
            {\begin{pmatrix*}[r]
                0           & \m\frac{i}{2} \\
                \frac{i}{2} &  0
            \end{pmatrix*}
            \begin{pmatrix}
                \,1\, \\ 0
            \end{pmatrix}} \\
        &= \innerp*{\begin{pmatrix}
                \,1\, \\ 0
            \end{pmatrix}}
            {\begin{pmatrix}
                \,0\, \\ \tfrac{i}{2}
            \end{pmatrix}} \\
        &= 0, \\
    \innerp{\downarrow}{dU\bigl(\tfrac{\sigma_y}{2}\bigr) {\downarrow}}
        &= \innerp*{\begin{pmatrix}
                \,0\, \\ 1
            \end{pmatrix}}
            {\begin{pmatrix*}[r]
                0           & \m\frac{i}{2} \\
                \frac{i}{2} & 0
            \end{pmatrix*}
            \begin{pmatrix}
                \,0\, \\ 1
            \end{pmatrix}} \\
        &= \innerp*{\begin{pmatrix}
                \,0\, \\ 1
            \end{pmatrix}}
            {\begin{pmatrix}
                -\tfrac{i}{2} \\ \,\, 0
            \end{pmatrix}} \\
        &= 0.
\end{align*}

\end{sol}

\begin{ex}

Show that $\mathfrak{sl}(n, \mathbb{R})$, $\mathfrak{sl}(n, \mathbb{C})$, $\mathfrak{so}(p, q)$ and $\mathfrak{su}(n)$ are semisimple, except for certain low-dimensional cases, which you should determine.

\end{ex}

\begin{sol}

We say that $\mathfrak{g}$ is a \emph{semisimple} Lie algebra if every element of $\mathfrak{g}$ is a linear combination of the Lie bracket of other elements.

%Define an ideal of $\mathfrak{g}$ as a Lie subalgebra $\mathfrak{i} \subseteq \mathfrak{g}$ satisfying $[\mathfrak{g}, \mathfrak{i}] \subseteq \mathfrak{i}$. Therefore we can equivalently say that $\mathfrak{g}$ is semisimple if it has only nontrivial ideals.

Consider first $\mathfrak{sl}(n, \mathbb{C})$, which we know from exercise~\ref{ex:detexptrace} has a representation as all $n \times n$ traceless complex matrices.

\begin{itemize}

    \item Let $z \in \mathfrak{sl}(1, \mathbb{C})$.
    Then $\tr(z) = 0$, so $z = 0$ and $\mathfrak{sl}(1, \mathbb{C}) = \{0\}$ is semisimple.

    \item When $n = 2$, we can use the basis
    \[
        e = \begin{pmatrix}
                0 & 1 \\
                0 & 0
            \end{pmatrix}, \qquad
        f = \begin{pmatrix}
                0 & 0 \\
                1 & 0
            \end{pmatrix}, \qquad
        h = \begin{pmatrix*}[r]
                1 & 0 \\
                0 & \m 1
            \end{pmatrix*}
    \]
    of $\mathfrak{sl}(2, \mathbb{C})$ satisfying
    \[
        [h, e] = 2e, \quad
        [h, f] = -2f, \quad
        [e, f] = h,
    \]
    implying every element of $\mathfrak{sl}(2, \mathbb{C})$ is a linear combination of the Lie bracket of other elements and therefore $\mathfrak{sl}(2, \mathbb{C})$ is semisimple.

    \item Let $E_{ij}$ be the matrix with $1$ at the $i, j$ position and zero elsewhere.
    Notice that $E_{ik} E_{lj} = \delta_{kl} E_{ij}$, so
    \[
        [E_{ik}, E_{lj}] = \delta_{kl} E_{ij} - \delta_{ij} E_{lk},
    \]
    implying every element of $\mathfrak{sl}(n, \mathbb{C})$ is a linear combination of the Lie bracket of other elements and therefore $\mathfrak{sl}(n, \mathbb{C})$ is semisimple.

\end{itemize}

The same argument holds when restricting the field to $\mathbb{R}$, so $\mathfrak{sl}(n, \mathbb{R})$ is also semisimple.

From exercise~\ref{ex:lorentzliealgebra}, a representation of $\mathfrak{so}(p, q)$ consists of all real traceless $n \times n$ matrices $T$ satisfying $T_{\mu\nu} = \pm T_{\nu\mu}$, where the sign is negative if $\mu, \nu < q$ (0-indexed), otherwise positive.

Recall the matrix of Lorentz generators $M$ from solution~\ref{sol:lorentzliealgebra} and generalise it to use a metric $g$ on $\mathbb{R}^n$ of signature $(p, q)$, so
\[
    {(M_{\alpha\beta})}_{\mu\nu}
        = g_{\alpha\mu} g_{\beta\nu} - g_{\beta\mu} g_{\alpha\nu}
\]
or, contracting, ${{(M_{\alpha\beta})}^\mu\!}_\nu = \delta^\mu_\alpha g_{\beta\nu} - \delta^\mu_\beta g_{\alpha\nu}$.
Notice that
\begin{align*}
    {{(M_{\alpha\beta})}^\mu\!}_\rho {{(M_{\gamma\delta})}^\rho\!}_\nu
        &= (\delta^\mu_\alpha g_{\beta\rho} - \delta^\mu_\beta g_{\alpha\rho})
           (\delta^\rho_\gamma g_{\delta\nu} - \delta^\rho_\delta g_{\gamma\nu}) \\
        &= \delta^\mu_\alpha g_{\beta\rho} \delta^\rho_\gamma g_{\delta\nu}
           - \delta^\mu_\alpha g_{\beta\rho} \delta^\rho_\delta g_{\gamma\nu} \\
        &\mathrel{\phantom{=}}{}
           - \delta^\mu_\beta g_{\alpha\rho} \delta^\rho_\gamma g_{\delta\nu}
           + \delta^\mu_\beta g_{\alpha\rho} \delta^\rho_\delta g_{\gamma\nu} \\
        &= g_{\beta\gamma} \delta^\mu_\alpha g_{\delta\nu}
           - g_{\beta\delta} \delta^\mu_\alpha g_{\gamma\nu} \\
        &\mathrel{\phantom{=}}{}
           - g_{\alpha\gamma} \delta^\mu_\beta g_{\delta\nu}
           + g_{\alpha\delta} \delta^\mu_\beta g_{\gamma\nu},
\end{align*}
so
\begin{align*}
    {{[M_{\alpha\beta}, M_{\gamma\delta}]}^\mu\!}_\nu
        &= {{(M_{\alpha\beta})}^\mu\!}_\rho {{(M_{\gamma\delta})}^\rho\!}_\nu
           - {{(M_{\gamma\delta})}^\mu\!}_\rho {{(M_{\alpha\beta})}^\rho\!}_\nu \\
        &= g_{\beta\gamma} \delta^\mu_\alpha g_{\delta\nu}
           - g_{\beta\delta} \delta^\mu_\alpha g_{\gamma\nu}
           - g_{\alpha\gamma} \delta^\mu_\beta g_{\delta\nu}
           + g_{\alpha\delta} \delta^\mu_\beta g_{\gamma\nu} \\
        &\mathrel{\phantom{=}}{}
           - g_{\delta\alpha} \delta^\mu_\gamma g_{\beta\nu}
           + g_{\delta\beta} \delta^\mu_\gamma g_{\alpha\nu}
           + g_{\gamma\alpha} \delta^\mu_\delta g_{\beta\nu}
           - g_{\gamma\beta} \delta^\mu_\delta g_{\alpha\nu} \\
        &= g_{\beta\gamma} (\delta^\mu_\alpha g_{\delta\nu}
                            - \delta^\mu_\delta g_{\alpha\nu})
           - g_{\beta\delta} (\delta^\mu_\alpha g_{\gamma\nu}
                              - \delta^\mu_\gamma g_{\alpha\nu}) \\
        &\mathrel{\phantom{=}}{}
           - g_{\alpha\gamma} (\delta^\mu_\beta g_{\delta\nu}
                               - \delta^\mu_\delta g_{\beta\nu})
           + g_{\alpha\delta} (\delta^\mu_\beta g_{\gamma\nu}
                               - \delta^\mu_\gamma g_{\beta\nu}) \\
        &= g_{\beta\gamma} {{(M_{\alpha\delta})}^\mu\!}_\nu
           - g_{\beta\delta} {{(M_{\alpha\gamma})}^\mu\!}_\nu
           - g_{\alpha\gamma} {{(M_{\beta\delta})}^\mu\!}_\nu
           + g_{\alpha\delta} {{(M_{\beta\gamma})}^\mu\!}_\nu,
\end{align*}
giving
\[
    [M_{\alpha\beta}, M_{\gamma\delta}] = g_{\beta\gamma} M_{\alpha\delta}
                                          - g_{\beta\delta} M_{\alpha\gamma}
                                          - g_{\alpha\gamma} M_{\beta\delta}
                                          + g_{\alpha\delta} M_{\beta\gamma}.
\]
This implies that every element of $\mathfrak{so}(p, q)$, $n > 2$, is a linear combination of the Lie bracket of other elements and therefore $\mathfrak{so}(p, q)$, $n > 2$ is semisimple.

From exercise~\ref{ex:suntracelessskewadjoint}, a representation of $\mathfrak{su}(n)$ consists of all traceless skew-adjoint complex $n \times n$ matrices.
As with $\mathfrak{sl}(1, \mathbb{C})$, $\mathfrak{su}(1)$ is zero-dimensional and trivially semisimple.
For $n \geq 2$, consider generators $T_a$ similar to the $E_{ij}$s of $\mathfrak{sl}(2, \mathbb{C})$ which are skew-adjoint and traceless.
There are $n^2 - 1$ such linearly independent entities and therefore they form a basis of $\mathfrak{su}(n)$.
As before, we can construct $[T_a, T_b] = f_{abc} T_c$ for $f_{abc}$ structure constants.
Trusting that these constants are non-zero, every element of $\mathfrak{su}(n)$ is a linear combination of the Lie bracket of other elements and therefore $\mathfrak{su}(n)$ is semisimple.

\end{sol}

\begin{ex}

Show that if $\mathfrak{g}$ and $\mathfrak{h}$ are Lie algebras, so is the direct sum $\mathfrak{g} \oplus \mathfrak{h}$, with bracket given by
\[
    \bigl[(x, x'), (y, y') \bigr] = \bigl( [x, y], [x'\!, y'] \bigr).
\]
Show that if $G$ and $H$ are Lie groups with Lie algebras $\mathfrak{g}$ and $\mathfrak{h}$, the Lie algebra of $G \times H$ is isomorphic to $\mathfrak{g} \oplus \mathfrak{h}$.
Show that if $\mathfrak{g}$ and $\mathfrak{h}$ are semisimple, so is $\mathfrak{g} \oplus \mathfrak{h}$.

\end{ex}

\begin{sol}

To show that $\mathfrak{g} \oplus \mathfrak{h}$ is a Lie algebra, we must check identities
\ref{sol:liebracketmatrix:anticommutativity}, %chktex 2
\ref{sol:liebracketmatrix:linearity} and      %chktex 2
\ref{sol:liebracketmatrix:jacobiidentity}     %chktex 2
from solution~\ref{sol:liebracketmatrix}.

\begin{enumerate}

    \item For anticommutativity,
    \begin{align*}
        \bigl[(x, x'), (y, y') \bigr] &= \bigl( [x, y], [x'\!, y'] \bigr) \\
                                      &= \bigl( -[y, x], -[y'\!, x'] \bigr) \\
                                      &= -\bigl[(y, y'), (x, x') \bigr].
    \end{align*}

    \item For linearity,
    \begin{align*}
        \bigl[(x, x'), \alpha (y, y') + \beta (z, z') \bigr]
            &= \bigl[ (x, x'), (\alpha y + \beta z, \alpha y' + \beta z') \bigr] \\
            &= \bigl( [x, \alpha y + \beta z], [x', \alpha y' + \beta z'] \bigr) \\
            &= \bigl( \alpha [x, y] + \beta [x, z],
                      \alpha [x', y'] + \beta [x', z'] \bigr) \\
            &= \alpha \bigl( [x, y], [x', y'] \bigr)
               + \beta \bigl( [x, z], [x', z'] \bigr) \\
            &= \alpha \bigl[ (x, x'), (y, y') \bigr]
               + \beta \bigl[ (x, x'), (z, z') \bigr].
    \end{align*}

    \item For the Jacobi identity,
    \begin{align*}
        \bigl[ (x, x'), [(y, y'), (z, z')] \bigr]
            &= \bigl[ (x, x'), \bigl( [y, z], [y', z'] \bigr) \bigr] \\
            &= \bigl( \bigl[ x, [y, z] \bigr], \bigl[ x', [y', z'] \bigr] \bigr)
    \end{align*}
    and similarly,
    \begin{align*}
        \bigl[ (y, y'), [(z, z'), (x, x')] \bigr]
            &= \bigl( \bigl[ y, [z, x] \bigr], \bigl[ y', [z', x'] \bigr] \bigr), \\
        \bigl[ (z, z'), [(x, x'), (y, y')] \bigr]
            &= \bigl( \bigl[ z, [x, y] \bigr], \bigl[ z', [x', y'] \bigr] \bigr)
    \end{align*}
    so
    \begin{align*}
        \begin{aligned}[b]
        &\bigl[ (x, x'), [(y, y'), (z, z')] \bigr] \\
        &+ \bigl[ (y, y'), [(z, z'), (x, x')] \bigr] \\
        &+ \bigl[ (z, z'), [(x, x'), (y, y')] \bigr]
        \end{aligned}
            &= \bigl( \bigl[ x, [y, z] \bigr],
                      \bigl[ x'\!, [y'\!, z'] \bigr] \bigr) \\
            &\mathrel{\phantom{=}}{}
               + \bigl( \bigl[ y, [z, x] \bigr],
                        \bigl[ y'\!, [z'\!, x'] \bigr] \bigr) \\
            &\mathrel{\phantom{=}}{}
               + \bigl( \bigl[ z, [x, y] \bigr],
                        \bigl[ z'\!, [x'\!, y'] \bigr] \bigr) \\
            &= \begin{aligned}[t]
                    \bigl( & \bigl[ x, [y, z] \bigr]
                           + \bigl[ y, [z, x] \bigr]
                           + \bigl[ z, [x, y] \bigr], \bigr. \\
                    \bigl. & \bigl[ x'\!, [y'\!, z'] \bigr]
                           + \bigl[ y'\!, [z'\!, x'] \bigr]
                           + \bigl[ z'\!, [x'\!, y'] \bigr] \bigr)
                \end{aligned} \\
            &=(0, 0).
    \end{align*}

\end{enumerate}

Consider the linear map
\begin{align*}
    f &: \mathfrak{g} \oplus \mathfrak{h} \to T_{\id}\, G \times H \\
      &: (x, x') \mapsto x \oplus x'
\end{align*}
which preserves the Lie bracket above as
\begin{align*}
    f \bigl( \bigl[(x, x'), (y, y') \bigr] \bigr)
        &= f \bigl( [x, y], [x'\!, y'] \bigr) \\
        &= [x, y] \oplus [x'\!, y'] \\
        &= [x \oplus x', y \oplus y'] \\
        &= \bigl[ f(x, x'), f(y, y') \bigr].
\end{align*}
Since $f$ is bijective, the Lie algebra of $G \times H$ is isomorphic to $\mathfrak{g} \oplus \mathfrak{h}$.

If $\mathfrak{g}$, $\mathfrak{h}$ are semisimple then any element of $\mathfrak{g}$, $\mathfrak{h}$ can be written as a linear combination of the Lie bracket of other elements.
By linearity, we need only consider $x = [y, z] \in \mathfrak{g}$ and $x' = [y', z'] \in \mathfrak{h}$.
Then
\[
    (x, x') = \bigl( [y, z], [y', z'] )
            = \bigl[ (y, y'), (z, z') \bigr]
\]
so $\mathfrak{g} \oplus \mathfrak{h}$ is also semisimple.

\end{sol}

\end{document}
