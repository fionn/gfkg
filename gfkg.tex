\documentclass[11pt, a4paper]{article}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[tracking=true, kerning=true]{microtype}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{inconsolata}
\usepackage{parskip}

\DeclareMicrotypeAlias{lmss}{cmr}

\frenchspacing

\mathtoolsset{%
    showonlyrefs,
    showmanualtags
}

\newcommand*{\email}[1]{\normalsize\texttt{\href{mailto:#1}{#1}}}
\newcommand*{\norm}[1]{\ensuremath{\left\Vert#1\right\Vert}}

\theoremstyle{definition}
\newtheorem{ex}{Exercise}[part]
\newtheorem{sol}{Solution}[part]

\title{Gauge Fields, Knots and Gravity Solutions}
\author{Fionn Fitzmaurice}
\date{}

\makeatletter
\hypersetup{pdftitle = \@title,
            pdfauthor = \@author,
            pdfcreator = TeX,
            hidelinks,
            pdfpagemode = UseNone
}
\makeatother

\author{Fionn Fitzmaurice \hspace{20pt} \email{fionn@maths.tcd.ie}}

\begin{document}

\maketitle
\thispagestyle{empty}

\part{Electromagnetism}

\section{Maxwell's Equations}

\begin{ex}

Let $\vec{k}$ be a vector in $\mathbb{R}^3$ and let $\omega = |\vec{k}|$. Fix $\vec{E} \in \mathbb{C}^3$ with $\vec{k} \cdot \vec{E} = 0$ and $i \vec{k} \times \vec{E} = \omega \vec{E}$. Show that
\[
    \vec{\mathcal{E}}(t, \vec{x}) = \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})}
\]
satisfies the vacuum Maxwell equations.

\end{ex}

\begin{sol}

Recall that Maxwell's equations (where $c = 1$) are
\begin{align*}
    \nabla \cdot \vec{B} &= 0, \tag{M1}\\
    \nabla \times \vec{E} + \frac{\partial \vec{B}}{\partial t} &= 0, \tag{M2}\\
    \nabla \cdot \vec{E} &= \rho, \tag{M3} \\
    \nabla \times \vec{B} - \frac{\partial \vec{E}}{\partial t} &= \vec{\jmath}. \tag{M4}
\end{align*}
The vacuum equations are invariant under
\[
    \vec{B} \mapsto \vec{E}, \qquad \vec{E} \mapsto - \vec{B}
\]
(electromagnetic duality) or, equivalently, for a complex-valued vector field $\vec{\mathcal{E}} = \vec{E} + i \vec{B}$,
\[
    \vec{\mathcal{E}} \mapsto i \vec{\mathcal{E}}.
\]
This lets us express the vacuum equations in terms of $\vec{\mathcal{E}}$ as
\[
    \nabla \cdot \vec{\mathcal{E}} = 0, \qquad \nabla \times \vec{\mathcal{E}} = i \frac{\partial \vec{\mathcal{E}}}{\partial t}.
\]

We'll rely on
\[
    \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} = i k_j e^{-i(\omega t - \vec{k} \cdot \vec{x})}
\]
to show that our $\vec{\mathcal{E}}$ satisfies the vacuum equations.

For the divergence,
\begin{align*}
    \nabla \cdot \vec{\mathcal{E}} &= \nabla \cdot \left(\vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})}\right) \\
        &= \sum_{j = 1}^3 \partial_j \left( E_j  e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= \sum_{j = 1}^3 E_j \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \sum_{j = 1}^3 E_j i k_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= i \vec{k} \cdot \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= 0.
\end{align*}

For the curl (dropping the summation and using Einstein notation),
\begin{align*}
    {\left( \nabla \times \vec{\mathcal{E}} \right)}_i &= \epsilon_{ijk} \partial_j \mathcal{E}_k \\
        &= \epsilon_{ijk} \partial_j \left(E_k e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= \epsilon_{ijk} E_k \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \epsilon_{ijk}i k_j E_k e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= {\left(i \vec{k} \times \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right)}_i \\
        &= \omega \vec{E}_i e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \omega \vec{\mathcal{E}}_i,
\end{align*}
so $\nabla \times \vec{\mathcal{E}} = \omega \vec{\mathcal{E}}$.
But
\begin{align*}
    \frac{\partial \vec{\mathcal{E}}}{\partial t} &= \frac{\partial}{\partial t} \left(\vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= -i \omega \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= -i \omega \vec{\mathcal{E}},
\end{align*}
giving
\[
    \nabla \times \vec{\mathcal{E}} = \omega \vec{\mathcal{E}} = i \frac{\partial \vec{\mathcal{E}}}{\partial t}
\]
and satisfying the second vacuum equation.

\end{sol}

\section{Manifolds}

A function $f: X \to Y$ from one topological space to another is defined to be continuous if, given any open set $U \subseteq Y$, the inverse image $f^{-1}(U) \subseteq X$ is open.

\begin{ex}

Show that a function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous according to the above definition if and only if it is continuous according to the epsilon--delta definition: for all $x \in \mathbb{R}^n$ and all $\epsilon > 0$, there exists $\delta > 0$ such that $\norm{y - x} < \delta$ implies $\norm{f(y) - f(x)} < \epsilon$.

\end{ex}

\begin{sol}

Suppose $f$ is continuous according to the epsilon--delta definition of continuity.
Let $V \subseteq \mathbb{R}^m$ be an open set.
For any $x \in f^{-1}(V)$, since $f(x) \in V$ there exists a ball of radius $\epsilon$, $B(f(x), \epsilon) \subseteq V$, centered at $f(x)$.
Then by the epsilon--delta condition there exists a ball of radius $\delta$, $B(x, \delta) \subseteq \mathbb{R}^n$ such that
\[
    f(B(x, \delta)) \subset B(f(x), \epsilon).
\]
Since $x$ was arbitrary, $f^{-1}(V)$ is open as all points sufficiently close to $x$ are also in $f^{-1}(V)$.

Suppose $f$ is continuous according to the topological definition of continuity.
Let $x \in \mathbb{R}^n$ and $\epsilon > 0$.
Consider the open set $f^{-1}(B(f(x), \epsilon)) \subseteq \mathbb{R}^n$.
There exists a $\delta > 0$ such that
\[
    B(x, \delta) \subset f^{-1}(B(f(x), \epsilon)).
\]
Therefore for any point $y \in B(x,\delta)$, $f(y) \in B(f(x), \epsilon)$ or, equivalently, $\norm{y - x} < \delta$ implies $\norm{f(y) - f(x)} < \epsilon$.

\end{sol}

\begin{ex}

Given a topological space $X$ and a subset $S \subseteq X$, define the \emph{induced topology} on $S$ to be the topology in which the open sets are of the form $U \cap S$, where $U$ is open in $X$.

Let $S^n$, the $n$-sphere, be the unit sphere in $\mathbb{R}^{n + 1}$:
\[
    S^n = \biggl\{\vec{x} \in \mathbb{R}^{n + 1} \Bigm| \sum_{i = 1}^{n + 1} {(x^i)}^2 = 1 \biggr\}.
\]
Show that $S^n \subset \mathbb{R}^{n + 1}$ with its induced topology is a manifold.

\end{ex}

\begin{sol}

We need to show that:
\begin{itemize}
    \item the open sets of the induced topology $\{U_\alpha\}$ cover $S^n$,
    \item there exists an atlas of charts $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ for all $\alpha$,
    \item the transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}: \mathbb{R}^n \to \mathbb{R}^n$ are smooth where defined (since we include ``smooth'' in our definition of a manifold).
\end{itemize}

Consider the sets
\[
    U_1 = S^n \setminus \left\{(0, \ldots, 0, 1)\right\}, \qquad
    U_{-1} = S^n \setminus \left\{(0, \ldots, 0, -1)\right\}
\]
which each exclude a single pole. Each $U_\alpha$ is of the form $U \cap S^n$ where $U$ is open in $\mathbb{R}^{n + 1}$.
The induced topology $\left\{U_1, U_{-1}\right\}$ is a cover of $S^n$.

Let $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ be the stereographic projection (for $\alpha \in \{-1, 1\})$.
For some $\vec{p} \in S^n$, $\varphi_\alpha(\vec{p}) \in \mathbb{R}^n$ should be a point on the line that intersects $S^n$ at $\vec{s}_\alpha = (0, \ldots, 0, \alpha)$.
Take a segment of this line parameterised by $t \in [0, 1]$ as
\begin{align*}
    (1 - t) \vec{s}_\alpha + t\vec{p} &= (t p_1, \ldots t p_n, \alpha(1 - t) + tp_{n + 1}) \\
        &= (t p_1, \ldots t p_n, \alpha + t(p_{n + 1} - \alpha)).
\end{align*}
This intersects $\mathbb{R}^n$ when the last coordinate $\alpha + t(p_{n + 1} - \alpha) = 0$, so $t = \frac{1}{1 - \alpha p_{n + 1}}$ and the projection is therefore given by
\[
    \varphi_\alpha: \vec{p} \mapsto \left(\frac{p_1}{1 - \alpha p_{n + 1}}, \ldots, \frac{p_n}{1 - \alpha p_{n + 1}}\right).
\]
Each projection is a chart and the collection of these charts is an atlas, since the union of their domains covers $S^n$.

Denoting $\varphi_\alpha(\vec{p}) = \vec{x}_\alpha = \left(x_\alpha^1, \ldots, x_\alpha^n\right)$, the $L\!^2$-norm
\begin{align*}
    r_\alpha^2 &= \sum_{i = 1}^n {(x_\alpha^i)}^2 \\
               &= \frac{p_1^2 + \cdots + p_n^2}{{(1 - \alpha p_{n + 1})}^2} \\
               &= \frac{1 - p_{n + 1}^2}{{(1 - \alpha p_{n + 1})}^2} \\
               &= \frac{(1 + p_{n + 1})(1 - p_{n + 1})}{{(1 - \alpha p_{n + 1})}^2} \\
               &= {\left(\frac{1 + p_{n + 1}}{1 - p_{n + 1}}\right)}^\alpha,
\end{align*}
so
\[
    p_{n + 1} = \alpha\frac{r_\alpha^2 - 1}{r_\alpha^2 + 1}.
\]
This gives us a general expression for the points $p_1, \ldots, p_n$ on the manifold in terms of our chart's coordinate system as
\begin{align*}
    p_i &= x_\alpha^i (1 - \alpha p_{n + 1}) \\
        &= \frac{2 x_\alpha^i}{r_\alpha^2 + 1},
\end{align*}
so the inverse projections $\varphi_\alpha^{-1}: \mathbb{R}^n \to S^n$ are given by
\[
    \varphi_\alpha^{-1}: \vec{x} \mapsto \left(\frac{2 x^1}{r^2 + 1}, \ldots, \frac{2 x^n}{r^2 + 1}, \alpha \frac{r^2 - 1}{r^2 + 1} \right).
\]
For inverse map $\varphi_\beta^{-1}$, note that the point $p_{n + 1}$ is given by
\[
    p_{n + 1} = \beta \frac{r^2 - 1}{r^2 + 1}.
\]
From this, and assuming $\alpha$, $\beta$ are distinct so $\alpha \beta = -1$, we get that
\[
    \frac{1}{1 - \alpha p_{n + 1}} = \frac{r^2 + 1}{2 r^2}.
\]
The transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}: \mathbb{R}^n \to \mathbb{R}^n$ (with distinct $\alpha$, $\beta$) are then given by
\begin{align*}
    \varphi_\alpha \circ \varphi_\beta^{-1} (\vec{x}) &= \varphi_\alpha \left(\left(\frac{2 x^1}{r^2 + 1}, \ldots, \frac{2 x^n}{r^2 + 1}, \beta \frac{r^2 - 1}{r^2 + 1} \right)\right) \\
        &= \left(\frac{2 x^1}{r^2 + 1} \cdot \frac{r^2 + 1}{2 r^2},
                 \ldots,
                 \frac{2 x^n}{r^2 + 1} \cdot \frac{r^2 + 1}{2 r^2}
            \right) \\
        &= \frac{\vec{x}}{\norm{x}^2}.
\end{align*}
These transition functions are inversions on the $n$-sphere and are smooth where they are defined.

\end{sol}

\begin{ex}

Show that if $M$ is a manifold and $U$ is an open subset of $M$, then $U$ with its induced topology is a manifold.

\end{ex}

\begin{sol}

All subsets $U_\alpha \subset U$ are of the form $V \cap U$ where $V$ is open in $M$, so the open sets of the induced topology cover $U$.

We can construct an atlas by taking the charts on $M$, $\varphi_\alpha: V_\alpha \to \mathbb{R}^n$, and defining
\begin{align*}
    \varphi^U_\alpha&: U_\alpha \to \mathbb{R}^n, \\
                    &: u \mapsto \varphi_\alpha(u),
\end{align*}
i.e. $\varphi^U_\alpha = \varphi_\alpha$ for all $U_\alpha$.
Since $U_\alpha$ is open, we have well defined transition functions so $U$ with the induced topology is a manifold.

\end{sol}

\begin{ex}

Given topological spaces $X$ and $Y$, we give $X \times Y$ the \emph{product topology} in which a set is open if and only if it is a union of sets of the form $U \times V$, where $U$ is open in $X$ and $V$ is open in $Y$. Show that if $M$ is an $m$-dimensional manifold and $N$ is an $n$-dimensional manifold, $M \times N$ is an $(m + n)$-dimensional manifold.

\end{ex}

\begin{sol}

For every point $(u, v) \in M \times N$, there exists a set $U \times V$ where $U$ is open in $M$ and $V$ is open in $N$ such that $u \in U$, $v \in V$.
Therefore $U \times V$ is an open set under the product topology and $M \times N$ is a topological space.

Given $M$, $N$ are manifolds, they have atlases
\[
    \left\{\varphi^M_\alpha : U_\alpha \to \mathbb{R}^m \right\}, \qquad
    \left\{\varphi^N_\beta : V_\beta \to \mathbb{R}^n \right\}
\]
for all $U_\alpha$ open in $M$, $V_\beta$ open in $N$.

For some $u \in U_\alpha$, $v \in V_\beta$, denote
\[
    \varphi^M_\alpha: u \mapsto \vec{x} = (x_1, \ldots, x_m), \quad
    \varphi^N_\beta: v \mapsto \vec{y} = (y_1, \ldots, y_n).
\]

We can construct maps $\tilde{\varphi}_{\alpha\beta}:\ U_\alpha \times V_\beta \to \mathbb{R}^m \times \mathbb{R}^n$ as
\begin{align*}
    \tilde{\varphi}_{\alpha\beta} (u, v) &= \left(\varphi^M_\alpha(u), \varphi^N_\beta(v)\right) \\
        &= (\vec{x}, \vec{y}).
\end{align*}
This is obviously invertible via
\[
    \tilde{\varphi}_{\alpha\beta}^{-1}(\vec{x}, \vec{y}) = \left({(\varphi^M_\alpha)}^{-1}(\vec{x}), {(\varphi^N_\beta)}^{-1}(\vec{y})\right) = (u, v).
\]
because the inverse charts are guaranteed to exist.

The product space $\mathbb{R}^m \times \mathbb{R}^n$ is homeomorphic to $\mathbb{R}^{m + n}$ under
\[
    h(\vec{x}, \vec{y}) = (x_1, \ldots, x_m, y_1, \ldots, y_n),
\]
so we can construct new smooth maps $\varphi_{\alpha\beta} = h \circ \tilde{\varphi}_{\alpha\beta}$ that target $\mathbb{R}^{m + n}$.
The transition functions
\[
    \varphi_{\alpha\beta} \circ \varphi_{\alpha\beta}^{-1}: \mathbb{R}^{m + n} \to \mathbb{R}^{m + n}
\]
are similarly obviously smooth where defined, so $\varphi_{\alpha\beta}$ is a chart and the collection of these charts for all $U_\alpha$, $V_\beta$ is an atlas, therefore $M \times N$ is a manifold.

\end{sol}

\begin{ex}

Given topological spaces $X$ and $Y$, we give $X \cup Y$ the \emph{disjoint union topology} in which a set is open if and only if it is the union of an open subset of $X$ and an open subset of $Y$.
Show that if $M$ and $N$ are $n$-dimensional manifolds the disjoint union $M \cup N$ is an $n$-dimensional manifold.

\end{ex}

\begin{sol}

Any point $p \in M \cup N$ is either in $M$ or $N$. Consider a neighbourhood $X$ of $p$. This will be of the form $U \cup V$ for $U$, $V$ open subsets of $M$, $N$ since $p \in X$ is equivalent to $p \in X \cup \varnothing$.

Given $M$, $N$ are manifolds, they have atlases
\[
    \left\{\varphi^M_\alpha : U_\alpha \to \mathbb{R}^n \right\}, \qquad
    \left\{\varphi^N_\beta : V_\beta \to \mathbb{R}^n \right\}
\]
for all $U_\alpha$ open in $M$, $V_\beta$ open in $N$. Therefore any neighbourhood of $p \in M \cup N$ has a chart, for all $p$.

Since the transition functions exist independently, they are automatically smooth. Therefore $M \cup N$ is an $n$-dimensional manifold.

\end{sol}

\section{Vector Fields}

\begin{ex}

Show that $v + w$ and $gw \in \text{Vect}(M)$.

\end{ex}

\begin{sol}

For the sum,
\begin{align*}
    (v + w)(f + g) &= v(f + g) + w(f + g) \\
        &= v(f) + v(g) + w(f) + w(g) \\
        &= (v + w)(f) + (v + w)(g),
\end{align*}
\begin{align*}
    (v + w)(\alpha f) &= v(\alpha f) + w(\alpha f) \\
        &= \alpha v(f) + \alpha w(f) \\
        &= \alpha (v(f) + w(f)) \\
        &= \alpha (v + w)(f),
\end{align*}
\begin{align*}
    (v + w)(fg) &= v(fg) + w(fg) \\
        &= v(f)g + fv(g) + w(f)g + fw(g) \\
        &= (v(f) + w(f))g + f \cdot (v(g) + w(g)) \\
        &= (v + w)(f)g + f \cdot (v + w)(g).
\end{align*}

For the multiplication,
\begin{align*}
    gw(f + h) &= g \cdot (w(f) + w(h)) \\
        &= gw(f) + gw(h),
\end{align*}
\begin{align*}
    gw(\alpha f) &= g \cdot \alpha w(f) \\
        &= \alpha g w(f),
\end{align*}
\begin{align*}
    gw(fh) &= g \cdot (w(f)h + f w(h)) \\
        &= g w(f) h + g f w(h) \\
        &= gw(f) h + fgw(h).
\end{align*}

\end{sol}

\begin{ex}\label{ex:module}

Show that the following rules [hold] for all $v, w \in \text{Vect}(M)$ and $f, g \in C^\infty(M)$:
\begin{align*}
    f(v + w) &= fv + fw, \\
    (f + g)v &= fv + gv, \\
    (fg)v &= f(gv), \\
    1v &= v.
\end{align*}
(Here ``$1$'' denotes the constant function equal to $1$ on all of $M$.) Mathematically, we summarize these rules by saying that $\text{Vect}(M)$ is a \emph{module over} $C^\infty(M)$.

\end{ex}

\begin{sol}\label{sol:module}

For all $g \in C^\infty(M)$,
\[
    f(v + w)g = fv(g) + fw(g) = (fv + fw)(g),
\]
so $f(v + w) = fv + fw$.

For all $h \in C^\infty(M)$,
\[
    (f + g)v(h) = fv(h) + gv(h) = (fv + gv)(h),
\]
so $(f + g)v = fv + gv$.


For all $h \in C^\infty(M)$,
\[
    (fg)v(h) = f \cdot gv(h) = f(gv)(h)
\]
so $(fg)v = f(gv)$.

For all $f \in C^\infty(M)$,
\[
    (1v)(f) = 1v(f) = v(f).
\]

Therefore $\text{Vect}(M)$ is a module over $C^\infty(M)$.

\end{sol}

\begin{ex}

Show that if $v^\mu\partial_\mu = 0$, that is, $v^\mu \partial_\mu f = 0$ for all $f \in C^\infty(\mathbb{R}^n)$, we must have $v^\mu = 0$ for all $\mu$.

\end{ex}

\begin{sol}\label{sol:basis}

Choose a function $f: \vec{x} \mapsto x^\nu$ for some index $0 < \nu \leq n$. Then
\[
    v^\mu \partial_\mu x^\nu = v^\mu \delta_\mu^\nu = v^\nu.
\]
If $v^\mu\partial_\mu = 0$, we get $v^\mu$ = 0 from above.

\end{sol}

\subsection{Tangent Vectors}

\begin{ex}

Let $v, w \in \text{Vect}(M)$. Show that $v = w$ if and only if $v_p = w_p$ for all $p \in M$.

\end{ex}

\begin{sol}\label{sol:tangentvectorpointequality}

Recall the definition $v_p(f) = v(f)(p)$.

If $v = w$, then
\[
    v_p(f) = v(f)(p) = w(f)(p) = w_p(f)
\]
so $v_p = w_p$.

The other way around, if $v_p(f) = w_p(f)$ then $v(f)(p) = w(f)(p)$, which must be true for all $p \in M$, so $v(f) = w(f)$ and therefore $v = w$.

\end{sol}

\begin{ex}

Show that $T_p M$ is a vector space over the real numbers.

\end{ex}

\begin{sol}

We must show that tangent vectors $v_p \in T_p M$ satisfy the axioms of vector spaces.

Let $u, v, w \in T_p M$ and $\alpha, \beta \in \mathbb{R}$.

To check associativity,
\begin{align*}
    (u + (v + w))(f) &= u(f) + (v + w)(f) \\
        &= u(f) + v(f) + w(f) \\
        &= (u(f) + v(f)) + w(f) \\
        &= (u + v)(f) + w(f),
\end{align*}
so $u + (v + w) = (u + v) + w$.

Commutativity holds since $\mathbb{R}$ is commutative.

An additive identity vector $0$ exists since
\[
    (v + 0)(f) = v(f) + 0(f) = v(f)
\]
by defining $0$ to be the tangent vector that maps all functions to $0$.

We can construct for every tangent vector $v$ an additive inverse $-v$ as $(-v)(f) = -v(f)$.

We have compatibility of scalar and field multiplication since
\[
    \alpha(\beta v)(f) = \alpha (\beta v(f)) = \alpha \beta v(f) = (\alpha \beta) v(f).
\]

The existence of a scalar multiplicative identity follows from solution~\ref{sol:module}.

For distributivity,
\[
    \alpha(u + v)(f) = \alpha (u(f) + v(f)) = \alpha u(f) + \alpha v(f)
\]
and
\[
    (\alpha + \beta) v(f) = \alpha v(f) + \beta v(f).
\]

\end{sol}

\begin{ex}

Check that $\gamma'(t) \in T_{\gamma(t)}M$ using the definitions.

\end{ex}

\begin{sol}

We have that
\[
    \gamma'(t): f \mapsto \frac{d}{dt} f\left( \gamma(t) \right).
\]
Notice that
\begin{align*}
    &\gamma'(t)(f + g) = \gamma'(t)(f) + \gamma'(t)(g), \\
    &\gamma'(t)(\alpha f) = \alpha \gamma'(t)(f), \\
    &\gamma'(t)(f g) = \gamma'(t)(f) g + f \gamma'(t)(g),
\end{align*}
so $\gamma'(t)$ is a tangent vector.

\end{sol}

\subsection{Covariant Versus Contravariant}

\begin{ex}

Let $\phi: \mathbb{R} \to \mathbb{R}$ be given by $\phi(t) = e^t$.
Let $x$ be the usual coordinate function on $\mathbb{R}$.
Show that $\phi^* x = e^x$.

\end{ex}

\begin{sol}\label{sol:pullbackexponential}

The pullback $\phi^*: C^\infty(N) \to C^\infty(M) $ of $f: N \to \mathbb{R}$ by $\phi: M \to N$ is defined as
\[
    \phi^*f = f \circ \phi. \tag{pullback of a function}\label{eq:pullbackfunction}
\]
Consider a chart $\varphi: M \to \mathbb{R}^n$ mapping $p \in M$ to $\varphi(p) = \left\{x^\mu(p)\right\}$.
Note that each $x^\mu$ is a function taking $p$ to the $\mu$\textsuperscript{th} coordinate of its image in $\mathbb{R}^n$.

Since our manifold is $\mathbb{R}$, the ``usual coordinate function'' in this case is the identity (under trivial coordinate transformation $t \to x$, say), so
\[
    (\phi^* x)(t) = x(\phi(t)) = x(e^t) = e^x
\]
(where we abuse notation and identify the coordinate transformation function and its target as $x$).

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ be rotation counterclockwise by an angle $\theta$. Let $x$, $y$ be the usual coordinate functions on $\mathbb{R}^2$. Show that
\begin{align*}
    \phi^* x &= \cos(\theta) x - \sin(\theta) y, \\
    \phi^* y &= \sin(\theta) x + \cos(\theta) y.
\end{align*}

\end{ex}

\begin{sol}\label{sol:pullbackrotation}

If $\phi$ is a positive rotation by a (fixed) angle $\theta$, we can express it as
\[
    \phi: \begin{pmatrix}
            u \\ v
        \end{pmatrix}
        \mapsto
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix}
        \begin{pmatrix}
            u \\ v
        \end{pmatrix}
        =
        \begin{pmatrix}
            \cos(\theta) u - \sin(\theta) v \\
            \sin(\theta) u + \cos(\theta) v
        \end{pmatrix}.
\]
As before, consider the chart $\varphi(p) = \left\{x^\mu(p)\right\} = \left\{x(p), y(p)\right\}$.
Then $\phi^* x(p) = x(\phi(p))$ is the $x$-coordinate, so for $p = (u, v)$,
\begin{align*}
    \phi^* x(p) &= x(\phi(p)) \\
        &= \cos(\theta) u - \sin(\theta) v \\
        &= \cos(\theta) x(p) - \sin(\theta) y(p)
\end{align*}
and similarly for $\phi^* y$.

\end{sol}

\begin{ex}

Show that this definition of smoothness is consistent with the previous definitions of smooth functions $f: M \to \mathbb{R}$ and smooth curves $\gamma: \mathbb{R} \to M$.

\end{ex}

\begin{sol}

Recall the definition of smooth functions between manifolds.

\begin{quote}
$\phi: M \to N$ is smooth if $f \in C^\infty(N)$ implies that $\phi^* f \in C^\infty(M)$.
\end{quote}

Our other two definitions of smooth functions and smooth curves are:
\begin{itemize}
    \item a function $f: M \to \mathbb{R}$ is smooth if for all $\alpha$, $f \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$ is smooth,
    \item a curve $\gamma: \mathbb{R} \to M$ is smooth if $f(\gamma(t))$ depends smoothly on $t$ for any $f \in C^\infty(M)$.
\end{itemize}

If $N = \mathbb{R}$, our definition of smooth functions between manifolds is that $\phi: M \to \mathbb{R}$ is smooth if $f \in C^\infty(\mathbb{R})$ implies that $\phi^* f \in C^\infty(M)$.
But if we assume $f \in C^\infty(\mathbb{R})$ then $\phi^* f = f \circ \phi \in C^\infty(M)$ requires that $\phi \in C^\infty(M)$ and $\phi: M \to \mathbb{R}$ is smooth if for all $\alpha$, $\phi \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$ is smooth.

Let $\phi: M \to \mathbb{R}$ be a smooth function (i.e.\ for all $\alpha$, $\phi \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$).
Let $f \in C^\infty(\mathbb{R})$.
Then $f \circ \phi \circ \varphi_\alpha^{-1}$ is smooth since it is the composition of smooth functions, so $f \circ \phi = \phi^* f$ is smooth.

If the domain is $\mathbb{R}$, our definition of smooth functions between manifolds is that $\gamma: \mathbb{R} \to M$ is smooth if $f \in C^\infty(M)$ implies that $\gamma^* f \in C^\infty(\mathbb{R})$.
But if we assume $f \in C^\infty(M)$ then $\gamma^* f = f \circ \gamma \in C^\infty(\mathbb{R})$ is smooth by the definition of smooth curves.

Let $\gamma: \mathbb{R} \to M$ be smooth, i.e. $f\circ \gamma$ is smooth for all $f \in C^\infty(M)$.
Since $\gamma^* f = f \circ \gamma$, $\gamma^* f$ is smooth too.

\end{sol}

\begin{ex}

Prove that $(\phi \circ \gamma)'(t) = \phi_*(\gamma'(t))$.

\end{ex}

\begin{sol}

The pushforward $\phi_*: T_p M \to T_{\phi(p)} N$ of $v \in T_p M$ by $\phi: M \to N$ is given by
\[
    (\phi_* v) (f) = v (\phi^* f). \tag{pushforward of a vector}\label{eq:pushforwardvector}
\]
Then
\begin{align*}
    (\phi \circ \gamma)'(t)(f) &= \frac{d}{dt} f\left((\phi \circ \gamma)(t)\right) \\
        &= \frac{d}{dt} (f \circ \phi \circ \gamma)(t) \\
        &= \frac{d}{dt} (f \circ \phi) (\gamma(t)) \\
        &= \gamma'(t)(f \circ \phi) \\
        &= \gamma'(t)(\phi^* f) \\
        &= (\phi_* (\gamma'(t)))(f).
\end{align*}

\end{sol}

\begin{ex}

Show that the pushforward operation
\[
    \phi_*: T_p M \to T_{\phi(p)} N
\]
is linear.

\end{ex}

\begin{sol}

Let $v, w \in T_p M$, $\alpha, \beta \in \mathbb{R}$, $f \in C^\infty(N)$. $\phi_*$ is linear since
\begin{align*}
    \left(\phi_*(\alpha v + \beta w)\right)(f) &= (\alpha v + \beta w) (\phi^* f) \\
        &= \alpha v (\phi^* f) + \beta w (\phi^* f) \\
        &= \alpha (\phi_* v)(f) + \beta (\phi_* w)(f) \\
        &= \left(\alpha (\phi_* v) + \beta (\phi_* w)\right)(f).
\end{align*}

\end{sol}

\begin{ex}

Show that if $\phi: M \to N$ is a diffeomorphism, we can push forward a vector field $v$ on $M$ to obtain a vector field $\phi_*v$ on $N$ satisfying
\[
    {(\phi_* v)}_q = \phi_*(v_p)
\]
whenever $\phi(p) = q$.

\end{ex}

\begin{sol}\label{sol:pushforwardvectorfield}

Note that the definition of the pushforward is sloppy, since the left side must be evaluated on $N$ while the right side is on $M$.

Looking at the action of $\phi_* v$ on a function $f \in C^\infty (N)$ and denoting the points that each side act on as $p \in M$, $q \in N$,
\begin{align*}
    {(\phi_* v)}_q (f) &= (\phi_* v) (f) (q) \\
        &= v(\phi^* f)(p) \\
        &= v_p(\phi^* f) \\
        &= (\phi_* (v_p))(f).
\end{align*}
But
\begin{align*}
    v_p (\phi^* f) &= v_p (f \circ \phi) \\
            &= v(f(\phi(p))) \\
            &= w_{\phi(p)}(f)
\end{align*}
for some $w \in \text{Vect}(N)$.

It's tempting to write this as $v_{\phi(p)}(f)$, but $v \in \text{Vect}(M)$ whereas $\phi(p) \in N$.
Instead we need exactly the pushforward of $v$, so we get $w_{\phi(p)} = {(\phi_* v)}_{\phi(p)}$ and the equality holds when $\phi(p) = q$.

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ be [a] rotation counterclockwise by an angle $\theta$. Let $\partial_x$, $\partial_y$ be the coordinate vector fields on $\mathbb{R}^2$. Show that at any point of $\mathbb{R}^2$,
\begin{align*}
    \phi_* \partial_x &= \cos(\theta)\partial_x + \sin(\theta)\partial_y, \\
    \phi_* \partial_y &= - \sin(\theta)\partial_x + \cos(\theta)\partial_y.
\end{align*}

\end{ex}

\begin{sol}\label{sol:pushforwardrotation}

Denote $\phi: (x, y) \mapsto (u(x, y), v(x, y))$ where $u, v$ are functions as per solution~\ref{sol:pullbackrotation} and let $f \in C^\infty(\mathbb{R}^2)$.

For a vector $\partial_i$, the pushforward acting on $f$ is
\begin{align*}
    (\phi_* \partial_i)(f) &= \partial_i (\phi^* f) \\
        &= \partial_i (f \circ \phi) \\
        &= \partial_u f \cdot \partial_i u + \partial_v f \cdot \partial_i v
\end{align*}
and at a point $p = (x, y) \in \mathbb{R}^2$,
\[
     {(\phi_* \partial_i)}_p(f) = \partial_i u \cdot \partial_u f(u, v) + \partial_i v \cdot \partial_v f(u, v).
\]
We want to consider $f$ at $p$ rather than at $\phi(p)$, so change variables as $\partial_u f(u, v) = \partial_x f(x, y)$, $\partial_v f(u, v) = \partial_y f(x, y)$.

Consider $\phi_* \partial_x$ and $\phi_* \partial_y$,
\begin{align*}
    {(\phi_* \partial_x)}_p(f) &= \partial_x u \cdot \partial_x f(x, y) + \partial_x v \cdot \partial_y f(x, y) \\
        &= \cos(\theta) \partial_x f(x, y) + \sin(\theta) \partial_y f(x, y), \\[2\jot]
    {(\phi_* \partial_y)}_p(f) &= \partial_x u \cdot \partial_x f(x, y) + \partial_y v \cdot \partial_y f(x, y) \\
        &= -\sin(\theta) \partial_x f(x, y) + \cos(\theta) \partial_y f(x, y),
\end{align*}
giving us
\begin{align*}
    \phi_* \partial_x &= \cos(\theta) \partial_x + \sin(\theta) \partial_y, \\
    \phi_* \partial_y &= -\sin(\theta) \partial_x + \cos(\theta) \partial_y.
\end{align*}
We can see that this is consistent by taking the result from solution~\ref{sol:pullbackrotation},
\begin{align*}
    (\phi_* \partial_x) x &= \partial_x(\phi^* x) = \cos(\theta), \\
    (\phi_* \partial_x) y &= \partial_x(\phi^* y) = \sin(\theta), \\
    (\phi_* \partial_y) x &= \partial_y(\phi^* x) = -\sin(\theta), \\
    (\phi_* \partial_y) y &= \partial_y(\phi^* y) = \cos(\theta),
\end{align*}
where we get back the $x$- and $y$-components of $\phi_* \partial_x$, $\phi_* \partial_y$, respectively.

\end{sol}

\subsection{Flows and the Lie Bracket}

\begin{ex}

Let $v$ be the vector field $x^2 \partial_x + y \partial_y$ on $\mathbb{R}^2$. Calculate the integral curves $\gamma(t)$ and see which ones are defined for all $t$.

\end{ex}

\begin{sol}

Integral curves satisfy $\gamma'(t) = v_{\gamma(t)}$, $\gamma(0) = p$.

Denote $\gamma(t) = (x(t), y(t)) \in \mathbb{R}^2$. Then from the definition of tangent curves,
\begin{align*}
    \frac{d}{dt} f(\gamma(t)) &= \frac{d}{dt} f(x, y) \\
    &= \partial_x f(x, y) \dot{x} + \partial_y f(x, y) \dot{y} \\
    &\overset{!}{=} x^2 \partial_x f(x, y) + y \partial_y f(x, y)
\end{align*}
giving us differential equations $\dot{x}(t) = {x(t)}^2$, $\dot{y}(t) = y(t)$ with solutions
\[
    x(t) = \frac{1}{\alpha - t}, \qquad y(t) = \beta e^t.
\]
Fix the constants $\alpha$, $\beta$ with initial condition $\gamma(0) = p = (x(0), y(0))$. Then
\[
    x(t) = \frac{x(0)}{1 - x(0)t}, \qquad y(t) = y(0) e^t.
\]
When $x(0) = 0$ we get $x(t) = 0$ for all $t$. Otherwise, we get a singularity at $t = \frac{1}{x(0)}$, so the integral curves $\gamma$ are defined for all $t$ when starting at $p = (0, b)$ for any $b \in \mathbb{R}$.

\end{sol}

\begin{ex}

Show that $\phi_0$ is the identity map $\text{id}: X \to X$ and that for all $s, t \in \mathbb{R}$ we have $\phi_t \circ \phi_s = \phi_{t + s}$.

\end{ex}

\begin{sol}

By definition, the flow $\phi_t(p)$ is defined to be the point on the integral curve a parameter distance $t$ from $p$, therefore at $t = 0$, $\phi_0(p) = p$.

Pick some value $t = t_0$ and label the point $\phi_{t_0}(p) = q$. Let $t_1 = t_0 + s$, so $\phi_{t_1}(p) = \phi_{t_0 + s}(p)$. But this is a parameter distance $s$ from $q$, so $\phi_{t_1}(p) = \phi_s(q)$ and thus
\[
    \phi_{t_0 + s}(p) = \phi_s \circ \phi_{t_0}(p).
\]
It follows from this that $\phi_s^{-1} = \phi_{-s}$, so the flow is an Abelian group.

\end{sol}

\begin{ex}

Consider the normalised vector fields in the $r$ and $\theta$ directions on the plane in polar coordinates (not defined at the origin):
\[
    v = \frac{x \partial_x + y \partial_y}{\sqrt{x^2 + y^2}}, \qquad w = \frac{x \partial_y - y \partial_x}{\sqrt{x^2 + y^2}}.
\]
Calculate $[v, w]$.

\end{ex}

\begin{sol}

Since $x = r \cos(\theta)$, $y = r \sin(\theta)$, we have for some $f \in C^\infty(\mathbb{R}^2)$,
\begin{align*}
    \partial_r f &=\cos(\theta)  \partial_x f + \sin(\theta) \partial_y f \\
    \partial_\theta f &= -r \sin(\theta) \partial_x f + r \cos(\theta) \partial_y f
\end{align*}
so $v = \partial_r$, $w = \frac{\partial_\theta}{r}$.
Then
\begin{align*}
    [v, w]f &= v(w(f)) - w(v(f)) \\
        &= v\left(\frac{\partial_\theta f}{r}\right) - w(\partial_r f) \\
        &= \partial_r\left(\frac{\partial_\theta f}{r}\right) - \frac{\partial_\theta}{r}(\partial_r f) \\
        &= \frac{r \partial_r \partial_\theta f - \partial_\theta f}{r^2} - \frac{\partial_\theta \partial_r f}{r} \\
        &= \frac{1}{r} \left(\partial_r \partial_\theta f - \frac{\partial_\theta f}{r} - \partial_\theta \partial_r f\right) \\
        &= -\frac{\partial_\theta f}{r^2} \\
        &= -\frac{w}{r} f
\end{align*}
so $[v, w] = -\frac{w}{r}$.

We could also do this the hard way,
\begin{align*}
    [v, w]f &= v(w(f)) - w(v(f)) \\
        &= \frac{(x \partial_x + y \partial_y)(x \partial_y f - y \partial_x f) - (x \partial_y - y \partial_x)(x \partial_x f + y \partial_y f)}{x^2 + y^2} \\
        &= \frac{\splitdfrac{x \partial_x (x\partial_y f)
                             - x\partial_x(y \partial_x f)
                             + y \partial_y(x\partial_y f)
                             - y \partial_y(y \partial_x f)}
                            {{}-x\partial_y(x\partial_x f)
                             - x\partial_y(y\partial_y f)
                             + y\partial_x(x \partial_x f)
                             + y\partial_x (y \partial_y f)}
                 }{x^2 + y^2} \\
        &= \frac{y \partial_x f - x \partial_y f}{x^2 + y^2}
\end{align*}
giving the same result
\[
    [v, w] = \frac{y \partial_x - x \partial_y}{x^2 + y^2} = -\frac{w}{r}.
\]

\end{sol}

\begin{ex}

Check the equation above.

\end{ex}

\begin{sol}

We need to check that for any $f \in C^\infty(M)$,
\[
    [v, w](f)(p) = \frac{\partial^2}{\partial t \partial s} \Bigl( f(\psi_s(\phi_t(p))) - f(\phi_t(\psi_s(p))) \Bigr) \Big|_{s = t = 0}
\]
where $\phi_t$, $\psi_s$ are flows generated by $v$ and $w$, respectively.

We have that
\[
    (vf)(p) = \frac{d}{dt} f(\phi_t(p)) \Big|_{t=0}, \qquad
    (wf)(p) = \frac{d}{ds} f(\psi_s(p)) \Big|_{s=0},
\]
so
\begin{align*}
    (vw)(f)(p) &= \frac{d}{dt} wf(\phi_t(p)) \Big|_{t = 0} \\
        &= \frac{\partial^2}{\partial t \partial s} f(\psi_s(\phi_t(p))) \Big|_{s = t = 0}
\intertext{and similarly}
    (wv)(f)(p) &= \frac{d}{ds} vf(\psi_s(p)) \Big|_{s = 0} \\
    &= \frac{\partial^2}{\partial s \partial t} f(\phi_t(\psi_s(p))) \Big|_{t = s = 0}.
\end{align*}
The result follows immediately.

\end{sol}

\begin{ex}

Show that for all vector fields $u$, $v$, $w$ on a manifold, and all real numbers $\alpha$, $\beta$, we have:
\begin{enumerate}
    \item $[v, w] = - [w, v]$,
    \item $[u, \alpha v + \beta w] = \alpha[u, v] + \beta [u, w]$,
    \item the \emph{Jacobi identity}: $\big[u, [v, w]\big] + \big[v, [w, u]\big] + \big[w, [u, v]\big] = 0$.
\end{enumerate}

\end{ex}

\begin{sol}
\mbox{}

\begin{enumerate}

\item The Lie bracket is antisymmetric.
    \[
        [v, w] = vw - wv = -(wv - vw) = -[w, v].
    \]

\item The Lie bracket is linear.
    \begin{align*}
        [u, \alpha v + \beta w] &= u (\alpha v + \beta w) - (\alpha v + \beta w) u \\
            &= \alpha uv + \beta uw - \alpha vu - \beta wu \\
            &= \alpha(uv - vu) + \beta(uw - wu) \\
            &= \alpha [u, v] + \beta [u, w].
    \end{align*}

\item The Lie bracket satisfies the Jacobi identity.
    \begin{align*}
        \big[u, [v, w]\big] &= u[v, w] - [v, w]u \\
            &= u(vw - wv) - (vw - wv)u \\
            &= uvw - uwv - vwu + wvu,
    \end{align*}
    so similarly,
    \begin{align*}
        \big[v, [w, u]\big] &= vwu - vuw - wuv + uwv, \\
        \big[w, [u, v]\big] &= wuv - wvu - uvw + vuw.
    \end{align*}
    Combining everything, we get
    \begin{align*}
        \big[u, [v, w]\big] + \big[v, [w, u]\big] + \big[w, [u, v]\big] =&\ uvw - uwv - vwu + wvu \\
            &+ vwu - vuw - wuv + uwv \\
            &+ wuv - wvu - uvw + vuw \\
            =&\ 0.
    \end{align*}
\end{enumerate}

\end{sol}

\section{Differential Forms}

\subsection{1-forms}

\begin{ex}

Show that $\omega + \mu$ and $f\omega$ are really 1-forms, i.e., show linearity over $C^\infty(M)$.

\end{ex}

\begin{sol}

Let $g, h \in C^\infty(M)$, $v, w \in \text{Vect}(M)$.

$\omega + \mu$ is linear over $C^\infty(M)$ since
\begin{align*}
    (\omega + \mu)(gv + hw) &= (\omega + \mu)(gv) + (\omega + \mu)(hw) \\
        &= \omega(gv) + \mu(gv) + \omega(hw) + \mu(hw) \\
        &= g\omega(v) + g\mu(v) + h\omega(w) + h\mu(w) \\
        &= g(\omega + \mu)(v)  + h(\omega + \mu)(w)
\end{align*}
and $f\omega$ is linear over $C^\infty(M)$ since
\begin{align*}
    (f\omega)(gv + hw) &= f\omega(gv + hw) \\
        &= fg\omega(v) + fh\omega(w) \\
        &= gf\omega(v) + hf\omega(w) \\
        &= g(f\omega)(v) + h(f\omega)(w)
\end{align*}
(this is a product, not composition, so $fg = gf$).

\end{sol}

\begin{ex}

Show that $\Omega^1(M)$ is a module over $C^\infty(M)$ (see the definition in exercise~\ref{ex:module}).

\end{ex}

\begin{sol}

Let $\omega, \mu \in \Omega^1(M)$, $v\in \text{Vect}(M)$.

For all $f \in C^\infty(M)$,
\[
    f(\omega + \mu)(v) = f(\omega v + \mu v) = f\omega v + f\mu v
\]
so $f(\omega + \mu) = f\omega + f\mu$.

For all $f, g \in C^\infty(M)$,
\[
    (f + g)\omega(v) = f\omega(v) + g\omega(v)
\]
so $(f + g)\omega = f\omega + g\omega$.

For all $f, g \in C^\infty(M)$,
\[
    (fg)\omega(v) = f(g\omega)(v) = (fg\omega)(v)
\]
so $(fg)\omega = fg\omega$.

Let $1$ be the constant function equal to $1$ on all of $M$. Then
\[
    (1\omega)(v) = 1\omega(v) = \omega(v).
\]

Therefore $\Omega^1(M)$ is a module over $C^\infty(M)$.

\end{sol}

\begin{ex}

Show that
\begin{align*}
    & d(f + g) = df + dg, \\
    & d(\alpha f) = \alpha\, df, \\
    & (f + g) dh = f\, dh + g\, dh, \\
    & d(fg) = f \, dg + g\, df
\end{align*}
for any $f, g, h \in C^\infty(M)$ and any $\alpha \in \mathbb{R}$.

\end{ex}

\begin{sol}

Let $v \in \text{Vect}(M)$. First consider linearity.
\begin{align*}
    d(f + g)v &= v(f + g) \\
        &= vf + vg \\
        &= df(v) + dg(v) \\
        &= (df + dg)(v),
\end{align*}
\[
    d(\alpha f)(v) = v(\alpha f) = \alpha v(f) = \alpha \, df(v),
\]
\begin{align*}
    (f + g)dh(v) &= (f + g)v(h) \\
        &= fv(h) + gv(h) \\
        &= f\, dh(v) + g \, dh(v).
\end{align*}
The Leibniz law holds since
\begin{align*}
    d(fg)(v) &= v(fg) \\
        &= f v(g) + g v(f) \\
        &= f \, dg(v) + g \, df(v).
\end{align*}

\end{sol}

\begin{ex}

Suppose $f(x^1, \ldots, x^n)$ is a function on $\mathbb{R}^n$. Show that
\[
    df = \partial_\mu f \, dx^\mu.
\]

\end{ex}

\begin{sol}\label{sol:gradient}

Recall from solution~\ref{sol:basis} that $\left\{\partial_\mu\right\}$ forms a basis for $\mathbb{R}^n$, so $v = v^\mu \partial_\mu$ for some components $\left\{v^\mu\right\}$, $v \in \text{Vect}(\mathbb{R}^n)$. Consider some test vector $v$,
\[
    df(v) = v(f) = v^\mu \partial_\mu f.
\]
On the other hand,
\begin{align*}
    \partial_\mu f \, dx^\mu (v) &= \partial_\mu f v(x^\mu) \\
    &= v^\nu \partial_\mu f \partial_\nu x^\mu \\
    &= v^\nu \partial_\mu f \delta_\nu^\mu \\
    &= v^\mu \partial_\mu f,
\end{align*}
giving $df(v) = \partial_\mu f\, dx^\mu(v)$ and therefore $df = \partial_\mu f\, dx^\mu$.

\end{sol}

\begin{ex}

Show that the 1-forms $\left\{dx^\mu\right\}$ are linearly independent, i.e., if
\[
    \omega = \omega_\mu dx^\mu = 0
\]
then all the functions $\omega_\mu$ are zero.

\end{ex}

\begin{sol}

As in solution~\ref{sol:gradient}, consider some vector field $v$.
\begin{align*}
    \omega (v) &= \omega_\mu dx^\mu (v) \\
        &= \omega_\mu v(x^\mu) \\
        &= v^\nu \omega_\mu \delta_\nu^\mu \\
        &= v^\mu \omega_\mu
\end{align*}
so $\omega(v) = 0$ implies $v^\mu \omega_\mu = 0$. But since $v$ is arbitrary, $\omega_\mu = 0$ for all $\mu$.

\end{sol}

\subsection{Cotangent Vectors}

\begin{ex}

For the mathematically inclined: show that the $\omega_p$ is really well-defined by the formula above.
That is, show that $\omega(v)(p)$ really depends only on $v_p$, not on the values of $v$ at other points.
Also, show that a 1-form is determined by its values at points.
In other words, if $\omega, \nu$ are two 1-forms on $M$ with $\omega_p = \nu_p$ for every point $p \in M$, then $\omega = \nu$.

\end{ex}

\begin{sol}\label{sol:welldefined1forms}

Let $u, w \in \text{Vect}(M)$ with $u \neq w$. Let $u_p = w_p$, with $u_q \neq w_q$ necessarily, $q \in M, q \neq p$.
Consider the vector field $v = u - w$. Then to show that $\omega_p$ is well-defined, it is sufficient to show that for any $\omega = df$,
\begin{align*}
    \omega_p(v_p) &= \omega(v)(p) \\
        &= df(v)(p) \\
        &= v(f)(p) \\
        &= (u - w)(f)(p) \\
        &= u(f)(p) - w(f)(p) \\
        &= u_p(f) - w_p(f) \\
        &= (u_p - w_p)(f) \\
        &= 0.
\end{align*}
Just as in solution~\ref{sol:tangentvectorpointequality}, if $\omega_p = \nu_p$ for every point $p \in M$ then $\omega_p(v_p) = \nu_p(v_p)$ for some $v_p \in T_p M$. But
\begin{align*}
\omega (v)(p) &= \omega_p(v_p) \\
    &= \nu_p(v_p) \\
    &= \nu (v)(p)
\end{align*}
for all $p \in M$ and therefore, since $v$ is arbitrary, $\omega = \nu$.

\end{sol}

\begin{ex}

Show that the dual of the identity map on a vector space $V$ is the identity map on $V^*$.
Suppose that we have linear maps $f: V \to W$ and $g: W \to X$. Show that ${(gf)}^* = f^* g^*$.

\end{ex}

\begin{sol}

The dual of a linear map $f: V \to W$ is defined by
\[
    (f^*\omega)(v) = \omega(f(v))
\]
where $f^*: W^* \to V^*$.

Let $\text{id}: V \to V$ be the identity map on $V$. For some $v \in V$,
\begin{align*}
    (\text{id}^*\omega)(v) &= \omega(\text{id}(v)) \\
        &= \omega(v)
\end{align*}
giving $\text{id}^*\omega = \omega$, therefore $\text{id}^*: V^* \to V^*$ is the identity map in the dual space.

For the composition $gf = g \circ f$, recall the definition of the \ref{eq:pullbackfunction}. %chktex 2
Let $h: X \to Y$ and consider the pullback
\begin{align*}
    {(g \circ f)}^* h &= h \circ (g \circ f) \\
        &= h \circ g \circ f \\
        &= (h \circ g) \circ f \\
        &= (g^* h) \circ f \\
        &= f^* g^* h,
\end{align*}
giving ${(gf)}^* = f^* g^*$.

We can also pretend that we don't know this is a pullback and use only the definition of the dual space above, by saying
\begin{align*}
    ({(g \circ f)}^*\omega)(v) &= \omega((g \circ f)(v)) \\
        &= (g^* \omega)(f(v)) \\
        &= (f^* g^* \omega)(v).
\end{align*}

\end{sol}

\begin{ex}

Show that the pullback of 1-forms defined by the formula above really exists and is unique.

\end{ex}

\begin{sol}\label{sol:pullback1form}

Let $\phi: M \to N$, $p \mapsto \phi(p) = q$. Then for\footnote{$\omega$ is in $T_q^* N$, see \url{https://math.ucr.edu/home/baez/errata.html}.} $v \in T_p M$, $\omega \in T_q^* N$, the pullback $\phi^*: T_q^* N \to T_p^* M$ of $\omega$ by $\phi$ is defined as
\[
    (\phi^* \omega)(v) = \omega(\phi_* v) \tag{pullback of a 1-form}\label{eq:pullback1form}
\]
and globally we get ${(\phi^* \omega)}_p = \phi^*(\omega_q)$.

To see this, take a test vector $v \in T_p M$ and, similar to solution~\ref{sol:pushforwardvectorfield},
\begin{align*}
    {(\phi^* \omega)}_p v_p &= (\phi^* \omega) (v) (p) \\
        &= \omega(\phi_* v)(q) \\
        &= \omega_q (\phi_* v_q) \\
        &= \phi^* (\omega_q) v_q.
\end{align*}

Let $\phi^*\nu \in T_p^* M$ be some 1-form where ${(\phi^*\omega)}_p = {(\phi^*\nu)}_p$. It follows from solution~\ref{sol:welldefined1forms} that $\omega = \nu$.

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R} \to \mathbb{R}$ be given by $\phi(t) = \sin(t)$. Let $dx$ be the usual 1-form on $\mathbb{R}$. Show that $\phi^*dx = \cos(t) dt$.

\end{ex}

\begin{sol}

Using the fact that the exterior derivative is \emph{natural}, i.e. $\phi^*(df) = d(\phi^*f)$, for some vector $v = f(t)\partial_t$
\begin{align*}
    {(\phi^* dx)}_t v &= d(\phi^* x) (v)(t) \\
        &= v(\phi^* x) (t) \\
        &= v(x \circ \phi)(t) \\
        &= v(\sin(t)) \\
        &= f(t) \partial_t \sin(t) \\
        &= f(t) \cos(t) \\
        &= f(t) \cos(t) \partial_t t \\
        &= \cos(t) v(t) \\
        &= \cos(t) \, dt(v).
\end{align*}

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ denote rotation counterclockwise by the angle $\theta$. Let $dx$, $dy$ be the usual basis of 1-forms on $\mathbb{R}^2$. Show that
\begin{align*}
    \phi^* dx &= \cos(\theta) dx - \sin(\theta) dy, \\
    \phi^* dy &= \sin(\theta) dx + \cos(\theta) dy.
\end{align*}

\end{ex}

\begin{sol}

Let $v = f_i(x, y)\partial_i$ be some vector in $\text{Vect}(\mathbb{R}^2)$ and $p = (x, y) \in \mathbb{R}^2$. For $\phi$ as in solutions~\ref{sol:pullbackrotation},~\ref{sol:pushforwardrotation},
\begin{align*}
    {(\phi^* dx)}_p v &= d(\phi^* x)(v)(p) \\
        &= d(x \circ \phi)(v)(p) \\
        &= v(\cos(\theta)x - \sin(\theta)y) \\
        &= f_1(x, y) \partial_x (\cos(\theta)x - \sin(\theta)y) \\
        &\mathrel{\phantom{=}}{} + f_2(x, y) \partial_y (\cos(\theta)x - \sin(\theta)y) \\
        &= f_1(x, y) \cos(\theta) - f_2(x, y) \sin(\theta) \\
        &= \cos(\theta) f_1(x, y) \partial_x x - \sin(\theta) f_2(x, y) \partial_y y \\
        &= \cos(\theta) v(x) - \sin(\theta) v(y) \\
        &= \cos(\theta) dx(v) - \sin(\theta) dy(v)
\end{align*}
and similarly for $\phi^* dy$.

\end{sol}

\subsection{Change of Coordinates}

\begin{ex}\label{ex:coordinatetransformation1form}

Show that the coordinate 1-forms $dx^\mu$ really are the differentials of the local coordinates $x^\mu$ on $U$.

\end{ex}

\begin{sol}

The statement requires us to be ``working in the chart'', so for now we'll be explicit and denote the local coordinates on $U$ as $\varphi^*x^\mu$. Then the exterior derivative is
\[
    d(\varphi^*x^\mu) = \varphi^* dx^\mu.
\]
To show that this really forms a basis of coordinate 1-forms, consider the basis vectors ``in the chart'', $\varphi_*^{-1}\partial_\mu$.
\begin{align*}
    d(\varphi^* x^\mu)(\varphi_*^{-1}\partial_\nu) &= \varphi_*^{-1}\partial_\nu (\varphi^* x^\mu) \\
        &= \partial_\nu ((\phi^* x^\mu) \circ \varphi^{-1}) \\
        &= \delta_\nu^\mu.
\end{align*}

\end{sol}

\begin{ex}

In the situation above, show that
\[
    dx'^\nu = \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu.
\]
Show that for any 1-form $\omega$ on $\mathbb{R}^n$, writing
\[
    \omega = \omega_\mu dx^\mu = \omega'_\nu dx'^\nu,
\]
your components $\omega'_\nu$ are related to my components $\omega_\mu$ by
\[
    \omega'_\nu = \frac{\partial x^\mu}{\partial x'^\nu} \omega_\mu.
\]

\end{ex}

\begin{sol}

Since 1-forms form a basis, we can write
\[
    dx'^\nu = T_\mu^\nu dx^\mu
\]
for some linear transformation $T_\mu^\nu$. Acting on $\partial_\mu$, we get
\begin{align*}
    dx'^\nu \partial_\mu &= T_\lambda^\nu dx^\lambda \partial_\mu \\
        &= T_\lambda^\nu \delta_\mu^\lambda \\
        &= T_\mu^\nu,
\shortintertext{but}
    dx'^\nu\partial_\mu &= \partial_\mu x'^\nu \\
        &= \frac{\partial x'^\lambda}{\partial x^\mu} \partial'_\lambda x'^\nu \\
        &= \frac{\partial x'^\lambda}{\partial x^\mu} \delta_\lambda^\nu \\
        &= \frac{\partial x'^\nu}{\partial x^\mu}
\end{align*}
so the transformation rule for coordinate 1-forms is
\[
    dx'^\nu = \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu.
\]
We can use this to write any 1-form $\omega$ on $\mathbb{R}^n$ in a different basis, as
\[
    \omega = \omega_\mu dx^\mu = \omega_\mu \frac{\partial x^\mu}{\partial x'^\nu} dx'^\nu.
\]
In this coordinate system, we identify $\omega$'s components as
\[
    \omega'_\nu = \frac{\partial x^\mu}{\partial x'^\nu} \omega_\mu.
\]

\end{sol}

\begin{ex}

Show that
\[
    \phi^*(dx'^\nu) = \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu.
\]

\end{ex}

\begin{sol}

Consider the action on the coordinate vector field $\partial_\lambda$,
\begin{align*}
    \phi^*(dx'^\nu)\partial_\lambda &= d(\phi^*x'^\nu) \partial_\lambda \\
        &= \partial_\lambda(\phi^*x'^\nu) \\
        &\equiv \frac{\partial x'^\nu}{\partial x^\lambda} \tag{``get used to it''}\\
        &= \frac{\partial x'^\nu}{\partial x^\mu} \delta_\lambda^\mu \\
        &= \frac{\partial x'^\nu}{\partial x^\mu} \partial_\lambda x^\mu \\
        &= \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu \partial_\lambda.
\end{align*}
We could instead use the result from exercise~\ref{ex:coordinatetransformation1form}, again acting on the coordinate vector field $\partial_\lambda$,
\begin{align*}
   \phi^*(dx'^\nu)\partial_\lambda &= \phi^*\left(\frac{\partial x'^\nu}{\partial x^\mu} dx^\mu \right) \partial_\lambda \\
    &= \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu (\phi_* \partial_\lambda) \\
    &\equiv \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu \partial_\lambda
\end{align*}
where we are sloppy about the pullback in the last line, as is the convention.

\end{sol}

\begin{ex}\label{ex:changebasisinvertible}

Let
\[
    e_\mu = T_\mu^\nu\partial_\nu
\]
where $\partial_\nu$ are the coordinate vector fields associated to local coordinates on an open set $U$, and $T_\mu^\nu$ are functions on $U$.
Show that the vector fields $e_\mu$ are a basis of vector fields on $U$ if and only if for each $p \in U$ the matrix $T_\mu^\nu(p)$ is invertible.

\end{ex}

\begin{sol}

For $\left\{e_\mu\right\}$ to form a basis, they must be linearly independent and span $U$.

Suppose $T$ is invertible at $p$. Then acting on both sides by $S$ gives us
\begin{align*}
    S_\mu^\lambda e_\lambda &= S_\mu^\lambda T_\lambda^\nu \partial_\nu \\
        &= \delta_\mu^\nu \partial_\nu \\
        &= \partial_\mu.
\end{align*}
Any vector $u \in U$ can therefore be expressed as
\[
    u = u^\mu \partial_\mu = u^\mu S_\mu^\lambda e_\lambda = u'^\mu e_\mu
\]
so $\left\{e_\mu\right\}$ forms a basis for $U$.

Assume $\left\{e_\mu\right\}$ forms a basis for $U$. Then for some smooth functions on $U$, $S_\mu^\nu$,
\begin{align*}
    \partial_\mu &= S_\mu^\nu e_\nu \\
        &= S_\mu^\nu T_\nu^\lambda \partial_\lambda.
\end{align*}
We must identify $S_\mu^\nu T_\nu^\lambda = \delta_\mu^\lambda$, so $T$ is invertible.

\end{sol}

\begin{ex}\label{ex:dualbasisexists}

Use \hyperref[ex:changebasisinvertible]{the previous exercise} to show that the dual basis exists and is unique.

\end{ex}

\begin{sol}

If $\left\{e_\mu\right\}$ is a basis of vector fields on $U$, we automatically get a dual basis of 1-forms $\left\{f^\mu\right\}$ satisfying
\[
    f^\mu(e_\nu) = \delta_\nu^\mu.
\]

We can express
\[
    f^\mu = S_\nu^\mu dx^\nu
\]
for some smooth functions $S_\nu^\mu$ on $U$. Then
\begin{align*}
    f^\mu(e_\nu) &= S_\kappa^\mu dx^\kappa (T_\nu^\lambda \partial_\lambda) \\
        &= S_\kappa^\mu T_\nu^\lambda dx^\kappa \partial_\lambda \\
        &= S_\kappa^\mu T_\nu^\lambda \delta_\lambda^\kappa \\
        &= S_\lambda^\mu T_\nu^\lambda
\end{align*}
so the dual basis exists, since $T$ is invertible (from exercise~\ref{ex:changebasisinvertible}).

Suppose there exists 1-forms $\left\{g^\mu\right\}$ also satisfying $g^\mu(e_\nu) = \delta^\mu_\nu$.
Then for some smooth functions $S'^\mu_\nu$ on $U$, $g^\mu = S'^\mu_\nu dx^\nu$ and, eventually, $S'^\mu_\lambda T^\lambda_\nu = \delta ^\mu_\nu$. But the inverse of $T$ is unique, so $S' = S$ and therefore $g^\mu = f^\mu$.

\end{sol}

\begin{ex}

Let $e_\mu$ be a basis of vector fields on $U$ and let $f^\mu$ be the dual basis of 1-forms. Let
\[
    e'_\mu = T_\mu^\nu e_\nu
\]
be another basis of vector fields and let $f'^\mu$ be the corresponding basis of 1-forms. Show that
\[
    f'^\mu = {(T^{-1})}^\mu_\nu f^\nu.
\]
Show that if $v = v^\mu e_\mu = v'^\mu e'_\mu$, then
\[
    v'^\mu = {(T^{-1})}^\mu_\nu v^\nu
\]
and that if $\omega = \omega_\mu f^\mu = \omega'_\mu f'^\mu$, then
\[
    \omega'_\mu = T_\mu^\nu \omega_\nu.
\]

\end{ex}

\begin{sol}

We know that $f'^\mu = S_\nu^\mu f^\nu$ for some functions $S_\nu^\mu$ on $U$.
Then
\begin{align*}
    f'^\mu(e'_\nu) &= f'^\mu(T^\lambda_\nu e_\lambda) \\
        &= S^\mu_\kappa f^\kappa T^\lambda_\nu e_\lambda \\
        &= S^\mu_\kappa T^\lambda_\nu f^\kappa e_\lambda \\
        &= S^\mu_\kappa T^\lambda_\nu \delta^\kappa\lambda \\
        &= S^\mu_\lambda T^\lambda_\nu.
\end{align*}
But $f'^\mu(e'_\nu) = \delta^\mu_\nu$ from the definition of the dual basis, so $S = T^{-1}$.

If $v = v^\mu e_\mu = v'^\mu e'_\mu$, then $v^\nu e_\nu = v'^\lambda T_\lambda^\nu e_\nu$ and equating coefficients gets us $v^\nu = T_\lambda^\nu v'^\lambda$.
Applying $S = T^{-1}$,
\begin{align*}
    S^\mu_\nu v^\nu &= S^\mu_\nu T_\lambda^\nu v'^\lambda \\
        &= \delta^\mu_\lambda v'^\lambda \\
        &= v'^\mu
\end{align*}
so the components of a vector are contravariant.

If $\omega = \omega_\mu f^\mu = \omega'_\mu f'^\mu$, then $\omega_\nu f^\nu = \omega'_\lambda S^\lambda_\nu f^\nu$ and equating coefficients gets us $\omega_\nu = S_\nu^\lambda \omega'_\lambda$.
Applying $T$,
\begin{align*}
    T_\mu^\nu \omega_\nu &= T_\mu^\nu S_\nu^\lambda \omega'_\lambda \\
        &= \delta^\lambda_\mu \omega'_\lambda \\
        &= \omega'_\mu
\end{align*}
so the components of a 1-form are covariant.

\end{sol}

\subsection{\emph{p}-forms}

\begin{ex}\label{ex:triplewedgeproduct}

Show that
\[
    u \wedge v \wedge w = \det \begin{pmatrix}
        u_x & u_y & u_z \\
        v_x & v_y & v_z \\
        w_x & w_y & w_z
    \end{pmatrix} dx \wedge dy \wedge dz.
\]
Compare this to $\vec{u} \cdot (\vec{v} \times \vec{w})$.

\end{ex}

\begin{sol}

Let $u, v, w$ be vectors,
\begin{align*}
    u &= u_x dx + u_y dy + u_z dz, \\
    v &= v_x dx + v_y dy + v_z dz, \\
    w &= w_x dx + w_y dy + w_z dz.
\end{align*}
Then
\begin{align*}
    v \wedge w =&\ (v_x w_y - v_y w_x) \, dx \wedge dy \\
        &+ (v_y w_z - v_z w_y) \, dy \wedge dz \\
        &+ (v_z w_x - v_x w_z) \, dz \wedge dx,
\end{align*}
so the triple product
\begin{align*}
u \wedge v \wedge w =&\ u_x (v_y w_z - v_z w_y) \, dx \wedge dy \wedge dz \\
    &+ u_y (v_z w_x - v_x w_z) \, dy \wedge dz \wedge dx \\
    &+ u_z (v_x w_y - v_y w_x) \, dz \wedge dx \wedge dy \\
    =&\ u_x (v_y w_z - v_z w_y) \, dx \wedge dy \wedge dz \\
    &- u_y (v_x w_z - v_z w_x) \, dx \wedge dy \wedge dz \\
    &+ u_z (v_x w_y - v_y w_x) \, dx \wedge dy \wedge dz \\
    =&\ \det \begin{pmatrix}
        u_x & u_y & u_z \\
        v_x & v_y & v_z \\
        w_x & w_y & w_z
    \end{pmatrix} dx \wedge dy \wedge dz.
\end{align*}

Consider the traditional vectors $\vec{u}, \vec{v}, \vec{w}$ on $\mathbb{R}^3$. Then
\[
    \vec{v} \times \vec{w} = (v_y w_z - v_z w_y) \vec{\imath}
        - (v_z w_x - v_x w_z) \vec{\jmath}
        + (v_x w_y - v_y w_x) \vec{k},
\]
so the triple product
\begin{align*}
    \vec{u} \cdot (\vec{v} \times \vec{w}) = u_x (v_y w_z - v_z w_y)
        - u_y (v_x w_z - v_z w_x)
        + u_z (v_x w_y - v_y w_x),
\end{align*}
the single component of $u \wedge v \wedge w$.

\end{sol}

\begin{ex}

Show that if $a, b, c, d$ are four vectors in a 3-dimensional space then $a \wedge b \wedge c \wedge d = 0$.

\end{ex}

\begin{sol}

Using $dx, dy, dz$ as a basis, we have from exercise~\ref{ex:triplewedgeproduct} that
\[
    b \wedge c \wedge d = \alpha \, dx \wedge dy \wedge dz, \qquad
    \alpha = \det \begin{pmatrix}
        b_x & b_y & b_z \\
        c_x & c_y & c_z \\
        d_x & d_y & d_z
    \end{pmatrix}.
\]
Then
\begin{align*}
    a \wedge b \wedge c \wedge d =&\ a \wedge \alpha \, dx \wedge dy \wedge dz \\
        =&\ (a_x dx + a_y dy + a_z dz) \wedge \alpha \, dx \wedge dy \wedge dz \\
        =&\ \alpha a_x \, dx \wedge dx \wedge dy \wedge dz \\
        &+ \alpha a_y \, dy \wedge dx \wedge dy \wedge dz \\
        &+ \alpha a_z \, dz \wedge dx \wedge dy \wedge dz \\
        =&\ 0
\end{align*}
since $w \wedge w = 0$ by antisymmetry and each term contains one repeated basis element.

\end{sol}

\begin{ex}

Describe $\Lambda V$ if $V$ is 1-dimensional, 2-dimensional, or 4-dimensional.

\end{ex}

\begin{sol}

Let $u, v \in V$ over a field $\mathbb{F}$.

If $\dim(V) = 1$,
\[
    u = u_x dx, \quad v = v_x dx
\]
so $u \wedge v = 0$ by antisymmetry. Therefore $\Lambda V$ consists of $\mathbb{F}$ and all linear combinations of $dx$ (i.e. $V$).

If $\dim(V) = 2$,
\[
    u = u_x dx + u_y dy, \quad v = v_x dx + v_y dy
\]
so
\begin{align*}
    u \wedge v &= u_x v_y dx \wedge dy + u_y v_x dy \wedge dx \\
        &= (u_x v_y - u_y v_x) \, dx \wedge dy.
\end{align*}
Therefore $\Lambda V$ consists of $\mathbb{F}$, $V$ and all linear combinations of the 2-forms $dx \wedge dy$ above.

If $\dim(V) = 4$ with basis $\left\{dt, dx, dy, dz\right\}$, $\Lambda V$ will consist of $\mathbb{F}$, $V$ and all linear combinations of
\begin{gather*}
    dt \wedge dx, \quad
    dt \wedge dy, \quad
    dt \wedge dz, \quad
    dx \wedge dy, \quad
    dx \wedge dz, \quad
    dy \wedge dz, \\
    dt \wedge dx \wedge dy, \quad
    dt \wedge dx \wedge dz, \quad
    dt \wedge dy \wedge dz, \quad
    dx \wedge dy \wedge dz, \\
    dt \wedge dx \wedge dy \wedge dz.
\end{gather*}

\end{sol}

\begin{ex}

Let $V$ be an $n$-dimensional vector space. Show that $\Lambda^p V$ is empty for $p > n$ and that for $0 \leq p \leq n$ the dimension of $\Lambda^p V$ is $\frac{n!}{p! (n - p)!}$.

\end{ex}

\begin{sol}

Let $\left\{e_1, \ldots, e_n\right\}$ be a basis for $V$.
The subspace $\Lambda^p V$ consists of all linear combinations of the form $e_{i_1} \wedge \cdots \wedge e_{i_p}$.

$\Lambda^n V$ has the single basis element $e_1 \wedge \cdots \wedge e_n$.
The exterior product of any element of $\Lambda^n V$ with any $v \in V$ is necessarily zero since we have exhausted our supply of linearly independent vectors $e_i \in V$. Therefore $\Lambda^p V$ is empty for $p > n$.

The dimension of $\Lambda^p V$ is the number of subsets of size $p$ we can form from the set of $n$ basis vectors of $V$, so
\[
    \dim(\Lambda^p V) = \binom{n}{p} = \frac{n!}{p! (n - p)!}.
\]
This correctly reproduces edge cases such as $\dim(\Lambda^0 V) = \binom{n}{0} = 1$ (for a vector space $V(\mathbb{F})$, this is the underlying field $\mathbb{F}$) and $\dim(\Lambda^{n+1}V) = 0$.

\end{sol}

\begin{ex}

Show that $\Lambda V$ is the direct sum of the subspaces $\Lambda^p V$:
\[
    \Lambda V = \bigoplus \Lambda^p V,
\]
and that the dimension of $\Lambda V$ is $2^n$ if $V$ is $n$-dimensional.
\end{ex}

\begin{sol}

$\Lambda^p V$ is the subspace of $\Lambda V$ consisting of linear combinations of $p$-fold products of vectors in $V$.

For any $q \neq p$, the elements of $\Lambda^q V$ and $\Lambda^p V$ are linearly independent. Therefore for any $w \in \Lambda V$,
$w = w_0 + \cdots + w_n$
where each $w_p \in \Lambda^p V$, so
\begin{align*}
    \Lambda V &= \Lambda^0 V \oplus \cdots \oplus \Lambda^n V \\
        &= \bigoplus_{p=0}^n \Lambda^p V.
\end{align*}
The dimension of $\Lambda V$ is therefore
\begin{align*}
    \dim(\Lambda V) &= \sum_{p = 0}^n \dim(\Lambda^p V) \\
        &= \sum_{p = 0}^n \binom{n}{p} \\
        &= 2^n
\end{align*}
by the binomial theorem.

\end{sol}

\begin{ex}

Given a vector space $V$, show that $\Lambda V$ is a \emph{graded commutative} or \emph{supercommutative} algebra, that is, if $\omega \in \Lambda^p V$ and $\mu \in \Lambda^q V$ then
\[
    \omega \wedge \mu = {(-1)}^{pq} \mu \wedge \omega.
\]
Show that for any manifold $M$, $\Omega(M)$ is graded commutative.

\end{ex}

\begin{sol}

Let $\omega = \omega_1 \wedge \cdots \wedge \omega_p$ and $\mu = \mu_1 \wedge \cdots \wedge \mu_q$.
Then
\begin{align*}
    \omega \wedge \mu &= \omega_1 \wedge \cdots \wedge \omega_p \wedge \mu_1 \wedge \cdots \wedge \mu_q \\
        &= {(-1)}^p \mu_1 \wedge \omega_1 \wedge \cdots \wedge \omega_p \wedge \mu_2 \wedge \cdots \wedge \mu_q \\
        &= {(-1)}^{2p} \mu_1 \wedge \mu_2 \wedge \omega_1 \wedge \cdots \wedge \omega_p \wedge \mu_3 \wedge \cdots \wedge \mu_q \\
        &\vdotswithin{=} \\
        &= {(-1)}^{pq} \mu_1 \wedge \cdots \wedge \mu_q \wedge \omega_1 \wedge \cdots \wedge \omega_p \\
        &= {(-1)}^{pq} \mu \wedge \omega.
\end{align*}

The above result holds analogously for any $\omega \in \Omega^p(M)$ and $\mu \in \Omega^q(M)$. Since $\Omega(M) = \bigoplus \Omega^p(M)$, $\Omega(M)$ is graded commutative over any manifold $M$.

\end{sol}

\begin{ex}

Show that differential forms are contravariant. That is, show that if $\phi: M \to N$ is a map from the manifold $M$ to the manifold $N$, there is a unique pullback map
\[
    \phi^*: \Omega(N) \to \Omega(M)
\]
agreeing with the usual pullback on 0-forms (functions) and 1-forms and satisfying
\begin{align*}
    \phi^* (\alpha \omega) &= \alpha \phi^* \omega \\
    \phi^*(\omega + \mu) &= \phi^* \omega + \phi^* \mu \\
    \phi^*(\omega \wedge \mu) &= \phi^* \omega \wedge \phi^* \mu
\end{align*}
for all $\omega, \mu \in \Omega(N)$ and $\alpha \in \mathbb{R}$.

\end{ex}

\begin{sol}

Since any $\mu \in \Omega(N)$ can be expressed as $\mu = \mu_0 + \cdots + \mu_n$ where each $\mu_p \in \Omega^p(N)$, we can construct a pullback $\phi^*$ satisfying
\begin{align*}
    \phi^* \mu &= \phi^* (\mu_0 + \cdots + \mu_n) \\
        &= \phi^* \mu_0 + \cdots + \phi^* \mu_n
\end{align*}
by linearity and reduce this to considering how $\phi^*$ acts on each $p$-form.

The pullback of a $p$-form $\omega = \omega_1 \wedge \cdots \wedge \omega_p \in \Omega^p(N)$ should generalise the \ref{eq:pullback1form}.  %chktex 2
So on a collection of vectors $v_1, \ldots, v_p \in \text{Vect}(M)$ we would like to get
\begin{align*}
    (\phi^* \omega)(v_1, \ldots, v_p) &= \omega(\phi_* v_1, \ldots, \phi_* v_p) \\
        &=\omega_1 \wedge \cdots \wedge \omega_p (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \phi^* \omega_1 \wedge \cdots \wedge \phi^* \omega_p (v_1, \ldots, v_p).
\end{align*}
which holds since each $\omega_i$ acts on $\phi_* v_i$ independently. Then in terms of components,
\begin{align*}
    \phi^* \omega &= \phi^* (\tfrac{1}{p!}\omega_{i_1,\ldots,i_p}\, e^{i_1} \wedge \cdots \wedge e^{i_p}) \\
        &= \phi^* \tfrac{1}{p!} \omega_{i_1,\ldots,i_p} \phi^* (e^{i_1} \wedge \cdots \wedge e^{i_p}) \\
        &= \tfrac{1}{p!} \phi^* \omega_{i_1,\ldots,i_p} \phi^* e_{i^1} \wedge \cdots \wedge \phi^* e_{i^p}.
\end{align*}

Let $\omega, \mu \in \Omega^p(N)$. Then
\begin{align*}
    \phi^* (\alpha \omega) (v_1, \ldots, v_p) &= \alpha \omega (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \alpha \phi^* \omega (v_1, \ldots, v_p)
\end{align*}
so $\phi^* (\alpha \omega) = \alpha \phi^* \omega$,
\begin{align*}
    \phi^*(\omega + \mu) (v_1, \ldots, v_p) &= (\omega + \mu) (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \omega(\phi_* v_1, \ldots, \phi_* v_p) + \mu(\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \phi^* \omega (v_1, \ldots, v_p) + \phi^* \mu (v_1, \ldots, v_p) \\
        &= (\phi^* \omega + \phi^* \mu) (v_1, \ldots, v_p)
\end{align*}
so $\phi^*(\omega + \mu) = \phi^* \omega + \phi^* \mu$,
\begin{align*}
    \phi^*(\omega \wedge \mu) (v_1, \ldots, v_p) &= (\omega \wedge \mu) (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \phi^*\omega \wedge \phi^*\mu (v_1, \ldots, v_p)
\end{align*}
so $\phi^* (\alpha \omega) = \alpha \phi^* \omega$.

\end{sol}

\begin{ex}

Compare how 1-forms and 2-forms on $\mathbb{R}^3$ transform under \emph{parity}. That is, let $P: \mathbb{R}^3 \to \mathbb{R}^3$ be the map
\[
    P(x, y, z) = (-x, -y, -z),
\]
known as the ``parity transformation''. Note that $P$ maps right-handed bases to left-handed bases and vice versa. Compute $\phi^*(\omega)$ when $\omega$ is the 1-form $\omega_\mu dx^\mu$ and when it is the 2-form $\frac{1}{2}\omega_{\mu\nu}dx^\mu \wedge dx^\nu$.

\end{ex}

\begin{sol}

Assume $\phi^*$ is the pullback by $P$. Consider the pullback of $dx^\mu$ acting on the coordinate vector field $\partial_\nu$,
\begin{align*}
    (\phi^* dx^\mu) \partial_\nu &= d(\phi^* x^\mu) \partial_\nu \\
        &= \partial_\nu (\phi^* x^\mu) \\
        &= \partial_\nu (x^\mu \circ \phi) \\
        &= -\delta_\nu^\mu \\
        &= -\partial_\nu x^\mu \\
        &= - dx^\mu \partial_\nu,
\end{align*}
so $\phi^* dx^\mu = -dx^\mu$.

If $\omega \in \Omega^1(\mathbb{R}^3)$, then
\[
    \phi^* \omega = \phi^*(\omega_\mu dx^\mu) = -\omega
\]
and if $\omega \in \Omega^2(\mathbb{R}^3)$, then
\begin{align*}
    \phi^* \omega &= \phi^*\left(\tfrac{1}{2}\omega_{\mu\nu} dx^\mu \wedge dx^\nu\right) \\
        &= \tfrac{1}{2}\omega_{\mu\nu} \, \phi^*(dx^\mu \wedge dx^\nu) \\
        &= \tfrac{1}{2}\omega_{\mu\nu} \, \phi^* dx^\mu \wedge \phi^* dx^\nu \\
        &= \tfrac{1}{2}\omega_{\mu\nu} (-dx^\mu) \wedge (-dx^\nu) \\
        &= \omega.
\end{align*}

\end{sol}

\subsection{The Exterior Derivative}

\begin{ex}

Show that on $\mathbb{R}^n$ the exterior derivative of any 1-form is given by
\[
    d(\omega_\mu dx^\mu) = \partial_\nu \omega_\mu \, dx^\nu \wedge dx^\mu.
\]

\end{ex}

\begin{sol}

Since $\omega_\mu$ is a 0-form,
\begin{align*}
    d(\omega_\mu dx^\mu) &= d(\omega_\mu \wedge dx^\mu) \\
        &= d\omega_\mu \wedge dx^\mu + \omega_\mu \wedge d(dx^\mu) \\
        &= d\omega_\mu \wedge dx^\mu \\
        &= \partial_\nu \omega_\mu \, dx^\nu \wedge dx^\mu.
\end{align*}

\end{sol}

\section{Rewriting Maxwell's Equations}

\subsection{The First Pair of Equations}

\begin{ex}

Show that any 2-form $F$ on $\mathbb{R} \times S$ can be uniquely expressed as $B + E \wedge dt$ in such a way that for any local coordinates $x^i$ on $S$ we have $E = E_i dx^i$ and $B = \frac{1}{2}B_{ij}dx^i \wedge dx^j$.

\end{ex}

\begin{sol}\label{sol:bplusedt}

Since $\mathbb{R} \times S$ is a manifold, we have an atlas $\left\{\varphi_\alpha\right\}$ for all open sets $U_\alpha$ giving local coordinates $x^\mu = \varphi_\alpha(u)$, $u \in U_\alpha$.

Notice that $\left\{dx^i \wedge dt, dx^i \wedge dx^j\right\}$ spans $\Omega^2(U_\alpha)$.
If $F \in \Omega^2(U_\alpha)$, we can express it as
\begin{align*}
    F &= \frac{1}{2}F_{\mu\nu}dx^\mu \wedge dx^\nu \\
      &= \frac{1}{2} (F_{0i} dt \wedge dx^i + F_{i0} dx^i \wedge dt + F_{ij} dx^i \wedge dx^j) \\
      &= \frac{1}{2} (2F_{i0} dx^i \wedge dt + F_{ij} dx^i \wedge dx^j) \\
      &= \frac{1}{2} F_{ij} dx^i \wedge dx^j + F_{i0} dx^i \wedge dt
\end{align*}
where $F_{0i} = -F_{i0}$ by antisymmetry. Comparing coefficients, we get
\[
    F = B + E \wedge dt
\]
where $F_{ij} = B_{ij}$ and $F_{i0} = E_i$. Uniqueness is automatic since each component is determined by its basis 2-form.

\end{sol}

\begin{ex}

Show that for any form $\omega$ on $\mathbb{R} \times S$ there is a unique way to write $d\omega = dt \wedge \partial_t \omega + d_S \omega$ such that for any local coordinates $x^i$ on $S$, writing $t = x^0$, we have
\begin{align*}
    d_S\omega &= \partial_i \omega_I dx^i \wedge dx^I, \\
    dt \wedge \partial_t \omega &= \partial_0 \omega_I dx^0 \wedge dx^I.
\end{align*}

\end{ex}

\begin{sol}

Similarly to solution~\ref{sol:bplusedt}, since $\omega \in \Omega(U_\alpha)$ we have that $\omega = \omega_I dx^I$, so
\begin{align*}
    d\omega &= \partial_\mu \omega_I dx^\mu \wedge dx^I \\
        &= \partial_0 \omega_I dx^0 \wedge dx^I + \partial_i \omega_I dx^i \wedge dx^I \\
        &= dx^0 \wedge \partial_0 \omega_I \wedge dx^I + \partial_i \omega_I dx^i \wedge dx^I \\
        &= dx^0 \wedge \partial_0 \omega + \partial_i \omega_I dx^i \wedge dx^I \\
        &= dt \wedge \partial_t \omega + d_S \omega.
\end{align*}
Again, this is guaranteed to be unique by linearity.

\end{sol}

\subsection{The Metric}

\begin{ex}

Use the non-degeneracy of the metric to show that the map from $V$ to $V^*$ given by
\[
    v \mapsto g(v, \cdot)
\]
is an isomorphism, that is, one-to-one and onto.

\end{ex}

\begin{sol}\label{sol:metricisomorphism}

Let $v, w \in V$. By bilinearity,
\[
    g(v,\cdot) - g(w,\cdot) = g(v - w, \cdot)
\]
so $g(v,\cdot) - g(w,\cdot) = 0$ implies $v - w = 0$ by non-degeneracy or, equivalently, $g(v,\cdot) = g(w,\cdot)$ implies $v = w$. Therefore the map is injective.

Since the map is injective and, from solution~\ref{sol:gradient}, $\dim(V) = \dim(V^*)$, pick a basis $\left\{e_\mu\right\}$ for $V$ and we get a corresponding basis $\left\{f^\mu\right\}$ for $V^*$.

We claim that we can express any $\omega \in V^*$ as $\omega = g(v,\cdot)$ for some $v \in V$.
\begin{align*}
    \omega &= \omega_\nu f^\nu \\
        &= \omega_\nu g(e_\nu, \cdot) \\
        &= \omega(e_\nu) g(e_\nu, \cdot) \\
        &= g(v, e_\nu) g(e_\nu, \cdot) \\
        &= g(v^\mu e_\mu, e_\nu) g(e_\nu, \cdot) \\
        &= v^\mu g(e_\mu, e_\nu) g(e_\nu, \cdot).
\end{align*}
Because $g$ is non-degenerate, the above is solvable for $v^\mu$ and therefore the map is surjective.

\end{sol}

\begin{ex}\label{ex:loweringindex}

Let $v = v^\mu e_\mu$ be a vector field on a chart. Show that the corresponding 1-form $g(v, \cdot)$ is equal to $v_\nu f^\nu$, where $f^\nu$ is the dual basis of 1-forms and
\[
    v_\nu = g_{\mu\nu}v^\mu.
\]

\end{ex}

\begin{sol}

We'll use the same argument as in solution~\ref{sol:metricisomorphism}. Denote $\omega = g(v, \cdot)$, but since $\omega$ is a 1-form we can express it in components as
\begin{align*}
    \omega &= \omega_\nu f^\nu \\
        &= \omega(e_\nu) f^\nu \\
        &= g(v, e_\nu) f^\nu \\
        &= g(v^\mu e_\mu, e_\nu) f^\nu \\
        &= v^\mu g(e_\mu e_\nu) f^\nu \\
        &= v^\mu g_{\mu\nu} f^\nu \\
        &= g_{\mu\nu} v^\mu f^\nu \\
        &= v_\nu f^\nu
\end{align*}
where we identify $g_{\mu\nu}v^\mu = v_\nu$.

\end{sol}

\begin{ex}

Let $\omega = \omega_\mu f^\mu$ be a 1-form on a chart. Show that the corresponding vector field is equal to $\omega^\nu e_\nu$, where
\[
    \omega^\nu = g^{\mu\nu} \omega_\mu.
\]

\end{ex}

\begin{sol}

Recall that the metric $g$ is symmetric, so $g_{\mu\nu} = g(e_\mu, e_\nu) = g(e_\nu, e_\mu) = g_{\nu\mu}$.
From exercise~\ref{ex:loweringindex} we have that for a vector field $\omega^\mu e_\mu$, the corresponding 1-form is
\[
    \omega = \omega_\mu f^\mu = g_{\mu\nu}\omega^\nu f^\mu.
\]
Applying the inverse $g^{\mu\nu}$ to the components $\omega_\mu = g_{\mu\nu}\omega^\nu$,
\begin{align*}
    g^{\mu\nu} \omega_\mu &= g^{\mu\nu} g_{\mu\nu} \omega^\nu \\
        &= \omega^\nu.
\end{align*}

\end{sol}

\begin{ex}

Let $\eta$ be the Minkowski metric on $\mathbb{R}^4$ as defined above.
Show that its components in the standard basis are
\[
    \eta_{\mu\nu} = \begin{pmatrix}
        -1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1
    \end{pmatrix}.
\]

\end{ex}

\begin{sol}

For $v, w \in \text{Vect}(\mathbb{R}^4)$, the Minkowski metric $\eta$ is defined by
\[
    \eta(v, w) = -v^0 w^0 + v^1 w^1 + v^2 w^2 + v^3 w^3.
\]
Then in an orthonormal basis $\left\{e_\mu\right\}$,
\[
    \eta_{\mu\nu} = \eta(e_\mu, e_\nu) = \begin{dcases*}
            -1            & if $\mu = \nu = 0$, \\ %chktex 1
            \phantom{-} 1 & if $\mu = \nu$, $1 \leq \mu \leq 3$, \\ %chktex 1
            \phantom{-} 0 & otherwise,
        \end{dcases*}
\]
which we can write in matrix form as above.

\end{sol}

\end{document}
