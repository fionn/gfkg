\documentclass[11pt, a4paper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[tracking=true, kerning=true]{microtype}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{inconsolata}
\usepackage{parskip}

\DeclareMicrotypeAlias{lmss}{cmr}

\frenchspacing

\newcommand*{\email}[1]{\normalsize\texttt{\href{mailto:#1}{#1}}}
\newcommand*{\norm}[1]{\ensuremath{\left\Vert#1\right\Vert}}

\theoremstyle{definition}
\newtheorem{ex}{Exercise}[part]
\newtheorem{sol}{Solution}[part]

\title{Gauge Fields, Knots and Gravity Solutions}
\author{Fionn Fitzmaurice}
\date{}

\makeatletter
\hypersetup{pdftitle = \@title,
            pdfauthor = \@author,
            pdfcreator = TeX,
            hidelinks,
            pdfpagemode = UseNone
}
\makeatother

\author{Fionn Fitzmaurice \hspace{20pt} \email{fionn@maths.tcd.ie}}

\begin{document}

\maketitle
\thispagestyle{empty}

\part{Electromagnetism}

\section{Maxwell's Equations}

\begin{ex}

Let $\vec{k}$ be a vector in $\mathbb{R}^3$ and let $\omega = |\vec{k}|$. Fix $\vec{E} \in \mathbb{C}^3$ with $\vec{k} \cdot \vec{E} = 0$ and $i \vec{k} \times \vec{E} = \omega \vec{E}$. Show that
\[
    \vec{\mathcal{E}}(t, \vec{x}) = \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})}
\]
satisfies the vacuum Maxwell equations.

\end{ex}

\begin{sol}

Recall that Maxwell's equations (where $c = 1$) are
\begin{align*}
    \nabla \cdot \vec{B} &= 0, \tag{M1}\\
    \nabla \times \vec{E} + \frac{\partial \vec{B}}{\partial t} &= 0, \tag{M2}\\
    \nabla \cdot \vec{E} &= \rho, \tag{M3} \\
    \nabla \times \vec{B} - \frac{\partial \vec{E}}{\partial t} &= \vec{\jmath}. \tag{M4}
\end{align*}
The vacuum equations are invariant under
\[
    \vec{B} \mapsto \vec{E}, \qquad \vec{E} \mapsto - \vec{B}
\]
(electromagnetic duality) or, equivalently, for a complex-valued vector field $\vec{\mathcal{E}} = \vec{E} + i \vec{B}$,
\[
    \vec{\mathcal{E}} \mapsto i \vec{\mathcal{E}}.
\]
This lets us express the vacuum equations in terms of $\vec{\mathcal{E}}$ as
\[
    \nabla \cdot \vec{\mathcal{E}} = 0, \qquad \nabla \times \vec{\mathcal{E}} = i \frac{\partial \vec{\mathcal{E}}}{\partial t}.
\]

We'll rely on
\[
    \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} = i k_j e^{-i(\omega t - \vec{k} \cdot \vec{x})}
\]
to show that our $\vec{\mathcal{E}}$ satisfies the vacuum equations.

For the divergence,
\begin{align*}
    \nabla \cdot \vec{\mathcal{E}} &= \nabla \cdot \left(\vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})}\right) \\
        &= \sum_{j = 1}^3 \partial_j \left( E_j  e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= \sum_{j = 1}^3 E_j \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \sum_{j = 1}^3 E_j i k_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= i \vec{k} \cdot \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= 0.
\end{align*}

For the curl (dropping the summation and using Einstein notation),
\begin{align*}
    {\left( \nabla \times \vec{\mathcal{E}} \right)}_i &= \epsilon_{ijk} \partial_j \mathcal{E}_k \\
        &= \epsilon_{ijk} \partial_j \left(E_k e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= \epsilon_{ijk} E_k \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \epsilon_{ijk}i k_j E_k e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= {\left(i \vec{k} \times \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right)}_i \\
        &= \omega \vec{E}_i e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \omega \vec{\mathcal{E}}_i,
\end{align*}
so $\nabla \times \vec{\mathcal{E}} = \omega \vec{\mathcal{E}}$.
But
\begin{align*}
    \frac{\partial \vec{\mathcal{E}}}{\partial t} &= \frac{\partial}{\partial t} \left(\vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= -i \omega \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= -i \omega \vec{\mathcal{E}},
\end{align*}
giving
\[
    \nabla \times \vec{\mathcal{E}} = \omega \vec{\mathcal{E}} = i \frac{\partial \vec{\mathcal{E}}}{\partial t}
\]
and satisfying the second vacuum equation.

\end{sol}

\section{Manifolds}

A function $f: X \to Y$ from one topological space to another is defined to be continuous if, given any open set $U \subseteq Y$, the inverse image $f^{-1}(U) \subseteq X$ is open.

\begin{ex}

Show that a function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous according to the above definition if and only if it is continuous according to the epsilon--delta definition: for all $x \in \mathbb{R}^n$ and all $\epsilon > 0$, there exists $\delta > 0$ such that $\norm{y - x} < \delta$ implies $\norm{f(y) - f(x)} < \epsilon$.

\end{ex}

\begin{sol}

Suppose $f$ is continuous according to the epsilon--delta definition of continuity.
Let $V \subseteq \mathbb{R}^m$ be an open set.
For any $x \in f^{-1}(V)$, since $f(x) \in V$ there exists a ball of radius $\epsilon$, $B(f(x), \epsilon) \subseteq V$, centered at $f(x)$.
Then by the epsilon--delta condition there exists a ball of radius $\delta$, $B(x, \delta) \subseteq \mathbb{R}^n$ such that
\[
    f(B(x, \delta)) \subset B(f(x), \epsilon).
\]
Since $x$ was arbitrary, $f^{-1}(V)$ is open as all points sufficiently close to $x$ are also in $f^{-1}(V)$.

Suppose $f$ is continuous according to the topological definition of continuity.
Let $x \in \mathbb{R}^n$ and $\epsilon > 0$.
Consider the open set $f^{-1}(B(f(x), \epsilon)) \subseteq \mathbb{R}^n$.
There exists a $\delta > 0$ such that
\[
    B(x, \delta) \subset f^{-1}(B(f(x), \epsilon)).
\]
Therefore for any point $y \in B(x,\delta)$, $f(y) \in B(f(x), \epsilon)$ or, equivalently, $\norm{y - x} < \delta$ implies $\norm{f(y) - f(x)} < \epsilon$.

\end{sol}

\begin{ex}

Given a topological space $X$ and a subset $S \subseteq X$, define the \emph{induced topology} on $S$ to be the topology in which the open sets are of the form $U \cap S$, where $U$ is open in $X$.

Let $S^n$, the $n$-sphere, be the unit sphere in $\mathbb{R}^{n + 1}$:
\[
    S^n = \biggl\{\vec{x} \in \mathbb{R}^{n + 1} \Bigm| \sum_{i = 1}^{n + 1} {(x^i)}^2 = 1 \biggr\}.
\]
Show that $S^n \subset \mathbb{R}^{n + 1}$ with its induced topology is a manifold.

\end{ex}

\begin{sol}

We need to show that:
\begin{itemize}
    \item the open sets of the induced topology $\{U_\alpha\}$ cover $S^n$,
    \item there exists an atlas of charts $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ for all $\alpha$,
    \item the transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}: \mathbb{R}^n \to \mathbb{R}^n$ are smooth where defined (since we include ``smooth'' in our definition of a manifold).
\end{itemize}

Consider the sets
\[
    U_1 = S^n \setminus \left\{(0, \ldots, 0, 1)\right\}, \qquad
    U_{-1} = S^n \setminus \left\{(0, \ldots, 0, -1)\right\}
\]
which each exclude a single pole. Each $U_\alpha$ is of the form $U \cap S^n$ where $U$ is open in $\mathbb{R}^{n + 1}$.
The induced topology $\left\{U_1, U_{-1}\right\}$ is a cover of $S^n$.

Let $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ be the stereographic projection (for $\alpha \in \{-1, 1\})$.
For some $\vec{p} \in S^n$, $\varphi_\alpha(\vec{p}) \in \mathbb{R}^n$ should be a point on the line that intersects $S^n$ at $\vec{s}_\alpha = (0, \ldots, 0, \alpha)$.
Take a segment of this line parameterised by $t \in [0, 1]$ as
\begin{align*}
    (1 - t) \vec{s}_\alpha + t\vec{p} &= (t p_1, \ldots t p_n, \alpha(1 - t) + tp_{n + 1}) \\
        &= (t p_1, \ldots t p_n, \alpha + t(p_{n + 1} - \alpha)).
\end{align*}
This intersects $\mathbb{R}^n$ when the last coordinate $\alpha + t(p_{n + 1} - \alpha) = 0$, so $t = \frac{1}{1 - \alpha p_{n + 1}}$ and the projection is therefore given by
\[
    \varphi_\alpha: \vec{p} \mapsto \left(\frac{p_1}{1 - \alpha p_{n + 1}}, \ldots, \frac{p_n}{1 - \alpha p_{n + 1}}\right).
\]
Each projection is a chart and the collection of these charts is an atlas, since the union of their domains covers $S^n$.

Denoting $\varphi_\alpha(\vec{p}) = \vec{x}_\alpha = \left(x_\alpha^1, \ldots, x_\alpha^n\right)$, the $L\!^2$-norm
\begin{align*}
    r_\alpha^2 &= \sum_{i = 1}^n {(x_\alpha^i)}^2 \\
               &= \frac{p_1^2 + \cdots + p_n^2}{{(1 - \alpha p_{n + 1})}^2} \\
               &= \frac{1 - p_{n + 1}^2}{{(1 - \alpha p_{n + 1})}^2} \\
               &= \frac{(1 + p_{n + 1})(1 - p_{n + 1})}{{(1 - \alpha p_{n + 1})}^2} \\
               &= {\left(\frac{1 + p_{n + 1}}{1 - p_{n + 1}}\right)}^\alpha,
\end{align*}
so
\[
    p_{n + 1} = \alpha\frac{r_\alpha^2 - 1}{r_\alpha^2 + 1}.
\]
This gives us a general expression for the points $p_1, \ldots, p_n$ on the manifold in terms of our chart's coordinate system as
\begin{align*}
    p_i &= x_\alpha^i (1 - \alpha p_{n + 1}) \\
        &= \frac{2 x_\alpha^i}{r_\alpha^2 + 1},
\end{align*}
so the inverse projections $\varphi_\alpha^{-1}: \mathbb{R}^n \to S^n$ are given by
\[
    \varphi_\alpha^{-1}: \vec{x} \mapsto \left(\frac{2 x^1}{r^2 + 1}, \ldots, \frac{2 x^n}{r^2 + 1}, \alpha \frac{r^2 - 1}{r^2 + 1} \right).
\]
For inverse map $\varphi_\beta^{-1}$, note that the point $p_{n + 1}$ is given by
\[
    p_{n + 1} = \beta \frac{r^2 - 1}{r^2 + 1}.
\]
From this, and assuming $\alpha$, $\beta$ are distinct so $\alpha \beta = -1$, we get that
\[
    \frac{1}{1 - \alpha p_{n + 1}} = \frac{r^2 + 1}{2 r^2}.
\]
The transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}: \mathbb{R}^n \to \mathbb{R}^n$ (with distinct $\alpha$, $\beta$) are then given by
\begin{align*}
    \varphi_\alpha \circ \varphi_\beta^{-1} (\vec{x}) &= \varphi_\alpha \left(\left(\frac{2 x^1}{r^2 + 1}, \ldots, \frac{2 x^n}{r^2 + 1}, \beta \frac{r^2 - 1}{r^2 + 1} \right)\right) \\
        &= \left(\frac{2 x^1}{r^2 + 1} \cdot \frac{r^2 + 1}{2 r^2},
                 \ldots,
                 \frac{2 x^n}{r^2 + 1} \cdot \frac{r^2 + 1}{2 r^2}
            \right) \\
        &= \frac{\vec{x}}{\norm{x}^2}.
\end{align*}
These transition functions are inversions on the $n$-sphere and are smooth where they are defined.

\end{sol}

\begin{ex}

Show that if $M$ is a manifold and $U$ is an open subset of $M$, then $U$ with its induced topology is a manifold.

\end{ex}

\begin{sol}

All subsets $U_\alpha \subset U$ are of the form $V \cap U$ where $V$ is open in $M$, so the open sets of the induced topology cover $U$.

We can construct an atlas by taking the charts on $M$, $\varphi_\alpha: V_\alpha \to \mathbb{R}^n$, and defining
\begin{align*}
    \varphi^U_\alpha&: U_\alpha \to \mathbb{R}^n, \\
                    &: u \mapsto \varphi_\alpha(u),
\end{align*}
i.e. $\varphi^U_\alpha = \varphi_\alpha$ for all $U_\alpha$.
Since $U_\alpha$ is open, we have well defined transition functions so $U$ with the induced topology is a manifold.

\end{sol}

\begin{ex}

Given topological spaces $X$ and $Y$, we give $X \times Y$ the \emph{product topology} in which a set is open if and only if it is a union of sets of the form $U \times V$, where $U$ is open in $X$ and $V$ is open in $Y$. Show that if $M$ is an $m$-dimensional manifold and $N$ is an $n$-dimensional manifold, $M \times N$ is an $(m + n)$-dimensional manifold.

\end{ex}

\begin{sol}

For every point $(u, v) \in M \times N$, there exists a set $U \times V$ where $U$ is open in $M$ and $V$ is open in $N$ such that $u \in U$, $v \in V$.
Therefore $U \times V$ is an open set under the product topology and $M \times N$ is a topological space.

Given $M$, $N$ are manifolds, they have atlases
\[
    \left\{\varphi^M_\alpha : U_\alpha \to \mathbb{R}^m \right\}, \qquad
    \left\{\varphi^N_\beta : V_\beta \to \mathbb{R}^n \right\}
\]
for all $U_\alpha$ open in $M$, $V_\beta$ open in $N$.

For some $u \in U_\alpha$, $v \in V_\beta$, denote
\[
    \varphi^M_\alpha: u \mapsto \vec{x} = (x_1, \ldots, x_m), \quad
    \varphi^N_\beta: v \mapsto \vec{y} = (y_1, \ldots, y_n).
\]

We can construct maps $\tilde{\varphi}_{\alpha\beta}:\ U_\alpha \times V_\beta \to \mathbb{R}^m \times \mathbb{R}^n$ as
\begin{align*}
    \tilde{\varphi}_{\alpha\beta} (u, v) &= \left(\varphi^M_\alpha(u), \varphi^N_\beta(v)\right) \\
        &= (\vec{x}, \vec{y}).
\end{align*}
This is obviously invertible via
\[
    \tilde{\varphi}_{\alpha\beta}^{-1}(\vec{x}, \vec{y}) = \left({(\varphi^M_\alpha)}^{-1}(\vec{x}), {(\varphi^N_\beta)}^{-1}(\vec{y})\right) = (u, v).
\]
because the inverse charts are guaranteed to exist.

The product space $\mathbb{R}^m \times \mathbb{R}^n$ is homeomorphic to $\mathbb{R}^{m + n}$ under
\[
    h(\vec{x}, \vec{y}) = (x_1, \ldots, x_m, y_1, \ldots, y_n),
\]
so we can construct new smooth maps $\varphi_{\alpha\beta} = h \circ \tilde{\varphi}_{\alpha\beta}$ that target $\mathbb{R}^{m + n}$.
The transition functions
\[
    \varphi_{\alpha\beta} \circ \varphi_{\alpha\beta}^{-1}: \mathbb{R}^{m + n} \to \mathbb{R}^{m + n}
\]
are similarly obviously smooth where defined, so $\varphi_{\alpha\beta}$ is a chart and the collection of these charts for all $U_\alpha$, $V_\beta$ is an atlas, therefore $M \times N$ is a manifold.

\end{sol}

\begin{ex}

Given topological spaces $X$ and $Y$, we give $X \cup Y$ the \emph{disjoint union topology} in which a set is open if and only if it is the union of an open subset of $X$ and an open subset of $Y$.
Show that if $M$ and $N$ are $n$-dimensional manifolds the disjoint union $M \cup N$ is an $n$-dimensional manifold.

\end{ex}

\begin{sol}

Any point $p \in M \cup N$ is either in $M$ or $N$. Consider a neighbourhood $X$ of $p$. This will be of the form $U \cup V$ for $U$, $V$ open subsets of $M$, $N$ since $p \in X$ is equivalent to $p \in X \cup \varnothing$.

Given $M$, $N$ are manifolds, they have atlases
\[
    \left\{\varphi^M_\alpha : U_\alpha \to \mathbb{R}^n \right\}, \qquad
    \left\{\varphi^N_\beta : V_\beta \to \mathbb{R}^n \right\}
\]
for all $U_\alpha$ open in $M$, $V_\beta$ open in $N$. Therefore any neighbourhood of $p \in M \cup N$ has a chart, for all $p$.

Since the transition functions exist independently, they are automatically smooth. Therefore $M \cup N$ is an $n$-dimensional manifold.

\end{sol}

\section{Vector Fields}

\begin{ex}

Show that $v + w$ and $gw \in \text{Vect}(M)$.

\end{ex}

\begin{sol}

For the sum,
\begin{align*}
    (v + w)(f + g) &= v(f + g) + w(f + g) \\
        &= v(f) + v(g) + w(f) + w(g) \\
        &= (v + w)(f) + (v + w)(g),
\end{align*}
\begin{align*}
    (v + w)(\alpha f) &= v(\alpha f) + w(\alpha f) \\
        &= \alpha v(f) + \alpha w(f) \\
        &= \alpha (v(f) + w(f)) \\
        &= \alpha (v + w)(f),
\end{align*}
\begin{align*}
    (v + w)(fg) &= v(fg) + w(fg) \\
        &= v(f)g + fv(g) + w(f)g + fw(g) \\
        &= (v(f) + w(f))g + f \cdot (v(g) + w(g)) \\
        &= (v + w)(f)g + f \cdot (v + w)(g).
\end{align*}

For the multiplication,
\begin{align*}
    gw(f + h) &= g \cdot (w(f) + w(h)) \\
        &= gw(f) + gw(h),
\end{align*}
\begin{align*}
    gw(\alpha f) &= g \cdot \alpha w(f) \\
        &= \alpha g w(f),
\end{align*}
\begin{align*}
    gw(fh) &= g \cdot (w(f)h + f w(h)) \\
        &= g w(f) h + g f w(h) \\
        &= gw(f) h + fgw(h).
\end{align*}

\end{sol}

\begin{ex}

Show that the following rules [hold] for all $v, w \in \text{Vect}(M)$ and $f, g \in C^\infty(M)$:
\begin{align*}
    f(v + w) &= fv + fw, \\
    (f + g)v &= fv + gv, \\
    (fg)v &= f(gv), \\
    1v &= v.
\end{align*}
(Here ``$1$'' denotes the constant function equal to $1$ on all of $M$.) Mathematically, we summarize these rules by saying that $\text{Vect}(M)$ is a \emph{module over} $C^\infty(M)$.

\end{ex}

\begin{sol}\label{sol:module}

For all $g \in C^\infty(M)$,
\[
    f(v + w)g = fv(g) + fw(g) = (fv + fw)(g),
\]
so $f(v + w) = fv + fw$.

For all $h \in C^\infty(M)$,
\[
    (f + g)v(h) = fv(h) + gv(h) = (fv + gv)(h),
\]
so $(f + g)v = fv + gv$.


For all $h \in C^\infty(M)$,
\[
    (fg)v(h) = f \cdot gv(h) = f(gv)(h)
\]
so $(fg)v = f(gv)$.

For all $f \in C^\infty(M)$,
\[
    (1v)(f) = 1v(f) = v(f).
\]

Therefore $\text{Vect}(M)$ is a module over $C^\infty(M)$.

\end{sol}

\begin{ex}

Show that if $v^\mu\partial_\mu = 0$, that is, $v^\mu \partial_\mu f = 0$ for all $f \in C^\infty(\mathbb{R}^n)$, we must have $v^\mu = 0$ for all $\mu$.

\end{ex}

\begin{sol}

Choose a function $f: \vec{x} \mapsto x^\nu$ for some index $0 < \nu \leq n$. Then
\[
    v^\mu \partial_\mu x^\nu = v^\mu \delta_\mu^\nu = v^\nu.
\]
If $v^\mu\partial_\mu = 0$, we get $v^\mu$ = 0 from above.

\end{sol}

\subsection{Tangent Vectors}

\begin{ex}

Let $v, w \in \text{Vect}(M)$. Show that $v = w$ if and only if $v_p = w_p$ for all $p \in M$.

\end{ex}

\begin{sol}

Recall the definition $v_p(f) = v(f)(p)$.

If $v = w$, then
\[
    v_p(f) = v(f)(p) = w(f)(p) = w_p(f)
\]
so $v_p = w_p$.

The other way around, if $v_p(f) = w_p(f)$ then $v(f)(p) = w(f)(p)$, which must be true for all $p \in M$, so $v(f) = w(f)$ and therefore $v = w$.

\end{sol}

\begin{ex}

Show that $T_p M$ is a vector space over the real numbers.

\end{ex}

\begin{sol}

We must show that tangent vectors $v_p \in T_p M$ satisfy the axioms of vector spaces.

Let $u, v, w \in T_p M$ and $\alpha, \beta \in \mathbb{R}$.

To check associativity,
\begin{align*}
    (u + (v + w))(f) &= u(f) + (v + w)(f) \\
        &= u(f) + v(f) + w(f) \\
        &= (u(f) + v(f)) + w(f) \\
        &= (u + v)(f) + w(f),
\end{align*}
so $u + (v + w) = (u + v) + w$.

Commutativity holds since $\mathbb{R}$ is commutative.

An additive identity vector $0$ exists since
\[
    (v + 0)(f) = v(f) + 0(f) = v(f)
\]
by defining $0$ to be the tangent vector that maps all functions to $0$.

We can construct for every tangent vector $v$ an additive inverse $-v$ as $(-v)(f) = -v(f)$.

We have compatibility of scalar and field multiplication since
\[
    \alpha(\beta v)(f) = \alpha (\beta v(f)) = \alpha \beta v(f) = (\alpha \beta) v(f).
\]

The existence of a scalar multiplicative identity follows from solution~\ref{sol:module}.

For distributivity,
\[
    \alpha(u + v)(f) = \alpha (u(f) + v(f)) = \alpha u(f) + \alpha v(f)
\]
and
\[
    (\alpha + \beta) v(f) = \alpha v(f) + \beta v(f).
\]

\end{sol}

\begin{ex}

Check that $\gamma'(t) \in T_{\gamma(t)}M$ using the definitions.

\end{ex}

\begin{sol}

We have that
\[
    \gamma'(t): f \mapsto \frac{d}{dt} f\left( \gamma(t) \right).
\]
Notice that
\begin{align*}
    &\gamma'(t)(f + g) = \gamma'(t)(f) + \gamma'(t)(g), \\
    &\gamma'(t)(\alpha f) = \alpha \gamma'(t)(f), \\
    &\gamma'(t)(f g) = \gamma'(t)(f) g + f \gamma'(t)(g),
\end{align*}
so $\gamma'(t)$ is a tangent vector.

\end{sol}

\subsection{Covariant Versus Contravariant}

\begin{ex}

Let $\phi: \mathbb{R} \to \mathbb{R}$ be given by $\phi(t) = e^t$.
Let $x$ be the usual coordinate function on $\mathbb{R}$.
Show that $\phi^* x = e^x$.

\end{ex}

\begin{sol}\label{sol:pullbackexponential}

Consider a chart $\varphi: M \to \mathbb{R}^n$ mapping $p \in M$ to $\varphi(p) = \left\{x^\mu(p)\right\}$.
Note that each $x^\mu$ is a function taking $p$ to the $\mu$\textsuperscript{th} coordinate of its image in $\mathbb{R}^n$.

Since our manifold is $\mathbb{R}$, the ``usual coordinate function'' in this case is the identity (under trivial coordinate transformation $t \to x$, say), so
\[
    (\phi^* x)(t) = x(\phi(t)) = x(e^t) = e^x
\]
(where we abuse notation and identify the coordinate transformation function and its target as $x$).

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ be rotation counterclockwise by an angle $\theta$. Let $x$, $y$ be the usual coordinate functions on $\mathbb{R}^2$. Show that
\begin{align*}
    \phi^* x &= \cos(\theta) x - \sin(\theta) y, \\
    \phi^* y &= \sin(\theta) x + \cos(\theta) y.
\end{align*}

\end{ex}

\begin{sol}\label{sol:pullbackrotation}

If $\phi$ is a positive rotation by a (fixed) angle $\theta$, we can express it as
\[
    \phi: \begin{pmatrix}
            u \\ v
        \end{pmatrix}
        \mapsto
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix}
        \begin{pmatrix}
            u \\ v
        \end{pmatrix}
        =
        \begin{pmatrix}
            \cos(\theta) u - \sin(\theta) v \\
            \sin(\theta) u + \cos(\theta) v
        \end{pmatrix}.
\]
As before, consider the chart $\varphi(p) = \left\{x^\mu(p)\right\} = \left\{x(p), y(p)\right\}$.
Then $\phi^* x(p) = x(\phi(p))$ is the $x$-coordinate, so for $p = (u, v)$,
\begin{align*}
    \phi^* x(p) &= x(\phi(p)) \\
        &= \cos(\theta) u - \sin(\theta) v \\
        &= \cos(\theta) x(p) - \sin(\theta) y(p)
\end{align*}
and similarly for $\phi^* y$.

\end{sol}

\begin{ex}

Show that this definition of smoothness is consistent with the previous definitions of smooth functions $f: M \to \mathbb{R}$ and smooth curves $\gamma: \mathbb{R} \to M$.

\end{ex}

\begin{sol}

Recall the definition of smooth functions between manifolds.

\begin{quote}
$\phi: M \to N$ is smooth if $f \in C^\infty(N)$ implies that $\phi^* f \in C^\infty(M)$.
\end{quote}

Our other two definitions of smooth functions and smooth curves are:
\begin{itemize}
    \item a function $f: M \to \mathbb{R}$ is smooth if for all $\alpha$, $f \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$ is smooth,
    \item a curve $\gamma: \mathbb{R} \to M$ is smooth if $f(\gamma(t))$ depends smoothly on $t$ for any $f \in C^\infty(M)$.
\end{itemize}

If $N = \mathbb{R}$, our definition of smooth functions between manifolds is that $\phi: M \to \mathbb{R}$ is smooth if $f \in C^\infty(\mathbb{R})$ implies that $\phi^* f \in C^\infty(M)$.
But if we assume $f \in C^\infty(\mathbb{R})$ then $\phi^* f = f \circ \phi \in C^\infty(M)$ requires that $\phi \in C^\infty(M)$ and $\phi: M \to \mathbb{R}$ is smooth if for all $\alpha$, $\phi \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$ is smooth.

Let $\phi: M \to \mathbb{R}$ be a smooth function (i.e.\ for all $\alpha$, $\phi \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$).
Let $f \in C^\infty(\mathbb{R})$.
Then $f \circ \phi \circ \varphi_\alpha^{-1}$ is smooth since it is the composition of smooth functions, so $f \circ \phi = \phi^* f$ is smooth.

If the domain is $\mathbb{R}$, our definition of smooth functions between manifolds is that $\gamma: \mathbb{R} \to M$ is smooth if $f \in C^\infty(M)$ implies that $\gamma^* f \in C^\infty(\mathbb{R})$.
But if we assume $f \in C^\infty(M)$ then $\gamma^* f = f \circ \gamma \in C^\infty(\mathbb{R})$ is smooth by the definition of smooth curves.

Let $\gamma: \mathbb{R} \to M$ be smooth, i.e. $f\circ \gamma$ is smooth for all $f \in C^\infty(M)$.
Since $\gamma^* f = f \circ \gamma$, $\gamma^* f$ is smooth too.

\end{sol}

\begin{ex}

Prove that $(\phi \circ \gamma)'(t) = \phi_*(\gamma'(t))$.

\end{ex}

\begin{sol}

The pushforward $\phi_*: T_p M \to T_{\phi(p)} N$ of $v \in T_p M$ by $\phi: M \to N$ is given by $(\phi_* v) (f) = v (\phi^* f)$.
\begin{align*}
    (\phi \circ \gamma)'(t)(f) &= \frac{d}{dt} f\left((\phi \circ \gamma)(t)\right) \\
        &= \frac{d}{dt} (f \circ \phi \circ \gamma)(t) \\
        &= \frac{d}{dt} (f \circ \phi) (\gamma(t)) \\
        &= \gamma'(t)(f \circ \phi) \\
        &= \gamma'(t)(\phi^* f) \\
        &= (\phi_* (\gamma'(t)))(f).
\end{align*}

\end{sol}

\begin{ex}

Show that the pushforward operation
\[
    \phi_*: T_p M \to T_{\phi(p)} N
\]
is linear.

\end{ex}

\begin{sol}

Let $v, w \in T_p M$, $\alpha, \beta \in \mathbb{R}$, $f \in C^\infty(N)$. $\phi_*$ is linear since
\begin{align*}
    \left(\phi_*(\alpha v + \beta w)\right)(f) &= (\alpha v + \beta w) (\phi^* f) \\
        &= \alpha v (\phi^* f) + \beta w (\phi^* f) \\
        &= \alpha (\phi_* v)(f) + \beta (\phi_* w)(f) \\
        &= \left(\alpha (\phi_* v) + \beta (\phi_* w)\right)(f).
\end{align*}

\end{sol}

\begin{ex}

Show that if $\phi: M \to N$ we can push forward a vector field $v$ on $M$ to obtain a vector field $\phi_*v$ on $N$ satisfying
\[
    {(\phi_* v)}_q = \phi_*(v_p)
\]
whenever $\phi(p) = q$.

\end{ex}

\begin{sol}

Note that the definition of the pushforward is sloppy, since the left side must be evaluated on $N$ while the right side is on $M$.

Looking at the action of $\phi_* v$ on a function $f \in C^\infty (N)$ and denoting the points that each side act on as $p \in M$, $q \in N$,
\begin{align*}
    {(\phi_* v)}_q (f) &= (\phi_* v) (f) (q) \\
        &= v(\phi^* f)(p) \\
        &= v_p(\phi^* f) \\
        &= (\phi_* (v_p))(f).
\end{align*}
But
\begin{align*}
    v_p (\phi^* f) &= v_p (f \circ \phi) \\
            &= v(f(\phi(p))) \\
            &= w_{\phi(p)}(f)
\end{align*}
for some $w \in \text{Vect}(N)$.

It's tempting to write this as $v_{\phi(p)}(f)$, but $v \in \text{Vect}(M)$ whereas $\phi(p) \in N$.
Instead we need exactly the pushforward of $v$, so we get $w_{\phi(p)} = {(\phi_* v)}_{\phi(p)}$ and the equality holds when $\phi(p) = q$.

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ be [a] rotation counterclockwise by an angle $\theta$. Let $\partial_x$, $\partial_y$ be the coordinate vector fields on $\mathbb{R}^2$. Show that at any point of $\mathbb{R}^2$,
\begin{align*}
    \phi_* \partial_x = \cos(\theta)\partial_x - \sin(\theta)\partial_y, \\
    \phi_* \partial_y = \sin(\theta)\partial_x + \cos(\theta)\partial_y.
\end{align*}

\end{ex}

\begin{sol}

Denote $\phi: (x, y) \mapsto (u(x, y), v(x, y))$ where $u, v$ are functions as per Solution~\ref{sol:pullbackrotation} and let $f \in C^\infty(\mathbb{R}^2)$.

For a vector $\partial_i$, the pushforward acting on $f$ is
\begin{align*}
    (\phi_* \partial_i)(f) &= \partial_i (\phi^* f) \\
        &= \partial_i (f \circ \phi) \\
        &= \partial_u f \cdot \partial_i u + \partial_v f \cdot \partial_i v
\end{align*}
and at a point $p = (x, y) \in \mathbb{R}^2$,
\[
     (\phi_* \partial_i)(f)(p) = \partial_i u \cdot \partial_u f(u, v) + \partial_i v \cdot \partial_v f(u, v).
\]
We want to consider $f$ at $p$ rather than at $\phi(p)$, so we simply change variables as $\partial_u f(u, v) = \partial_x f(x, y)$, $\partial_v f(u, v) = \partial_y f(x, y)$.

Consider $\phi_* \partial_x$ and $\phi_* \partial_y$,
\begin{align*}
    (\phi_* \partial_x)(f)(p) &= \partial_x u \cdot \partial_x f(x, y) + \partial_x v \cdot \partial_y f(x, y) \\
        &= \cos(\theta) \partial_x f(x, y) + \sin(\theta) \partial_y f(x, y),
\end{align*}
\begin{align*}
    (\phi_* \partial_y)(f)(p) &= \partial_x u \cdot \partial_x f(x, y) + \partial_y v \cdot \partial_y f(x, y) \\
        &= -\sin(\theta) \partial_x f(x, y) + \cos(\theta) \partial_y f(x, y),
\end{align*}
giving us\footnote{Note the sign difference.}
\begin{align*}
    \phi_* \partial_x &= \cos(\theta) \partial_x + \sin(\theta) \partial_y, \\
    \phi_* \partial_y &= -\sin(\theta) \partial_x + \cos(\theta) \partial_y.
\end{align*}

\end{sol}

\subsection{Flows and the Lie Bracket}

\begin{ex}

Let $v$ be the vector field $x^2 \partial_x + y \partial_y$ on $\mathbb{R}^2$. Calculate the integral curves $\gamma(t)$ and see which ones are defined for all $t$.

\end{ex}

\begin{sol}

Integral curves satisfy $\gamma'(t) = v_{\gamma(t)}$, $\gamma(0) = p$.

Denote $\gamma(t) = (x(t), y(t)) \in \mathbb{R}^2$. Then from the definition of tangent curves,
\begin{align*}
    \frac{d}{dt} f(\gamma(t)) &= \frac{d}{dt} f(x, y) \\
    &= \partial_x f(x, y) \dot{x} + \partial_y f(x, y) \dot{y} \\
    &\overset{!}{=} x^2 \partial_x f(x, y) + y \partial_y f(x, y)
\end{align*}
giving us differential equations $\dot{x}(t) = {x(t)}^2$, $\dot{y}(t) = y(t)$ with solutions
\[
    x(t) = \frac{1}{\alpha - t}, \qquad y(t) = \beta e^t.
\]
Fix the constants $\alpha$, $\beta$ with initial condition $\gamma(0) = p = (x(0), y(0))$. Then
\[
    x(t) = \frac{x(0)}{1 - x(0)t}, \qquad y(t) = y(0) e^t.
\]
When $x(0) = 0$ we get $x(t) = 0$ for all $t$. Otherwise, we get a singularity at $t = \frac{1}{x(0)}$, so the integral curves $\gamma$ are defined for all $t$ when starting at $p = (0, b)$ for any $b \in \mathbb{R}$.

\end{sol}

\begin{ex}

Show that $\phi_0$ is the identity map $\text{id}: X \to X$ and that for all $s, t \in \mathbb{R}$ we have $\phi_t \circ \phi_s = \phi_{t + s}$.

\end{ex}

\begin{sol}

By definition, the flow $\phi_t(p)$ is defined to be the point on the integral curve a parameter distance $t$ from $p$, therefore at $t = 0$, $\phi_0(p) = p$.

Pick some value $t = t_0$ and label the point $\phi_{t_0}(p) = q$. Let $t_1 = t_0 + s$, so $\phi_{t_1}(p) = \phi_{t_0 + s}(p)$. But this is a parameter distance $s$ from $q$, so $\phi_{t_1}(p) = \phi_s(q)$ and thus
\[
    \phi_{t_0 + s}(p) = \phi_s \circ \phi_{t_0}(p).
\]
It follows from this that $\phi_s^{-1} = \phi_{-s}$, so the flow is an Abelian group.

\end{sol}

\end{document}
