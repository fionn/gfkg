\documentclass[11pt, a4paper]{article}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[tracking=true, kerning=true]{microtype}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{inconsolata}
\usepackage{parskip}

\DeclareMicrotypeAlias{lmss}{cmr}

\frenchspacing

\mathtoolsset{%
    showonlyrefs,
    showmanualtags
}

\newcommand*{\email}[1]{\normalsize\texttt{\href{mailto:#1}{#1}}}
\newcommand*{\norm}[1]{\ensuremath{\left\Vert#1\right\Vert}}

\DeclarePairedDelimiterX{\innerp}[2]{\langle}{\rangle}{#1, #2}

\theoremstyle{definition}
\newtheorem{ex}{Exercise}[part]
\newtheorem{sol}{Solution}[part]

\title{Gauge Fields, Knots and Gravity Solutions}
\author{Fionn Fitzmaurice}
\date{}

\makeatletter
\hypersetup{pdftitle = \@title,
            pdfauthor = \@author,
            pdfcreator = TeX,
            hidelinks,
            pdfpagemode = UseNone
}
\makeatother

\author{Fionn Fitzmaurice \hspace{20pt} \email{fionn@maths.tcd.ie}}

\begin{document}

\maketitle
\thispagestyle{empty}

\part{Electromagnetism}

\section{Maxwell's Equations}

\begin{ex}

Let $\vec{k}$ be a vector in $\mathbb{R}^3$ and let $\omega = |\vec{k}|$. Fix $\vec{E} \in \mathbb{C}^3$ with $\vec{k} \cdot \vec{E} = 0$ and $i \vec{k} \times \vec{E} = \omega \vec{E}$. Show that
\[
    \vec{\mathcal{E}}(t, \vec{x}) = \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})}
\]
satisfies the vacuum Maxwell equations.

\end{ex}

\begin{sol}

Recall that Maxwell's equations are
\begin{gather*}
    \nabla \cdot \vec{B} = 0, \qquad
    \nabla \times \vec{E} + \frac{\partial \vec{B}}{\partial t} = 0, \\
    \nabla \cdot \vec{E} = \rho, \qquad
    \nabla \times \vec{B} - \frac{\partial \vec{E}}{\partial t} = \vec{\jmath}.
\end{gather*}
The vacuum equations are invariant under
\[
    \vec{B} \mapsto \vec{E}, \qquad
    \vec{E} \mapsto - \vec{B}
\]
(electromagnetic duality) or, equivalently, for a complex-valued vector field $\vec{\mathcal{E}} = \vec{E} + i \vec{B}$,
\[
    \vec{\mathcal{E}} \mapsto i \vec{\mathcal{E}}.
\]
This lets us express the vacuum equations in terms of $\vec{\mathcal{E}}$ as
\[
    \nabla \cdot \vec{\mathcal{E}} = 0, \qquad
    \nabla \times \vec{\mathcal{E}} = i \frac{\partial \vec{\mathcal{E}}}{\partial t}.
\]

We'll rely on
\[
    \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} = i k_j e^{-i(\omega t - \vec{k} \cdot \vec{x})}
\]
to show that our $\vec{\mathcal{E}}$ satisfies the vacuum equations.

For the divergence,
\begin{align*}
    \nabla \cdot \vec{\mathcal{E}} &= \nabla \cdot \left(\vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})}\right) \\
        &= \sum_{j = 1}^3 \partial_j \left( E_j  e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= \sum_{j = 1}^3 E_j \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \sum_{j = 1}^3 E_j i k_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= i \vec{k} \cdot \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= 0.
\end{align*}

For the curl (dropping the summation and using Einstein notation),
\begin{align*}
    {\left( \nabla \times \vec{\mathcal{E}} \right)}_i &= \epsilon_{ijk} \partial_j \mathcal{E}_k \\
        &= \epsilon_{ijk} \partial_j \left(E_k e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= \epsilon_{ijk} E_k \partial_j e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \epsilon_{ijk}i k_j E_k e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= {\left(i \vec{k} \times \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right)}_i \\
        &= \omega E_i e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= \omega \mathcal{E}_i,
\end{align*}
so $\nabla \times \vec{\mathcal{E}} = \omega \vec{\mathcal{E}}$.
But
\begin{align*}
    \frac{\partial \vec{\mathcal{E}}}{\partial t} &= \frac{\partial}{\partial t} \left(\vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \right) \\
        &= -i \omega \vec{E} e^{-i(\omega t - \vec{k} \cdot \vec{x})} \\
        &= -i \omega \vec{\mathcal{E}},
\end{align*}
giving
\[
    \nabla \times \vec{\mathcal{E}} = \omega \vec{\mathcal{E}} = i \frac{\partial \vec{\mathcal{E}}}{\partial t}
\]
and satisfying the second vacuum equation.

\end{sol}

\section{Manifolds}

\begin{ex}

Show that a function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous according to the above definition if and only if it is continuous according to the epsilon--delta definition: for all $x \in \mathbb{R}^n$ and all $\epsilon > 0$, there exists $\delta > 0$ such that $\norm{y - x} < \delta$ implies $\norm{f(y) - f(x)} < \epsilon$.

\end{ex}

\begin{sol}

A function $f: X \to Y$ from one topological space to another is defined to be continuous if, given any open set $U \subseteq Y$, the inverse image $f^{-1}(U) \subseteq X$ is open.

Suppose $f$ is continuous according to the epsilon--delta definition of continuity.
Let $V \subseteq \mathbb{R}^m$ be an open set.
For any $x \in f^{-1}(V)$, since $f(x) \in V$ there exists a ball of radius $\epsilon$, $B(f(x), \epsilon) \subseteq V$, centered at $f(x)$.
Then by the epsilon--delta condition there exists a ball of radius $\delta$, $B(x, \delta) \subseteq \mathbb{R}^n$ such that
\[
    f(B(x, \delta)) \subset B(f(x), \epsilon).
\]
Since $x$ was arbitrary, $f^{-1}(V)$ is open as all points sufficiently close to $x$ are also in $f^{-1}(V)$.

Suppose $f$ is continuous according to the topological definition of continuity.
Let $x \in \mathbb{R}^n$ and $\epsilon > 0$.
Consider the open set $f^{-1}(B(f(x), \epsilon)) \subseteq \mathbb{R}^n$.
There exists a $\delta > 0$ such that
\[
    B(x, \delta) \subset f^{-1}(B(f(x), \epsilon)).
\]
Therefore for any point $y \in B(x,\delta)$, $f(y) \in B(f(x), \epsilon)$ or, equivalently, $\norm{y - x} < \delta$ implies $\norm{f(y) - f(x)} < \epsilon$.

\end{sol}

\begin{ex}

Given a topological space $X$ and a subset $S \subseteq X$, define the \emph{induced topology} on $S$ to be the topology in which the open sets are of the form $U \cap S$, where $U$ is open in $X$.

Let $S^n$, the $n$-sphere, be the unit sphere in $\mathbb{R}^{n + 1}$:
\[
    S^n = \biggl\{\vec{x} \in \mathbb{R}^{n + 1} \Bigm| \sum_{i = 1}^{n + 1} {(x^i)}^2 = 1 \biggr\}.
\]
Show that $S^n \subset \mathbb{R}^{n + 1}$ with its induced topology is a manifold.

\end{ex}

\begin{sol}

We need to show that:
\begin{itemize}
    \item the open sets of the induced topology $\{U_\alpha\}$ cover $S^n$,
    \item there exists an atlas of charts $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ for all $\alpha$,
    \item the transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}: \mathbb{R}^n \to \mathbb{R}^n$ are smooth where defined (since we include ``smooth'' in our definition of a manifold).
\end{itemize}

Consider the sets
\[
    U_1 = S^n \setminus \left\{(0, \ldots, 0, 1)\right\}, \qquad
    U_{-1} = S^n \setminus \left\{(0, \ldots, 0, -1)\right\}
\]
which each exclude a single pole. Each $U_\alpha$ is of the form $U \cap S^n$ where $U$ is open in $\mathbb{R}^{n + 1}$.
The induced topology $\left\{U_1, U_{-1}\right\}$ is a cover of $S^n$.

Let $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$ be the stereographic projection (for $\alpha \in \{-1, 1\})$.
For some $\vec{p} \in S^n$, $\varphi_\alpha(\vec{p}) \in \mathbb{R}^n$ should be a point on the line that intersects $S^n$ at $\vec{s}_\alpha = (0, \ldots, 0, \alpha)$.
Take a segment of this line parameterised by $t \in [0, 1]$ as
\begin{align*}
    (1 - t) \vec{s}_\alpha + t\vec{p} &= (t p_1, \ldots t p_n, \alpha(1 - t) + tp_{n + 1}) \\
        &= (t p_1, \ldots t p_n, \alpha + t(p_{n + 1} - \alpha)).
\end{align*}
This intersects $\mathbb{R}^n$ when the last coordinate $\alpha + t(p_{n + 1} - \alpha) = 0$, so $t = \frac{1}{1 - \alpha p_{n + 1}}$ and the projection is therefore given by
\[
    \varphi_\alpha: \vec{p} \mapsto \left(\frac{p_1}{1 - \alpha p_{n + 1}}, \ldots, \frac{p_n}{1 - \alpha p_{n + 1}}\right).
\]
Each projection is a chart and the collection of these charts is an atlas, since the union of their domains covers $S^n$.

Denoting $\varphi_\alpha: \vec{p} \mapsto \vec{x}_\alpha = \left(x_\alpha^1, \ldots, x_\alpha^n\right)$, the $L\!^2$-norm
\begin{align*}
    r_\alpha^2 &= \sum_{i = 1}^n {(x_\alpha^i)}^2 \\
               &= \frac{p_1^2 + \cdots + p_n^2}{{(1 - \alpha p_{n + 1})}^2} \\
               &= \frac{1 - p_{n + 1}^2}{{(1 - \alpha p_{n + 1})}^2} \\
               &= \frac{(1 + p_{n + 1})(1 - p_{n + 1})}{{(1 - \alpha p_{n + 1})}^2} \\
               &= {\left(\frac{1 + p_{n + 1}}{1 - p_{n + 1}}\right)}^\alpha,
\end{align*}
so
\[
    p_{n + 1} = \alpha\frac{r_\alpha^2 - 1}{r_\alpha^2 + 1}.
\]
This gives us a general expression for the points $\vec{p} = (p_1, \ldots, p_n)$ on the manifold in terms of our chart's coordinate system as
\begin{align*}
    p_i &= x_\alpha^i (1 - \alpha p_{n + 1}) \\
        &= \frac{2 x_\alpha^i}{r_\alpha^2 + 1},
\end{align*}
so the inverse projections $\varphi_\alpha^{-1}: \mathbb{R}^n \to S^n$ are given by
\[
    \varphi_\alpha^{-1}: \vec{x} \mapsto \left(\frac{2 x^1}{r^2 + 1}, \ldots, \frac{2 x^n}{r^2 + 1}, \alpha \frac{r^2 - 1}{r^2 + 1} \right).
\]
For inverse map $\varphi_\beta^{-1}$, note that the point $p_{n + 1}$ is given by
\[
    p_{n + 1} = \beta \frac{r^2 - 1}{r^2 + 1}.
\]
From this, and assuming $\alpha$, $\beta$ are distinct so $\alpha \beta = -1$, we get that
\[
    \frac{1}{1 - \alpha p_{n + 1}} = \frac{r^2 + 1}{2 r^2}.
\]
The transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}: \mathbb{R}^n \to \mathbb{R}^n$ (with distinct $\alpha$, $\beta$) are then given by
\begin{align*}
    \varphi_\alpha \circ \varphi_\beta^{-1} (\vec{x}) &= \varphi_\alpha \left(\left(\frac{2 x^1}{r^2 + 1}, \ldots, \frac{2 x^n}{r^2 + 1}, \beta \frac{r^2 - 1}{r^2 + 1} \right)\right) \\
        &= \left(\frac{2 x^1}{r^2 + 1} \cdot \frac{r^2 + 1}{2 r^2},
                 \ldots,
                 \frac{2 x^n}{r^2 + 1} \cdot \frac{r^2 + 1}{2 r^2}
            \right) \\
        &= \frac{\vec{x}}{\norm{x}^2}.
\end{align*}
These transition functions are inversions on the $n$-sphere and are smooth where they are defined.

\end{sol}

\begin{ex}

Show that if $M$ is a manifold and $U$ is an open subset of $M$, then $U$ with its induced topology is a manifold.

\end{ex}

\begin{sol}

All subsets $U_\alpha \subset U$ are of the form $V \cap U$ where $V$ is open in $M$, so the open sets of the induced topology cover $U$.

We can construct an atlas by taking the charts on $M$, $\varphi_\alpha: V_\alpha \to \mathbb{R}^n$, and defining
\begin{align*}
    \SwapAboveDisplaySkip
    \varphi^U_\alpha&: U_\alpha \to \mathbb{R}^n, \\
                    &: u \mapsto \varphi_\alpha(u),
\end{align*}
i.e. $\varphi^U_\alpha = \varphi_\alpha$ for all $U_\alpha$.
Since $U_\alpha$ is open, we have well defined transition functions so $U$ with the induced topology is a manifold.

\end{sol}

\begin{ex}

Given topological spaces $X$ and $Y$, we give $X \times Y$ the \emph{product topology} in which a set is open if and only if it is a union of sets of the form $U \times V$, where $U$ is open in $X$ and $V$ is open in $Y$. Show that if $M$ is an $m$-dimensional manifold and $N$ is an $n$-dimensional manifold, $M \times N$ is an $(m + n)$-dimensional manifold.

\end{ex}

\begin{sol}

For every point $(u, v) \in M \times N$, there exists a set $U \times V$ where $U$ is open in $M$ and $V$ is open in $N$ such that $u \in U$, $v \in V$.
Therefore $U \times V$ is an open set under the product topology and $M \times N$ is a topological space.

Given $M$, $N$ are manifolds, they have atlases
\[
    \left\{\varphi^M_\alpha: U_\alpha \to \mathbb{R}^m \right\}, \qquad
    \left\{\varphi^N_\beta: V_\beta \to \mathbb{R}^n \right\}
\]
for all $U_\alpha$ open in $M$, $V_\beta$ open in $N$.

For some $u \in U_\alpha$, $v \in V_\beta$, denote
\[
    \varphi^M_\alpha: u \mapsto \vec{x} = (x_1, \ldots, x_m), \quad
    \varphi^N_\beta: v \mapsto \vec{y} = (y_1, \ldots, y_n).
\]

We can construct maps $\tilde{\varphi}_{\alpha\beta}:\ U_\alpha \times V_\beta \to \mathbb{R}^m \times \mathbb{R}^n$ as
\begin{align*}
    \tilde{\varphi}_{\alpha\beta} (u, v) &= \left(\varphi^M_\alpha(u), \varphi^N_\beta(v)\right) \\
        &= (\vec{x}, \vec{y}).
\end{align*}
This is obviously invertible via
\[
    \tilde{\varphi}_{\alpha\beta}^{-1}(\vec{x}, \vec{y}) = \left({(\varphi^M_\alpha)}^{-1}(\vec{x}), {(\varphi^N_\beta)}^{-1}(\vec{y})\right) = (u, v)
\]
because the inverse charts are guaranteed to exist.

The product space $\mathbb{R}^m \times \mathbb{R}^n$ is homeomorphic to $\mathbb{R}^{m + n}$ under
\[
    h(\vec{x}, \vec{y}) = (x_1, \ldots, x_m, y_1, \ldots, y_n),
\]
so we can construct new smooth maps $\varphi_{\alpha\beta} = h \circ \tilde{\varphi}_{\alpha\beta}$ that target $\mathbb{R}^{m + n}$.
The transition functions
\[
    \varphi_{\alpha\beta} \circ \varphi_{\alpha\beta}^{-1}: \mathbb{R}^{m + n} \to \mathbb{R}^{m + n}
\]
are similarly obviously smooth where defined, so $\varphi_{\alpha\beta}$ is a chart and the collection of these charts for all $U_\alpha$, $V_\beta$ is an atlas, therefore $M \times N$ is a manifold.

\end{sol}

\begin{ex}

Given topological spaces $X$ and $Y$, we give $X \cup Y$ the \emph{disjoint union topology} in which a set is open if and only if it is the union of an open subset of $X$ and an open subset of $Y$.
Show that if $M$ and $N$ are $n$-dimensional manifolds the disjoint union $M \cup N$ is an $n$-dimensional manifold.

\end{ex}

\begin{sol}

Any point $p \in M \cup N$ is either in $M$ or $N$. Consider a neighbourhood $X$ of $p$. This will be of the form $U \cup V$ for $U$, $V$ open subsets of $M$, $N$ since $p \in X$ is equivalent to $p \in X \cup \varnothing$.

Given $M$, $N$ are manifolds, they have atlases
\[
    \left\{\varphi^M_\alpha: U_\alpha \to \mathbb{R}^n \right\}, \qquad
    \left\{\varphi^N_\beta: V_\beta \to \mathbb{R}^n \right\}
\]
for all $U_\alpha$ open in $M$, $V_\beta$ open in $N$. Therefore any neighbourhood of $p \in M \cup N$ has a chart, for all $p$.

Since the transition functions exist independently, they are automatically smooth. Therefore $M \cup N$ is an $n$-dimensional manifold.

\end{sol}

\section{Vector Fields}

\begin{ex}

Show that $v + w$ and $gw \in \text{Vect}(M)$.

\end{ex}

\begin{sol}

For the sum,
\begin{align*}
    (v + w)(f + g) &= v(f + g) + w(f + g) \\
                   &= v(f) + v(g) + w(f) + w(g) \\
                   &= (v + w)(f) + (v + w)(g),
\end{align*}
\begin{align*}
    (v + w)(\alpha f) &= v(\alpha f) + w(\alpha f) \\
                      &= \alpha v(f) + \alpha w(f) \\
                      &= \alpha (v(f) + w(f)) \\
                      &= \alpha (v + w)(f),
\end{align*}
\begin{align*}
    (v + w)(fg) &= v(fg) + w(fg) \\
                &= v(f)g + fv(g) + w(f)g + fw(g) \\
                &= (v(f) + w(f))g + f \cdot (v(g) + w(g)) \\
                &= (v + w)(f)g + f \cdot (v + w)(g).
\end{align*}

For the product,
\begin{align*}
    gw(f + h) &= g \cdot (w(f) + w(h)) \\
              &= gw(f) + gw(h),
\end{align*}
\begin{align*}
    gw(\alpha f) &= g \cdot \alpha w(f) \\
        &= \alpha g w(f),
\end{align*}
\begin{align*}
    gw(fh) &= g \cdot (w(f)h + f w(h)) \\
           &= g w(f) h + g f w(h) \\
           &= gw(f) h + fgw(h).
\end{align*}

\end{sol}

\begin{ex}\label{ex:module}

Show that the following rules [hold] for all $v, w \in \text{Vect}(M)$ and $f, g \in C^\infty(M)$:
\begin{align*}
    f(v + w) &= fv + fw, \\
    (f + g)v &= fv + gv, \\
    (fg)v &= f(gv), \\
    1v &= v.
\end{align*}
(Here ``$1$'' denotes the constant function equal to $1$ on all of $M$.) Mathematically, we summarize these rules by saying that $\text{Vect}(M)$ is a \emph{module over} $C^\infty(M)$.

\end{ex}

\begin{sol}\label{sol:module}

For all $g \in C^\infty(M)$,
\[
    f(v + w)g = fv(g) + fw(g) = (fv + fw)(g),
\]
so $f(v + w) = fv + fw$.

For all $h \in C^\infty(M)$,
\[
    (f + g)v(h) = fv(h) + gv(h) = (fv + gv)(h),
\]
so $(f + g)v = fv + gv$.


For all $h \in C^\infty(M)$,
\[
    (fg)v(h) = f \cdot gv(h) = f(gv)(h)
\]
so $(fg)v = f(gv)$.

For all $f \in C^\infty(M)$,
\[
    (1v)(f) = 1v(f) = v(f).
\]

Therefore $\text{Vect}(M)$ is a module over $C^\infty(M)$.

\end{sol}

\begin{ex}

Show that if $v^\mu\partial_\mu = 0$, that is, $v^\mu \partial_\mu f = 0$ for all $f \in C^\infty(\mathbb{R}^n)$, we must have $v^\mu = 0$ for all $\mu$.

\end{ex}

\begin{sol}\label{sol:basis}

Choose a function $f: \vec{x} \mapsto x^\nu$ for some index $0 < \nu \leq n$. Then
\[
    v^\mu \partial_\mu x^\nu = v^\mu \delta_\mu^\nu = v^\nu.
\]
If $v^\mu\partial_\mu = 0$, we get $v^\mu$ = 0 from above.

\end{sol}

\subsection{Tangent Vectors}

\begin{ex}

Let $v, w \in \text{Vect}(M)$. Show that $v = w$ if and only if $v_p = w_p$ for all $p \in M$.

\end{ex}

\begin{sol}\label{sol:tangentvectorpointequality}

If $v = w$, then
\[
    v_p(f) = v(f)(p) = w(f)(p) = w_p(f)
\]
so $v_p = w_p$.

The other way around, if $v_p(f) = w_p(f)$ then $v(f)(p) = w(f)(p)$, which must be true for all $p \in M$, so $v(f) = w(f)$ and therefore $v = w$.

\end{sol}

\begin{ex}

Show that $T_p M$ is a vector space over the real numbers.

\end{ex}

\begin{sol}

We must show that tangent vectors $v_p \in T_p M$ satisfy the axioms of vector spaces.

Let $u, v, w \in T_p M$ and $\alpha, \beta \in \mathbb{R}$.

To check associativity,
\begin{align*}
    (u + (v + w))(f) &= u(f) + (v + w)(f) \\
                     &= u(f) + v(f) + w(f) \\
                     &= (u(f) + v(f)) + w(f) \\
                     &= (u + v)(f) + w(f),
\end{align*}
so $u + (v + w) = (u + v) + w$.

Commutativity holds since $\mathbb{R}$ is commutative.

An additive identity vector $0$ exists since
\[
    (v + 0)(f) = v(f) + 0(f) = v(f)
\]
by defining $0$ to be the tangent vector that maps all functions to $0$.

We can construct for every tangent vector $v$ an additive inverse $-v$ as $(-v)(f) = -v(f)$.

We have compatibility of scalar and field multiplication since
\[
    \alpha(\beta v)(f) = \alpha (\beta v(f))
        = \alpha \beta v(f) = (\alpha \beta) v(f).
\]

The existence of a scalar multiplicative identity follows from solution~\ref{sol:module}.

For distributivity,
\[
    \alpha(u + v)(f) = \alpha (u(f) + v(f)) = \alpha u(f) + \alpha v(f)
\]
and
\[
    (\alpha + \beta) v(f) = \alpha v(f) + \beta v(f).
\]

\end{sol}

\begin{ex}

Check that $\gamma'(t) \in T_{\gamma(t)}M$ using the definitions.

\end{ex}

\begin{sol}

We have that
\[
    \gamma'(t): f \mapsto \frac{d}{dt} f\left( \gamma(t) \right).
\]
Notice that
\begin{align*}
    \SwapAboveDisplaySkip
    &\gamma'(t)(f + g) = \gamma'(t)(f) + \gamma'(t)(g), \\
    &\gamma'(t)(\alpha f) = \alpha \gamma'(t)(f), \\
    &\gamma'(t)(f g) = \gamma'(t)(f) g + f \gamma'(t)(g),
\end{align*}
so $\gamma'(t)$ is a tangent vector.

\end{sol}

\subsection{Covariant Versus Contravariant}

\begin{ex}

Let $\phi: \mathbb{R} \to \mathbb{R}$ be given by $\phi(t) = e^t$.
Let $x$ be the usual coordinate function on $\mathbb{R}$.
Show that $\phi^* x = e^x$.

\end{ex}

\begin{sol}\label{sol:pullbackexponential}

The pullback $\phi^*: C^\infty(N) \to C^\infty(M) $ of $f: N \to \mathbb{R}$ by $\phi: M \to N$ is defined as
\[
    \phi^*f = f \circ \phi. \tag{pullback of a function}\label{eq:pullbackfunction}
\]
Consider a chart $\varphi: M \to \mathbb{R}^n$ mapping $p \in M$ to $\varphi(p) = \left\{x^\mu(p)\right\}$.
Note that each $x^\mu$ is a function taking $p$ to the $\mu$\textsuperscript{th} coordinate of its image in $\mathbb{R}^n$.

Since our manifold is $\mathbb{R}$, the ``usual coordinate function'' in this case is the identity (under trivial coordinate transformation $t \to x$, say), so
\[
    (\phi^* x)(t) = x(\phi(t)) = x(e^t) = e^x
\]
(where we abuse notation and identify the coordinate transformation function and its target as $x$).

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ be rotation counterclockwise by an angle $\theta$. Let $x$, $y$ be the usual coordinate functions on $\mathbb{R}^2$. Show that
\begin{align*}
    \phi^* x &= \cos(\theta) x - \sin(\theta) y, \\
    \phi^* y &= \sin(\theta) x + \cos(\theta) y.
\end{align*}

\end{ex}

\begin{sol}\label{sol:pullbackrotation}

If $\phi$ is a positive rotation by a (fixed) angle $\theta$, we can express it as
\[
    \phi: \begin{pmatrix}
            u \\ v
        \end{pmatrix}
        \mapsto
        \begin{pmatrix*}[r]
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix*}
        \begin{pmatrix}
            u \\ v
        \end{pmatrix}
        =
        \begin{pmatrix}
            \cos(\theta) u - \sin(\theta) v \\
            \sin(\theta) u + \cos(\theta) v
        \end{pmatrix}.
\]
As before, consider the chart $\varphi(p) = \left\{x^\mu(p)\right\} = \left\{x(p), y(p)\right\}$.
Then $\phi^* x(p) = x(\phi(p))$ is the $x$-coordinate, so for $p = (u, v)$,
\begin{align*}
    \phi^* x(p) &= x(\phi(p)) \\
                &= \cos(\theta) u - \sin(\theta) v \\
                &= \cos(\theta) x(p) - \sin(\theta) y(p)
\end{align*}
and similarly for $\phi^* y$.

\end{sol}

\begin{ex}

Show that this definition of smoothness is consistent with the previous definitions of smooth functions $f: M \to \mathbb{R}$ and smooth curves $\gamma: \mathbb{R} \to M$.

\end{ex}

\begin{sol}

Recall the definition of smooth functions between manifolds.

\begin{quote}
    $\phi: M \to N$ is smooth if $f \in C^\infty(N)$ implies that $\phi^* f \in C^\infty(M)$.
\end{quote}

Our other two definitions of smoothness are:
\begin{itemize}
    \item a function $f: M \to \mathbb{R}$ is smooth if for all $\alpha$, $f \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$ is smooth,
    \item a curve $\gamma: \mathbb{R} \to M$ is smooth if $f(\gamma(t))$ depends smoothly on $t$ for any $f \in C^\infty(M)$.
\end{itemize}

If $N = \mathbb{R}$, our definition of smooth functions between manifolds is that $\phi: M \to \mathbb{R}$ is smooth if $f \in C^\infty(\mathbb{R})$ implies that $\phi^* f \in C^\infty(M)$.
But if we assume $f \in C^\infty(\mathbb{R})$ then $\phi^* f = f \circ \phi \in C^\infty(M)$ requires that $\phi \in C^\infty(M)$ and $\phi: M \to \mathbb{R}$ is smooth if for all $\alpha$, $\phi \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$ is smooth.

Let $\phi: M \to \mathbb{R}$ be a smooth function (i.e.\ for all $\alpha$, $\phi \circ \varphi_\alpha^{-1}: \mathbb{R}^n \to \mathbb{R}$).
Let $f \in C^\infty(\mathbb{R})$.
Then $f \circ \phi \circ \varphi_\alpha^{-1}$ is smooth since it is the composition of smooth functions, so $f \circ \phi = \phi^* f$ is smooth.

If the domain is $\mathbb{R}$, our definition of smooth functions between manifolds is that $\gamma: \mathbb{R} \to M$ is smooth if $f \in C^\infty(M)$ implies that $\gamma^* f \in C^\infty(\mathbb{R})$.
But if we assume $f \in C^\infty(M)$ then $\gamma^* f = f \circ \gamma \in C^\infty(\mathbb{R})$ is smooth by the definition of smooth curves.

Let $\gamma: \mathbb{R} \to M$ be smooth, i.e. $f\circ \gamma$ is smooth for all $f \in C^\infty(M)$.
Since $\gamma^* f = f \circ \gamma$, $\gamma^* f$ is smooth too.

\end{sol}

\begin{ex}

Prove that $(\phi \circ \gamma)'(t) = \phi_*(\gamma'(t))$.

\end{ex}

\begin{sol}

The pushforward $\phi_*: T_p M \to T_{\phi(p)} N$ of $v \in T_p M$ by $\phi: M \to N$ is given by
\[
    (\phi_* v) (f) = v (\phi^* f). \tag{pushforward of a vector}\label{eq:pushforwardvector}
\]
Then
\begin{align*}
    \SwapAboveDisplaySkip
    (\phi \circ \gamma)'(t)(f) &= \frac{d}{dt} f\left((\phi \circ \gamma)(t)\right) \\
        &= \frac{d}{dt} (f \circ \phi \circ \gamma)(t) \\
        &= \frac{d}{dt} (f \circ \phi) (\gamma(t)) \\
        &= \gamma'(t)(f \circ \phi) \\
        &= \gamma'(t)(\phi^* f) \\
        &= (\phi_* (\gamma'(t)))(f).
\end{align*}

\end{sol}

\begin{ex}

Show that the pushforward operation
\[
    \phi_*: T_p M \to T_{\phi(p)} N
\]
is linear.

\end{ex}

\begin{sol}

Let $v, w \in T_p M$, $\alpha, \beta \in \mathbb{R}$, $f \in C^\infty(N)$. $\phi_*$ is linear since
\begin{align*}
    \left(\phi_*(\alpha v + \beta w)\right)(f) &= (\alpha v + \beta w) (\phi^* f) \\
        &= \alpha v (\phi^* f) + \beta w (\phi^* f) \\
        &= \alpha (\phi_* v)(f) + \beta (\phi_* w)(f) \\
        &= \left(\alpha (\phi_* v) + \beta (\phi_* w)\right)(f).
\end{align*}

\end{sol}

\begin{ex}

Show that if $\phi: M \to N$ is a diffeomorphism, we can push forward a vector field $v$ on $M$ to obtain a vector field $\phi_*v$ on $N$ satisfying
\[
    {(\phi_* v)}_q = \phi_*(v_p)
\]
whenever $\phi(p) = q$.

\end{ex}

\begin{sol}\label{sol:pushforwardvectorfield}

Note that the definition of the pushforward is sloppy, since the left side must be evaluated on $N$ while the right side is evaluated on $M$.

Looking at the action of $\phi_* v$ on a function $f \in C^\infty (N)$ and denoting the points that each side act on as $p \in M$, $q \in N$,
\begin{align*}
    {(\phi_* v)}_q (f) &= (\phi_* v) (f) (q) \\
                       &= v(\phi^* f)(p) \\
                       &= v_p(\phi^* f) \\
                       &= (\phi_* (v_p))(f).
\end{align*}
But
\begin{align*}
    \SwapAboveDisplaySkip
    v_p (\phi^* f) &= v_p (f \circ \phi) \\
                   &= v(f(\phi(p))) \\
                   &= w_{\phi(p)}(f)
\end{align*}
for some $w \in \text{Vect}(N)$.

It's tempting to write this as $v_{\phi(p)}(f)$, but $v \in \text{Vect}(M)$ whereas $\phi(p) \in N$.
Instead we need exactly the pushforward of $v$, so we get $w_{\phi(p)} = {(\phi_* v)}_{\phi(p)}$ and the equality holds when $\phi(p) = q$.

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ be [a] rotation counterclockwise by an angle $\theta$. Let $\partial_x$, $\partial_y$ be the coordinate vector fields on $\mathbb{R}^2$. Show that at any point of $\mathbb{R}^2$,
\begin{align*}
    \SwapAboveDisplaySkip
    \phi_* \partial_x &= \cos(\theta) \partial_x + \sin(\theta) \partial_y, \\
    \phi_* \partial_y &= -\sin(\theta) \partial_x + \cos(\theta) \partial_y.
\end{align*}

\end{ex}

\begin{sol}\label{sol:pushforwardrotation}

Denote $\phi: (x, y) \mapsto (u(x, y), v(x, y))$ where $u, v$ are functions as per solution~\ref{sol:pullbackrotation} and let $f \in C^\infty(\mathbb{R}^2)$.

For a vector $\partial_i$, the pushforward acting on $f$ is
\begin{align*}
    (\phi_* \partial_i)(f) &= \partial_i (\phi^* f) \\
        &= \partial_i (f \circ \phi) \\
        &= \partial_u f \cdot \partial_i u + \partial_v f \cdot \partial_i v
\end{align*}
and at a point $p = (x, y) \in \mathbb{R}^2$,
\[
     {(\phi_* \partial_i)}_p(f) = \partial_i u \cdot \partial_u f(u, v) + \partial_i v \cdot \partial_v f(u, v).
\]
We want to consider $f$ at $p$ rather than at $\phi(p)$, so change variables as $\partial_u f(u, v) = \partial_x f(x, y)$, $\partial_v f(u, v) = \partial_y f(x, y)$.

Consider $\phi_* \partial_x$ and $\phi_* \partial_y$,
\begin{align*}
    {(\phi_* \partial_x)}_p(f) &= \partial_x u \cdot \partial_x f(x, y) + \partial_x v \cdot \partial_y f(x, y) \\
        &= \cos(\theta) \partial_x f(x, y) + \sin(\theta) \partial_y f(x, y), \\[2\jot]
    {(\phi_* \partial_y)}_p(f) &= \partial_x u \cdot \partial_x f(x, y) + \partial_y v \cdot \partial_y f(x, y) \\
        &= -\sin(\theta) \partial_x f(x, y) + \cos(\theta) \partial_y f(x, y),
\end{align*}
giving us
\begin{align*}
    \SwapAboveDisplaySkip
    \phi_* \partial_x &= \cos(\theta) \partial_x + \sin(\theta) \partial_y, \\
    \phi_* \partial_y &= -\sin(\theta) \partial_x + \cos(\theta) \partial_y.
\end{align*}
We can see that this is consistent by taking the result from solution~\ref{sol:pullbackrotation},
\begin{align*}
    (\phi_* \partial_x) x &= \partial_x(\phi^* x) = \cos(\theta), \\
    (\phi_* \partial_x) y &= \partial_x(\phi^* y) = \sin(\theta), \\
    (\phi_* \partial_y) x &= \partial_y(\phi^* x) = -\sin(\theta), \\
    (\phi_* \partial_y) y &= \partial_y(\phi^* y) = \cos(\theta),
\end{align*}
where we get back the $x$- and $y$-components of $\phi_* \partial_x$, $\phi_* \partial_y$, respectively.

\end{sol}

\subsection{Flows and the Lie Bracket}

\begin{ex}

Let $v$ be the vector field $x^2 \partial_x + y \partial_y$ on $\mathbb{R}^2$. Calculate the integral curves $\gamma(t)$ and see which ones are defined for all $t$.

\end{ex}

\begin{sol}

Integral curves satisfy $\gamma'(t) = v_{\gamma(t)}$, $\gamma(0) = p$.

Denote $\gamma(t) = (x(t), y(t)) \in \mathbb{R}^2$. Then from the definition of tangent curves,
\begin{align*}
    \frac{d}{dt} f(\gamma(t)) &= \frac{d}{dt} f(x, y) \\
        &= \partial_x f(x, y) \dot{x} + \partial_y f(x, y) \dot{y} \\
        &\overset{!}{=} x^2 \partial_x f(x, y) + y \partial_y f(x, y)
\end{align*}
giving us differential equations $\dot{x}(t) = {x(t)}^2$, $\dot{y}(t) = y(t)$ with solutions
\[
    x(t) = \frac{1}{\alpha - t}, \qquad
    y(t) = \beta e^t.
\]
Fix the constants $\alpha$, $\beta$ with initial condition $\gamma(0) = p = (x(0), y(0))$. Then
\[
    x(t) = \frac{x(0)}{1 - x(0)t}, \qquad
    y(t) = y(0) e^t.
\]
When $x(0) = 0$ we get $x(t) = 0$ for all $t$. Otherwise, we get a singularity at $t = \frac{1}{x(0)}$, so the integral curves $\gamma$ are defined for all $t$ when starting at $p = (0, b)$ for any $b \in \mathbb{R}$.

\end{sol}

\begin{ex}

Show that $\phi_0$ is the identity map $\text{id}: X \to X$ and that for all $s, t \in \mathbb{R}$ we have $\phi_t \circ \phi_s = \phi_{t + s}$.

\end{ex}

\begin{sol}

By definition, the flow $\phi_t(p)$ is defined to be the point on the integral curve a parameter distance $t$ from $p$, therefore at $t = 0$, $\phi_0(p) = p$.

Pick some value $t = t_0$ and label the point $\phi_{t_0}(p) = q$. Let $t_1 = t_0 + s$, so $\phi_{t_1}(p) = \phi_{t_0 + s}(p)$. But this is a parameter distance $s$ from $q$, so $\phi_{t_1}(p) = \phi_s(q)$ and thus
\[
    \phi_{t_0 + s}(p) = \phi_s \circ \phi_{t_0}(p).
\]
It follows from this that $\phi_s^{-1} = \phi_{-s}$, so the flow is an Abelian group.

\end{sol}

\begin{ex}

Consider the normalised vector fields in the $r$ and $\theta$ directions on the plane in polar coordinates (not defined at the origin):
\[
    v = \frac{x \partial_x + y \partial_y}{\sqrt{x^2 + y^2}}, \qquad
    w = \frac{x \partial_y - y \partial_x}{\sqrt{x^2 + y^2}}.
\]
Calculate $[v, w]$.

\end{ex}

\begin{sol}

Since $x = r \cos(\theta)$, $y = r \sin(\theta)$, we have for some $f \in C^\infty(\mathbb{R}^2)$,
\begin{align*}
    \SwapAboveDisplaySkip
    \partial_r f &=\cos(\theta) \partial_x f + \sin(\theta) \partial_y f \\
    \partial_\theta f &= -r \sin(\theta) \partial_x f + r \cos(\theta) \partial_y f
\end{align*}
so $v = \partial_r$, $w = \frac{\partial_\theta}{r}$.
Then
\begin{align*}
    [v, w]f &= v(w(f)) - w(v(f)) \\
        &= v\left(\frac{\partial_\theta f}{r}\right) - w(\partial_r f) \\
        &= \partial_r\left(\frac{\partial_\theta f}{r}\right)
            - \frac{\partial_\theta}{r}(\partial_r f) \\
        &= \frac{r \partial_r \partial_\theta f - \partial_\theta f}{r^2}
            - \frac{\partial_\theta \partial_r f}{r} \\
        &= \frac{1}{r} \left(\partial_r \partial_\theta f - \frac{\partial_\theta f}{r}
            - \partial_\theta \partial_r f\right) \\
        &= -\frac{\partial_\theta f}{r^2} \\
        &= -\frac{w}{r} f
\end{align*}
so $[v, w] = -\frac{w}{r}$.

We could also do this the hard way,
\begin{align*}
    [v, w]f &= v(w(f)) - w(v(f)) \\
        &= \frac{(x \partial_x + y \partial_y)(x \partial_y f - y \partial_x f) - (x \partial_y - y \partial_x)(x \partial_x f + y \partial_y f)}{x^2 + y^2} \\
        &= \frac{\splitdfrac{x \partial_x (x\partial_y f)
                             - x\partial_x(y \partial_x f)
                             + y \partial_y(x\partial_y f)
                             - y \partial_y(y \partial_x f)}
                            {{}-x\partial_y(x\partial_x f)
                             - x\partial_y(y\partial_y f)
                             + y\partial_x(x \partial_x f)
                             + y\partial_x (y \partial_y f)}
                }{x^2 + y^2} \\
        &= \frac{y \partial_x f - x \partial_y f}{x^2 + y^2}
\end{align*}
giving the same result
\[
    [v, w] = \frac{y \partial_x - x \partial_y}{x^2 + y^2} = -\frac{w}{r}.
\]

\end{sol}

\begin{ex}

Check the equation above.

\end{ex}

\begin{sol}

We need to check that for any $f \in C^\infty(M)$,
\[
    [v, w](f)(p) = \frac{\partial^2}{\partial t\, \partial s} \Bigl( f(\psi_s(\phi_t(p))) - f(\phi_t(\psi_s(p))) \Bigr) \Big|_{s = t = 0}
\]
where $\phi_t$, $\psi_s$ are flows generated by $v$ and $w$, respectively.

We have that
\[
    (vf)(p) = \frac{d}{dt} f(\phi_t(p)) \Big|_{t=0}, \qquad
    (wf)(p) = \frac{d}{ds} f(\psi_s(p)) \Big|_{s=0},
\]
so
\begin{align*}
    \SwapAboveDisplaySkip
    (vw)(f)(p) &= \frac{d}{dt} wf(\phi_t(p)) \Big|_{t = 0} \\
        &= \frac{\partial^2}{\partial t \, \partial s} f(\psi_s(\phi_t(p))) \Big|_{s = t = 0}
\intertext{and similarly}
    (wv)(f)(p) &= \frac{d}{ds} vf(\psi_s(p)) \Big|_{s = 0} \\
        &= \frac{\partial^2}{\partial s \, \partial t} f(\phi_t(\psi_s(p))) \Big|_{t = s = 0}.
\end{align*}
The result follows immediately.

\end{sol}

\begin{ex}

Show that for all vector fields $u$, $v$, $w$ on a manifold, and all real numbers $\alpha$, $\beta$, we have:
\begin{enumerate}
    \item $[v, w] = - [w, v]$,
    \item $[u, \alpha v + \beta w] = \alpha[u, v] + \beta [u, w]$,
    \item the \emph{Jacobi identity}: $\big[u, [v, w]\big] + \big[v, [w, u]\big] + \big[w, [u, v]\big] = 0$.
\end{enumerate}

\end{ex}

\begin{sol}
\mbox{}

\begin{enumerate}

\item The Lie bracket is antisymmetric.
    \[
        [v, w] = vw - wv = -(wv - vw) = -[w, v].
    \]

\item The Lie bracket is linear.
    \begin{align*}
        [u, \alpha v + \beta w] &= u (\alpha v + \beta w) - (\alpha v + \beta w) u \\
            &= \alpha uv + \beta uw - \alpha vu - \beta wu \\
            &= \alpha(uv - vu) + \beta(uw - wu) \\
            &= \alpha [u, v] + \beta [u, w].
    \end{align*}

\item The Lie bracket satisfies the Jacobi identity.
    \begin{align*}
        \big[u, [v, w]\big] &= u[v, w] - [v, w]u \\
            &= u(vw - wv) - (vw - wv)u \\
            &= uvw - uwv - vwu + wvu,
    \end{align*}
    so similarly,
    \begin{align*}
        \big[v, [w, u]\big] &= vwu - vuw - wuv + uwv, \\
        \big[w, [u, v]\big] &= wuv - wvu - uvw + vuw.
    \end{align*}
    Combining everything, we get
    \begin{align*}
        \big[u, [v, w]\big] + \big[v, [w, u]\big] + \big[w, [u, v]\big] =&\ uvw - uwv - vwu + wvu \\
            &+ vwu - vuw - wuv + uwv \\
            &+ wuv - wvu - uvw + vuw \\
            =&\ 0.
    \end{align*}
\end{enumerate}

\end{sol}

\section{Differential Forms}

\subsection{1-forms}

\begin{ex}

Show that $\omega + \mu$ and $f\omega$ are really 1-forms, i.e., show linearity over $C^\infty(M)$.

\end{ex}

\begin{sol}

Let $g, h \in C^\infty(M)$, $v, w \in \text{Vect}(M)$.

$\omega + \mu$ is linear over $C^\infty(M)$ since
\begin{align*}
    (\omega + \mu)(gv + hw) &= (\omega + \mu)(gv) + (\omega + \mu)(hw) \\
        &= \omega(gv) + \mu(gv) + \omega(hw) + \mu(hw) \\
        &= g\omega(v) + g\mu(v) + h\omega(w) + h\mu(w) \\
        &= g(\omega + \mu)(v)  + h(\omega + \mu)(w)
\end{align*}
and $f\omega$ is linear over $C^\infty(M)$ since
\begin{align*}
    (f\omega)(gv + hw) &= f\omega(gv + hw) \\
                       &= fg\omega(v) + fh\omega(w) \\
                       &= gf\omega(v) + hf\omega(w) \\
                       &= g(f\omega)(v) + h(f\omega)(w).
\end{align*}

\end{sol}

\begin{ex}

Show that $\Omega^1(M)$ is a module over $C^\infty(M)$ (see the definition in exercise~\ref{ex:module}).

\end{ex}

\begin{sol}

Let $\omega, \mu \in \Omega^1(M)$, $v\in \text{Vect}(M)$.

For all $f \in C^\infty(M)$,
\[
    f(\omega + \mu)(v) = f(\omega v + \mu v) = f\omega v + f\mu v
\]
so $f(\omega + \mu) = f\omega + f\mu$.

For all $f, g \in C^\infty(M)$,
\[
    (f + g)\omega(v) = f\omega(v) + g\omega(v)
\]
so $(f + g)\omega = f\omega + g\omega$.

For all $f, g \in C^\infty(M)$,
\[
    (fg)\omega(v) = f(g\omega)(v) = (fg\omega)(v)
\]
so $(fg)\omega = fg\omega$.

Let $1$ be the constant function equal to $1$ on all of $M$. Then
\[
    (1\omega)(v) = 1\omega(v) = \omega(v).
\]

Therefore $\Omega^1(M)$ is a module over $C^\infty(M)$.

\end{sol}

\begin{ex}

Show that
\begin{align*}
    & d(f + g) = df + dg, \\
    & d(\alpha f) = \alpha\, df, \\
    & (f + g) dh = f\, dh + g\, dh, \\
    & d(fg) = f \, dg + g\, df
\end{align*}
for any $f, g, h \in C^\infty(M)$ and any $\alpha \in \mathbb{R}$.

\end{ex}

\begin{sol}

Let $v \in \text{Vect}(M)$. First consider linearity.
\begin{align*}
    d(f + g)v &= v(f + g) \\
              &= vf + vg \\
              &= df(v) + dg(v) \\
              &= (df + dg)(v),
\end{align*}
\[
    d(\alpha f)(v) = v(\alpha f) = \alpha v(f) = \alpha \, df(v),
\]
\begin{align*}
    (f + g)dh(v) &= (f + g)v(h) \\
                 &= fv(h) + gv(h) \\
                 &= f\, dh(v) + g \, dh(v).
\end{align*}
The Leibniz law holds since
\begin{align*}
    d(fg)(v) &= v(fg) \\
             &= f v(g) + g v(f) \\
             &= f \, dg(v) + g \, df(v).
\end{align*}

\end{sol}

\begin{ex}

Suppose $f(x^1, \ldots, x^n)$ is a function on $\mathbb{R}^n$. Show that
\[
    df = \partial_\mu f \, dx^\mu.
\]

\end{ex}

\begin{sol}\label{sol:gradient}

Recall from solution~\ref{sol:basis} that $\left\{\partial_\mu\right\}$ forms a basis for $\mathbb{R}^n$, so $v = v^\mu \partial_\mu$ for some components $\left\{v^\mu\right\}$, $v \in \text{Vect}(\mathbb{R}^n)$. Consider some test vector $v$,
\[
    df(v) = v(f) = v^\mu \partial_\mu f.
\]
On the other hand,
\begin{align*}
    \partial_\mu f \, dx^\mu (v) &= \partial_\mu f v(x^\mu) \\
        &= v^\nu \partial_\mu f \partial_\nu x^\mu \\
        &= v^\nu \partial_\mu f \delta_\nu^\mu \\
        &= v^\mu \partial_\mu f,
\end{align*}
giving $df(v) = \partial_\mu f\, dx^\mu(v)$ and therefore $df = \partial_\mu f\, dx^\mu$.

\end{sol}

\begin{ex}

Show that the 1-forms $\left\{dx^\mu\right\}$ are linearly independent, i.e., if
\[
    \omega = \omega_\mu dx^\mu = 0
\]
then all the functions $\omega_\mu$ are zero.

\end{ex}

\begin{sol}

As in solution~\ref{sol:gradient}, consider some vector field $v$.
\begin{align*}
    \omega (v) &= \omega_\mu dx^\mu (v) \\
               &= \omega_\mu v(x^\mu) \\
               &= v^\nu \omega_\mu \delta_\nu^\mu \\
               &= v^\mu \omega_\mu
\end{align*}
so $\omega(v) = 0$ implies $v^\mu \omega_\mu = 0$. But since $v$ is arbitrary, $\omega_\mu = 0$ for all $\mu$.

\end{sol}

\subsection{Cotangent Vectors}

\begin{ex}

For the mathematically inclined: show that the $\omega_p$ is really well-defined by the formula above.
That is, show that $\omega(v)(p)$ really depends only on $v_p$, not on the values of $v$ at other points.
Also, show that a 1-form is determined by its values at points.
In other words, if $\omega, \nu$ are two 1-forms on $M$ with $\omega_p = \nu_p$ for every point $p \in M$, then $\omega = \nu$.

\end{ex}

\begin{sol}\label{sol:welldefined1forms}

Let $u, w \in \text{Vect}(M)$ with $u \neq w$. Let $u_p = w_p$, with $u_q \neq w_q$ necessarily, $q \in M, q \neq p$.
Consider the vector field $v = u - w$. Then to show that $\omega_p$ is well-defined, it is sufficient to show that for any $\omega = df$,
\begin{align*}
    \omega_p(v_p) &= \omega(v)(p) \\
                  &= df(v)(p) \\
                  &= v(f)(p) \\
                  &= (u - w)(f)(p) \\
                  &= u(f)(p) - w(f)(p) \\
                  &= u_p(f) - w_p(f) \\
                  &= (u_p - w_p)(f) \\
                  &= 0.
\end{align*}
Just as in solution~\ref{sol:tangentvectorpointequality}, if $\omega_p = \nu_p$ for every point $p \in M$ then $\omega_p(v_p) = \nu_p(v_p)$ for some $v_p \in T_p M$. But
\begin{align*}
\omega (v)(p) &= \omega_p(v_p) \\
              &= \nu_p(v_p) \\
              &= \nu (v)(p)
\end{align*}
for all $p \in M$ and therefore, since $v$ is arbitrary, $\omega = \nu$.

\end{sol}

\begin{ex}

Show that the dual of the identity map on a vector space $V$ is the identity map on $V^*$.
Suppose that we have linear maps $f: V \to W$ and $g: W \to X$. Show that ${(gf)}^* = f^* g^*$.

\end{ex}

\begin{sol}

The dual of a linear map $f: V \to W$ is defined by
\[
    (f^*\omega)(v) = \omega(f(v))
\]
where $f^*: W^* \to V^*$.

Let $\text{id}: V \to V$ be the identity map on $V$. For some $v \in V$,
\begin{align*}
    (\text{id}^*\omega)(v) &= \omega(\text{id}(v)) \\
                           &= \omega(v)
\end{align*}
giving $\text{id}^*\omega = \omega$, therefore $\text{id}^*: V^* \to V^*$ is the identity map in the dual space.

For the composition $gf = g \circ f$, recall the definition of the \ref{eq:pullbackfunction}. %chktex 2
Let $h: X \to Y$ and consider the pullback
\begin{align*}
    {(g \circ f)}^* h &= h \circ (g \circ f) \\
                      &= h \circ g \circ f \\
                      &= (h \circ g) \circ f \\
                      &= (g^* h) \circ f \\
                      &= f^* g^* h,
\end{align*}
giving ${(gf)}^* = f^* g^*$.

We can also pretend that we don't know this is a pullback and use only the definition of the dual space above, by saying
\begin{align*}
    ({(g \circ f)}^*\omega)(v) &= \omega((g \circ f)(v)) \\
        &= (g^* \omega)(f(v)) \\
        &= (f^* g^* \omega)(v).
\end{align*}

\end{sol}

\begin{ex}

Show that the pullback of 1-forms defined by the formula above really exists and is unique.

\end{ex}

\begin{sol}\label{sol:pullback1form}

Let $\phi: M \to N$, $p \mapsto \phi(p) = q$. Then for\footnote{$\omega$ is in $T_q^* N$, see \url{https://math.ucr.edu/home/baez/errata.html}.} $v \in T_p M$, $\omega \in T_q^* N$, the pullback $\phi^*: T_q^* N \to T_p^* M$ of $\omega$ by $\phi$ is defined as
\[
    (\phi^* \omega)(v) = \omega(\phi_* v) \tag{pullback of a 1-form}\label{eq:pullback1form}
\]
and globally we get ${(\phi^* \omega)}_p = \phi^*(\omega_q)$.

To see this, take a test vector $v \in T_p M$ and, similar to solution~\ref{sol:pushforwardvectorfield},
\begin{align*}
    {(\phi^* \omega)}_p v_p &= (\phi^* \omega) (v) (p) \\
        &= \omega(\phi_* v)(q) \\
        &= \omega_q (\phi_* v_q) \\
        &= \phi^* (\omega_q) v_q.
\end{align*}

Let $\phi^*\nu \in T_p^* M$ be some 1-form where ${(\phi^*\omega)}_p = {(\phi^*\nu)}_p$. It follows from solution~\ref{sol:welldefined1forms} that $\omega = \nu$.

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R} \to \mathbb{R}$ be given by $\phi(t) = \sin(t)$. Let $dx$ be the usual 1-form on $\mathbb{R}$. Show that $\phi^*dx = \cos(t) dt$.

\end{ex}

\begin{sol}

Using the fact that the exterior derivative is \emph{natural}, i.e. $\phi^*(df) = d(\phi^*f)$, for some vector $v = f(t)\partial_t$
\begin{align*}
    {(\phi^* dx)}_t v &= d(\phi^* x) (v)(t) \\
        &= v(\phi^* x) (t) \\
        &= v(x \circ \phi)(t) \\
        &= v(\sin(t)) \\
        &= f(t) \partial_t \sin(t) \\
        &= f(t) \cos(t) \\
        &= f(t) \cos(t) \partial_t t \\
        &= \cos(t) v(t) \\
        &= \cos(t) \, dt(v).
\end{align*}

\end{sol}

\begin{ex}

Let $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ denote rotation counterclockwise by the angle $\theta$. Let $dx$, $dy$ be the usual basis of 1-forms on $\mathbb{R}^2$. Show that
\begin{align*}
    \phi^* dx &= \cos(\theta) dx - \sin(\theta) dy, \\
    \phi^* dy &= \sin(\theta) dx + \cos(\theta) dy.
\end{align*}

\end{ex}

\begin{sol}

Let $v = f_i(x, y)\partial_i$ be some vector in $\text{Vect}(\mathbb{R}^2)$ and $p = (x, y) \in \mathbb{R}^2$. For $\phi$ as in solutions~\ref{sol:pullbackrotation},~\ref{sol:pushforwardrotation},
\begin{align*}
    {(\phi^* dx)}_p v &= d(\phi^* x)(v)(p) \\
        &= d(x \circ \phi)(v)(p) \\
        &= v(\cos(\theta) x - \sin(\theta) y) \\
        &= f_1(x, y) \partial_x (\cos(\theta) x - \sin(\theta) y) \\
        &\mathrel{\phantom{=}}{} + f_2(x, y) \partial_y (\cos(\theta) x
                                 - \sin(\theta) y) \\
        &= f_1(x, y) \cos(\theta) - f_2(x, y) \sin(\theta) \\
        &= \cos(\theta) f_1(x, y) \partial_x x
           - \sin(\theta) f_2(x, y) \partial_y y \\
        &= \cos(\theta) v(x) - \sin(\theta) v(y) \\
        &= \cos(\theta) dx(v) - \sin(\theta) dy(v)
\end{align*}
and similarly for $\phi^* dy$.

\end{sol}

\subsection{Change of Coordinates}

\begin{ex}\label{ex:coordinatetransformation1form}

Show that the coordinate 1-forms $dx^\mu$ really are the differentials of the local coordinates $x^\mu$ on $U$.

\end{ex}

\begin{sol}

The statement requires us to be ``working in the chart'', so for now we'll be explicit and denote the local coordinates on $U$ as $\varphi^*x^\mu$. Then the exterior derivative is
\[
    d(\varphi^*x^\mu) = \varphi^* dx^\mu.
\]
To show that this really forms a basis of coordinate 1-forms, consider the basis vectors ``in the chart'', $\varphi_*^{-1}\partial_\mu$.
\begin{align*}
    d(\varphi^* x^\mu)(\varphi_*^{-1}\partial_\nu) &= \varphi_*^{-1}\partial_\nu (\varphi^* x^\mu) \\
        &= \partial_\nu ((\varphi^* x^\mu) \circ \varphi^{-1}) \\
        &= \delta_\nu^\mu.
\end{align*}

\end{sol}

\begin{ex}\label{ex:transform1form}

In the situation above, show that
\[
    dx'^\nu = \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu.
\]
Show that for any 1-form $\omega$ on $\mathbb{R}^n$, writing
\[
    \omega = \omega_\mu dx^\mu = \omega'_\nu dx'^\nu,
\]
your components $\omega'_\nu$ are related to my components $\omega_\mu$ by
\[
    \omega'_\nu = \frac{\partial x^\mu}{\partial x'^\nu} \omega_\mu.
\]

\end{ex}

\begin{sol}

Since 1-forms form a basis, we can write
\[
    dx'^\nu = T_\mu^\nu dx^\mu
\]
for some linear transformation $T_\mu^\nu$. Acting on $\partial_\mu$, we get
\begin{align*}
    dx'^\nu \partial_\mu &= T_\lambda^\nu dx^\lambda \partial_\mu \\
        &= T_\lambda^\nu \delta_\mu^\lambda \\
        &= T_\mu^\nu,
\shortintertext{but}
    dx'^\nu\partial_\mu &= \partial_\mu x'^\nu \\
        &= \frac{\partial x'^\lambda}{\partial x^\mu} \partial'_\lambda x'^\nu \\
        &= \frac{\partial x'^\lambda}{\partial x^\mu} \delta_\lambda^\nu \\
        &= \frac{\partial x'^\nu}{\partial x^\mu}
\end{align*}
so the transformation rule for coordinate 1-forms is
\[
    dx'^\nu = \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu.
\]
We can use this to write any 1-form $\omega$ on $\mathbb{R}^n$ in a different basis, as
\[
    \omega = \omega_\mu dx^\mu = \omega_\mu \frac{\partial x^\mu}{\partial x'^\nu} dx'^\nu.
\]
In this coordinate system, we identify the components of $\omega$ as
\[
    \omega'_\nu = \frac{\partial x^\mu}{\partial x'^\nu} \omega_\mu.
\]

\end{sol}

\begin{ex}\label{ex:pullbackdx}

Show that
\[
    \phi^*(dx'^\nu) = \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu.
\]

\end{ex}

\begin{sol}

Consider the action on the coordinate vector field $\partial_\lambda$,
\begin{align*}
    \phi^*(dx'^\nu)\partial_\lambda &= d(\phi^*x'^\nu) \partial_\lambda \\
        &= \partial_\lambda(\phi^*x'^\nu) \\
        &\equiv \frac{\partial x'^\nu}{\partial x^\lambda} \tag{``get used to it''}\\
        &= \frac{\partial x'^\nu}{\partial x^\mu} \delta_\lambda^\mu \\
        &= \frac{\partial x'^\nu}{\partial x^\mu} \partial_\lambda x^\mu \\
        &= \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu \partial_\lambda.
\end{align*}
We could instead use the result from exercise~\ref{ex:coordinatetransformation1form}, again acting on the coordinate vector field $\partial_\lambda$,
\begin{align*}
   \phi^*(dx'^\nu)\partial_\lambda &= \phi^*\left(\frac{\partial x'^\nu}{\partial x^\mu} dx^\mu \right) \partial_\lambda \\
        &= \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu (\phi_* \partial_\lambda) \\
        &\equiv \frac{\partial x'^\nu}{\partial x^\mu} dx^\mu \partial_\lambda
\end{align*}
where we are sloppy about the pullback in the last line, as is the convention.

\end{sol}

\begin{ex}\label{ex:changebasisinvertible}

Let
\[
    e_\mu = T_\mu^\nu\partial_\nu
\]
where $\partial_\nu$ are the coordinate vector fields associated to local coordinates on an open set $U$, and $T_\mu^\nu$ are functions on $U$.
Show that the vector fields $e_\mu$ are a basis of vector fields on $U$ if and only if for each $p \in U$ the matrix $T_\mu^\nu(p)$ is invertible.

\end{ex}

\begin{sol}

For $\left\{e_\mu\right\}$ to form a basis, they must be linearly independent and span $U$.

Suppose $T$ is invertible at $p$. Then acting on both sides by $S = T^{-1}$ gives us
\begin{align*}
    S_\mu^\lambda e_\lambda &= S_\mu^\lambda T_\lambda^\nu \partial_\nu \\
        &= \delta_\mu^\nu \partial_\nu \\
        &= \partial_\mu.
\end{align*}
Any vector $u \in U$ can therefore be expressed as
\[
    u = u^\mu \partial_\mu = u^\mu S_\mu^\lambda e_\lambda = u'^\mu e_\mu
\]
so $\left\{e_\mu\right\}$ forms a basis for $U$.

Assume $\left\{e_\mu\right\}$ forms a basis for $U$. Then for some smooth functions on $U$, $S_\mu^\nu$,
\begin{align*}
    \partial_\mu &= S_\mu^\nu e_\nu \\
                 &= S_\mu^\nu T_\nu^\lambda \partial_\lambda.
\end{align*}
We must identify $S_\mu^\nu T_\nu^\lambda = \delta_\mu^\lambda$, so $T$ is invertible.

\end{sol}

\begin{ex}\label{ex:dualbasisexists}

Use \hyperref[ex:changebasisinvertible]{the previous exercise} to show that the dual basis exists and is unique.

\end{ex}

\begin{sol}

If $\left\{e_\mu\right\}$ is a basis of vector fields on $U$, we automatically get a dual basis of 1-forms $\left\{f^\mu\right\}$ satisfying
\[
    f^\mu(e_\nu) = \delta_\nu^\mu.
\]

We can express
\[
    f^\mu = S_\nu^\mu dx^\nu
\]
for some smooth functions $S_\nu^\mu$ on $U$. Then
\begin{align*}
    f^\mu(e_\nu) &= S_\kappa^\mu dx^\kappa (T_\nu^\lambda \partial_\lambda) \\
                 &= S_\kappa^\mu T_\nu^\lambda dx^\kappa \partial_\lambda \\
                 &= S_\kappa^\mu T_\nu^\lambda \delta_\lambda^\kappa \\
                 &= S_\lambda^\mu T_\nu^\lambda
\end{align*}
so the dual basis exists, since $T$ is invertible (from exercise~\ref{ex:changebasisinvertible}).

Suppose there exists 1-forms $\left\{g^\mu\right\}$ also satisfying $g^\mu(e_\nu) = \delta^\mu_\nu$.
Then for some smooth functions $S'^\mu_\nu$ on $U$, $g^\mu = S'^\mu_\nu dx^\nu$ and, eventually, $S'^\mu_\lambda T^\lambda_\nu = \delta ^\mu_\nu$. But the inverse of $T$ is unique, so $S' = S$ and therefore $g^\mu = f^\mu$.

\end{sol}

\begin{ex}

Let $e_\mu$ be a basis of vector fields on $U$ and let $f^\mu$ be the dual basis of 1-forms. Let
\[
    e'_\mu = T_\mu^\nu e_\nu
\]
be another basis of vector fields and let $f'^\mu$ be the corresponding basis of 1-forms. Show that
\[
    f'^\mu = {(T^{-1})}^\mu_\nu f^\nu.
\]
Show that if $v = v^\mu e_\mu = v'^\mu e'_\mu$, then
\[
    v'^\mu = {(T^{-1})}^\mu_\nu v^\nu
\]
and that if $\omega = \omega_\mu f^\mu = \omega'_\mu f'^\mu$, then
\[
    \omega'_\mu = T_\mu^\nu \omega_\nu.
\]

\end{ex}

\begin{sol}

We know that $f'^\mu = S_\nu^\mu f^\nu$ for some functions $S_\nu^\mu$ on $U$.
Then
\begin{align*}
    f'^\mu(e'_\nu) &= f'^\mu(T^\lambda_\nu e_\lambda) \\
                   &= S^\mu_\kappa f^\kappa T^\lambda_\nu e_\lambda \\
                   &= S^\mu_\kappa T^\lambda_\nu f^\kappa e_\lambda \\
                   &= S^\mu_\kappa T^\lambda_\nu \delta^\kappa\lambda \\
                   &= S^\mu_\lambda T^\lambda_\nu.
\end{align*}
But $f'^\mu(e'_\nu) = \delta^\mu_\nu$ from the definition of the dual basis, so $S = T^{-1}$.

If $v = v^\mu e_\mu = v'^\mu e'_\mu$, then $v^\nu e_\nu = v'^\lambda T_\lambda^\nu e_\nu$ and equating coefficients gets us $v^\nu = T_\lambda^\nu v'^\lambda$.
Applying $S = T^{-1}$,
\begin{align*}
    S^\mu_\nu v^\nu &= S^\mu_\nu T_\lambda^\nu v'^\lambda \\
                    &= \delta^\mu_\lambda v'^\lambda \\
                    &= v'^\mu
\end{align*}
so the components of a vector are contravariant.

If $\omega = \omega_\mu f^\mu = \omega'_\mu f'^\mu$, then $\omega_\nu f^\nu = \omega'_\lambda S^\lambda_\nu f^\nu$ and equating coefficients gets us $\omega_\nu = S_\nu^\lambda \omega'_\lambda$.
Applying $T$,
\begin{align*}
    T_\mu^\nu \omega_\nu &= T_\mu^\nu S_\nu^\lambda \omega'_\lambda \\
                         &= \delta^\lambda_\mu \omega'_\lambda \\
                         &= \omega'_\mu
\end{align*}
so the components of a 1-form are covariant.

\end{sol}

\subsection{\emph{p}-forms}

\begin{ex}\label{ex:triplewedgeproduct}

Show that
\[
    u \wedge v \wedge w = \det \begin{pmatrix}
            u_x & u_y & u_z \\
            v_x & v_y & v_z \\
            w_x & w_y & w_z
        \end{pmatrix} dx \wedge dy \wedge dz.
\]
Compare this to $\vec{u} \cdot (\vec{v} \times \vec{w})$.

\end{ex}

\begin{sol}

Let $u, v, w$ be vectors,
\begin{align*}
    u &= u_x dx + u_y dy + u_z dz, \\
    v &= v_x dx + v_y dy + v_z dz, \\
    w &= w_x dx + w_y dy + w_z dz.
\end{align*}
Then
\begin{align*}
    \SwapAboveDisplaySkip
    v \wedge w =&\ (v_x w_y - v_y w_x) \, dx \wedge dy \\
                &+ (v_y w_z - v_z w_y) \, dy \wedge dz \\
                &+ (v_z w_x - v_x w_z) \, dz \wedge dx,
\end{align*}
so the triple product
\begin{align*}
    u \wedge v \wedge w =&\ u_x (v_y w_z - v_z w_y) \, dx \wedge dy \wedge dz \\
        &+ u_y (v_z w_x - v_x w_z) \, dy \wedge dz \wedge dx \\
        &+ u_z (v_x w_y - v_y w_x) \, dz \wedge dx \wedge dy \\
        =&\ u_x (v_y w_z - v_z w_y) \, dx \wedge dy \wedge dz \\
        &- u_y (v_x w_z - v_z w_x) \, dx \wedge dy \wedge dz \\
        &+ u_z (v_x w_y - v_y w_x) \, dx \wedge dy \wedge dz \\
        =&\ \det \begin{pmatrix}
                u_x & u_y & u_z \\
                v_x & v_y & v_z \\
                w_x & w_y & w_z
            \end{pmatrix} dx \wedge dy \wedge dz.
\end{align*}

Consider the traditional vectors $\vec{u}, \vec{v}, \vec{w}$ on $\mathbb{R}^3$.
\[
    \vec{v} \times \vec{w}
        = (v_y w_z - v_z w_y) \vec{\imath}
          - (v_z w_x - v_x w_z) \vec{\jmath}
          + (v_x w_y - v_y w_x) \vec{k},
\]
so the triple product
\[
    \vec{u} \cdot (\vec{v} \times \vec{w})
        = u_x (v_y w_z - v_z w_y)
          - u_y (v_x w_z - v_z w_x)
          + u_z (v_x w_y - v_y w_x),
\]
the single component of $u \wedge v \wedge w$.

\end{sol}

\begin{ex}

Show that if $a, b, c, d$ are four vectors in a 3-dimensional space then $a \wedge b \wedge c \wedge d = 0$.

\end{ex}

\begin{sol}

Using $dx, dy, dz$ as a basis, we have from exercise~\ref{ex:triplewedgeproduct} that
\[
    b \wedge c \wedge d = \alpha \, dx \wedge dy \wedge dz, \quad
    \alpha = \det \begin{pmatrix}
        b_x & b_y & b_z \\
        c_x & c_y & c_z \\
        d_x & d_y & d_z
    \end{pmatrix}.
\]
Then
\begin{align*}
    a \wedge b \wedge c \wedge d =&\ a \wedge \alpha \, dx \wedge dy \wedge dz \\
        =&\ (a_x dx + a_y dy + a_z dz) \wedge \alpha \, dx \wedge dy \wedge dz \\
        =&\ \alpha a_x \, dx \wedge dx \wedge dy \wedge dz \\
        &+ \alpha a_y \, dy \wedge dx \wedge dy \wedge dz \\
        &+ \alpha a_z \, dz \wedge dx \wedge dy \wedge dz \\
        =&\ 0
\end{align*}
since $w \wedge w = 0$ by antisymmetry and each term contains one repeated basis element.

\end{sol}

\begin{ex}

Describe $\Lambda V$ if $V$ is 1-dimensional, 2-dimensional, or 4-dimensional.

\end{ex}

\begin{sol}

Let $u, v \in V$ over a field $\mathbb{F}$.

If $\dim(V) = 1$,
\[
    u = u_x dx, \quad v = v_x dx
\]
so $u \wedge v = 0$ by antisymmetry. Therefore $\Lambda V$ consists of $\mathbb{F}$ and all linear combinations of $dx$ (i.e. $V$).

If $\dim(V) = 2$,
\[
    u = u_x dx + u_y dy, \quad v = v_x dx + v_y dy
\]
so
\begin{align*}
    \SwapAboveDisplaySkip
    u \wedge v &= u_x v_y dx \wedge dy + u_y v_x dy \wedge dx \\
               &= (u_x v_y - u_y v_x) \, dx \wedge dy.
\end{align*}
Therefore $\Lambda V$ consists of $\mathbb{F}$, $V$ and all linear combinations of the 2-forms $dx \wedge dy$ above.

If $\dim(V) = 4$ with basis $\left\{dt, dx, dy, dz\right\}$, $\Lambda V$ will consist of $\mathbb{F}$, $V$ and all linear combinations of
\begin{gather*}
    dt \wedge dx, \quad
    dt \wedge dy, \quad
    dt \wedge dz, \quad
    dx \wedge dy, \quad
    dx \wedge dz, \quad
    dy \wedge dz, \\
    dt \wedge dx \wedge dy, \quad
    dt \wedge dx \wedge dz, \quad
    dt \wedge dy \wedge dz, \quad
    dx \wedge dy \wedge dz, \\
    dt \wedge dx \wedge dy \wedge dz.
\end{gather*}

\end{sol}

\begin{ex}

Let $V$ be an $n$-dimensional vector space. Show that $\Lambda^p V$ is empty for $p > n$ and that for $0 \leq p \leq n$ the dimension of $\Lambda^p V$ is $\frac{n!}{p! (n - p)!}$.

\end{ex}

\begin{sol}

Let $\left\{e_1, \ldots, e_n\right\}$ be a basis for $V$.
The subspace $\Lambda^p V$ consists of all linear combinations of the form $e_{i_1} \wedge \cdots \wedge e_{i_p}$.

$\Lambda^n V$ has the single basis element $e_1 \wedge \cdots \wedge e_n$.
The exterior product of any element of $\Lambda^n V$ with any $v \in V$ is necessarily zero since we have exhausted our supply of linearly independent vectors $e_i \in V$. Therefore $\Lambda^p V$ is empty for $p > n$.

The dimension of $\Lambda^p V$ is the number of subsets of size $p$ we can form from the set of $n$ basis vectors of $V$, so
\[
    \dim(\Lambda^p V) = \binom{n}{p} = \frac{n!}{p! (n - p)!}.
\]
This correctly reproduces edge cases such as $\dim(\Lambda^0 V) = \binom{n}{0} = 1$ (for a vector space $V(\mathbb{F})$, this is the underlying field $\mathbb{F}$) and $\dim(\Lambda^{n+1}V) = 0$.

\end{sol}

\begin{ex}

Show that $\Lambda V$ is the direct sum of the subspaces $\Lambda^p V$:
\[
    \Lambda V = \bigoplus \Lambda^p V,
\]
and that the dimension of $\Lambda V$ is $2^n$ if $V$ is $n$-dimensional.
\end{ex}

\begin{sol}

$\Lambda^p V$ is the subspace of $\Lambda V$ consisting of linear combinations of $p$-fold products of vectors in $V$.

For any $q \neq p$, the elements of $\Lambda^q V$ and $\Lambda^p V$ are linearly independent. Therefore for any $w \in \Lambda V$,
$w = w_0 + \cdots + w_n$
where each $w_p \in \Lambda^p V$, so
\begin{align*}
    \Lambda V &= \Lambda^0 V \oplus \cdots \oplus \Lambda^n V \\
              &= \bigoplus_{p=0}^n \Lambda^p V.
\end{align*}
The dimension of $\Lambda V$ is therefore
\begin{align*}
    \dim(\Lambda V) &= \sum_{p = 0}^n \dim(\Lambda^p V) \\
                    &= \sum_{p = 0}^n \binom{n}{p} \\
                    &= 2^n
\end{align*}
by the binomial theorem.

\end{sol}

\begin{ex}

Given a vector space $V$, show that $\Lambda V$ is a \emph{graded commutative} or \emph{supercommutative} algebra, that is, if $\omega \in \Lambda^p V$ and $\mu \in \Lambda^q V$ then
\[
    \omega \wedge \mu = {(-1)}^{pq} \mu \wedge \omega.
\]
Show that for any manifold $M$, $\Omega(M)$ is graded commutative.

\end{ex}

\begin{sol}

Let $\omega = \omega_1 \wedge \cdots \wedge \omega_p$ and $\mu = \mu_1 \wedge \cdots \wedge \mu_q$.
Then
\begin{align*}
    \omega \wedge \mu &= \omega_1 \wedge \cdots \wedge \omega_p \wedge \mu_1 \wedge \cdots \wedge \mu_q \\
        &= {(-1)}^p \mu_1 \wedge \omega_1 \wedge \cdots \wedge \omega_p \wedge \mu_2 \wedge \cdots \wedge \mu_q \\
        &= {(-1)}^{2p} \mu_1 \wedge \mu_2 \wedge \omega_1 \wedge \cdots \wedge \omega_p \wedge \mu_3 \wedge \cdots \wedge \mu_q \\
        &\vdotswithin{=} \\
        &= {(-1)}^{pq} \mu_1 \wedge \cdots \wedge \mu_q \wedge \omega_1 \wedge \cdots \wedge \omega_p \\
        &= {(-1)}^{pq} \mu \wedge \omega.
\end{align*}

The above result holds analogously for any $\omega \in \Omega^p(M)$ and $\mu \in \Omega^q(M)$. Since $\Omega(M) = \bigoplus \Omega^p(M)$, $\Omega(M)$ is graded commutative over any manifold $M$.

\end{sol}

\begin{ex}

Show that differential forms are contravariant. That is, show that if $\phi: M \to N$ is a map from the manifold $M$ to the manifold $N$, there is a unique pullback map
\[
    \phi^*: \Omega(N) \to \Omega(M)
\]
agreeing with the usual pullback on 0-forms (functions) and 1-forms and satisfying
\begin{align*}
    \SwapAboveDisplaySkip
    \phi^* (\alpha \omega) &= \alpha \phi^* \omega \\
    \phi^*(\omega + \mu) &= \phi^* \omega + \phi^* \mu \\
    \phi^*(\omega \wedge \mu) &= \phi^* \omega \wedge \phi^* \mu
\end{align*}
for all $\omega, \mu \in \Omega(N)$ and $\alpha \in \mathbb{R}$.

\end{ex}

\begin{sol}

Since any $\mu \in \Omega(N)$ can be expressed as $\mu = \mu_0 + \cdots + \mu_n$ where each $\mu_p \in \Omega^p(N)$, we can construct a pullback $\phi^*$ satisfying
\begin{align*}
    \phi^* \mu &= \phi^* (\mu_0 + \cdots + \mu_n) \\
               &= \phi^* \mu_0 + \cdots + \phi^* \mu_n
\end{align*}
by linearity and only consider how $\phi^*$ acts on each $p$-form.

The pullback of a $p$-form $\omega = \omega_1 \wedge \cdots \wedge \omega_p \in \Omega^p(N)$ should generalise the \ref{eq:pullback1form}.  %chktex 2
So on a collection of vectors $v_1, \ldots, v_p \in \text{Vect}(M)$ we would like to get
\begin{align*}
    (\phi^* \omega)(v_1, \ldots, v_p) &= \omega(\phi_* v_1, \ldots, \phi_* v_p) \\
        &=\omega_1 \wedge \cdots \wedge \omega_p (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \phi^* \omega_1 \wedge \cdots \wedge \phi^* \omega_p (v_1, \ldots, v_p).
\end{align*}
which holds since each $\omega_i$ acts on $\phi_* v_i$ independently. Then in terms of components,
\begin{align*}
    \phi^* \omega &= \phi^* (\tfrac{1}{p!}\omega_{i_1,\ldots,i_p}\, e^{i_1} \wedge \cdots \wedge e^{i_p}) \\
        &= \phi^* \tfrac{1}{p!} \omega_{i_1,\ldots,i_p} \phi^* (e^{i_1} \wedge \cdots \wedge e^{i_p}) \\
        &= \tfrac{1}{p!} \phi^* \omega_{i_1,\ldots,i_p} \phi^* e_{i^1} \wedge \cdots \wedge \phi^* e_{i^p}.
\end{align*}

Let $\omega, \mu \in \Omega^p(N)$. Then
\begin{align*}
    \phi^* (\alpha \omega) (v_1, \ldots, v_p) &= \alpha \omega (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \alpha \phi^* \omega (v_1, \ldots, v_p)
\end{align*}
so $\phi^* (\alpha \omega) = \alpha \phi^* \omega$,
\begin{align*}
    \phi^*(\omega + \mu) (v_1, \ldots, v_p) &= (\omega + \mu) (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \omega(\phi_* v_1, \ldots, \phi_* v_p) + \mu(\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \phi^* \omega (v_1, \ldots, v_p) + \phi^* \mu (v_1, \ldots, v_p) \\
        &= (\phi^* \omega + \phi^* \mu) (v_1, \ldots, v_p)
\end{align*}
so $\phi^*(\omega + \mu) = \phi^* \omega + \phi^* \mu$,
\begin{align*}
    \phi^*(\omega \wedge \mu) (v_1, \ldots, v_p) &= (\omega \wedge \mu) (\phi_* v_1, \ldots, \phi_* v_p) \\
        &= \phi^*\omega \wedge \phi^*\mu (v_1, \ldots, v_p)
\end{align*}
so $\phi^* (\omega \wedge \mu) = \phi^* \omega \wedge \phi^* \mu$.

\end{sol}

\begin{ex}

Compare how 1-forms and 2-forms on $\mathbb{R}^3$ transform under \emph{parity}. That is, let $P: \mathbb{R}^3 \to \mathbb{R}^3$ be the map
\[
    P(x, y, z) = (-x, -y, -z),
\]
known as the ``parity transformation''. Note that $P$ maps right-handed bases to left-handed bases and vice versa. Compute $\phi^*(\omega)$ when $\omega$ is the 1-form $\omega_\mu dx^\mu$ and when it is the 2-form $\frac{1}{2}\omega_{\mu\nu}dx^\mu \wedge dx^\nu$.

\end{ex}

\begin{sol}

Assume $\phi^*$ is the pullback by $P$. Consider the pullback of $dx^\mu$ acting on the coordinate vector field $\partial_\nu$,
\begin{align*}
    (\phi^* dx^\mu) \partial_\nu &= d(\phi^* x^\mu) \partial_\nu \\
        &= \partial_\nu (\phi^* x^\mu) \\
        &= \partial_\nu (x^\mu \circ \phi) \\
        &= -\delta_\nu^\mu \\
        &= -\partial_\nu x^\mu \\
        &= - dx^\mu \partial_\nu,
\end{align*}
so $\phi^* dx^\mu = -dx^\mu$.

If $\omega \in \Omega^1(\mathbb{R}^3)$, then
\[
    \phi^* \omega = \phi^*(\omega_\mu dx^\mu) = -\omega
\]
and if $\omega \in \Omega^2(\mathbb{R}^3)$, then
\begin{align*}
    \phi^* \omega &= \phi^*\left(\tfrac{1}{2}\omega_{\mu\nu} dx^\mu \wedge dx^\nu\right) \\
        &= \tfrac{1}{2}\omega_{\mu\nu} \, \phi^*(dx^\mu \wedge dx^\nu) \\
        &= \tfrac{1}{2}\omega_{\mu\nu} \, \phi^* dx^\mu \wedge \phi^* dx^\nu \\
        &= \tfrac{1}{2}\omega_{\mu\nu} (-dx^\mu) \wedge (-dx^\nu) \\
        &= \omega.
\end{align*}

\end{sol}

\subsection{The Exterior Derivative}

\begin{ex}

Show that on $\mathbb{R}^n$ the exterior derivative of any 1-form is given by
\[
    d(\omega_\mu dx^\mu) = \partial_\nu \omega_\mu \, dx^\nu \wedge dx^\mu.
\]

\end{ex}

\begin{sol}

Since $\omega_\mu$ is a 0-form,
\begin{align*}
    d(\omega_\mu dx^\mu) &= d(\omega_\mu \wedge dx^\mu) \\
        &= d\omega_\mu \wedge dx^\mu + \omega_\mu \wedge d(dx^\mu) \\
        &= d\omega_\mu \wedge dx^\mu \\
        &= \partial_\nu \omega_\mu \, dx^\nu \wedge dx^\mu.
\end{align*}

\end{sol}

\section{Rewriting Maxwell's Equations}

\subsection{The First Pair of Equations}

\begin{ex}

Show that any 2-form $F$ on $\mathbb{R} \times S$ can be uniquely expressed as $B + E \wedge dt$ in such a way that for any local coordinates $x^i$ on $S$ we have $E = E_i dx^i$ and $B = \frac{1}{2}B_{ij}dx^i \wedge dx^j$.

\end{ex}

\begin{sol}\label{sol:bplusedt}

Since $\mathbb{R} \times S$ is a manifold, we have an atlas $\left\{\varphi_\alpha\right\}$ for all open sets $U_\alpha$ giving local coordinates $x^\mu = \varphi_\alpha(u)$, $u \in U_\alpha$.

Notice that $\left\{dx^i \wedge dt, dx^i \wedge dx^j\right\}$ spans $\Omega^2(U_\alpha)$.
If $F \in \Omega^2(U_\alpha)$, we can express it as
\begin{align*}
    F &= \frac{1}{2}F_{\mu\nu}dx^\mu \wedge dx^\nu \\
      &= \frac{1}{2} (F_{0i} dt \wedge dx^i + F_{i0} dx^i \wedge dt + F_{ij} dx^i \wedge dx^j) \\
      &= \frac{1}{2} (2F_{i0} dx^i \wedge dt + F_{ij} dx^i \wedge dx^j) \\
      &= \frac{1}{2} F_{ij} dx^i \wedge dx^j + F_{i0} dx^i \wedge dt
\end{align*}
where $F_{0i} = -F_{i0}$ by antisymmetry. Comparing coefficients, we get
\[
    F = B + E \wedge dt
\]
where $F_{ij} = B_{ij}$ and $F_{i0} = E_i$. Uniqueness is automatic since each component is determined by its basis 2-form.

\end{sol}

\begin{ex}\label{ex:pformspaceandtime}

Show that for any form $\omega$ on $\mathbb{R} \times S$ there is a unique way to write $d\omega = dt \wedge \partial_t \omega + d_S \omega$ such that for any local coordinates $x^i$ on $S$, writing $t = x^0$, we have
\begin{align*}
    d_S\omega &= \partial_i \omega_I dx^i \wedge dx^I, \\
    dt \wedge \partial_t \omega &= \partial_0 \omega_I dx^0 \wedge dx^I.
\end{align*}

\end{ex}

\begin{sol}

Similarly to solution~\ref{sol:bplusedt}, since $\omega \in \Omega(U_\alpha)$ we have that $\omega = \omega_I dx^I$, so
\begin{align*}
    d\omega &= \partial_\mu \omega_I dx^\mu \wedge dx^I \\
        &= \partial_0 \omega_I dx^0 \wedge dx^I + \partial_i \omega_I dx^i \wedge dx^I \\
        &= dx^0 \wedge \partial_0 \omega_I \wedge dx^I + \partial_i \omega_I dx^i \wedge dx^I \\
        &= dx^0 \wedge \partial_0 \omega + \partial_i \omega_I dx^i \wedge dx^I \\
        &= dt \wedge \partial_t \omega + d_S \omega.
\end{align*}
Again, this is guaranteed to be unique by linearity.

\end{sol}

\subsection{The Metric}

\begin{ex}

Use the non-degeneracy of the metric to show that the map from $V$ to $V^*$ given by
\[
    v \mapsto g(v, \cdot)
\]
is an isomorphism, that is, one-to-one and onto.

\end{ex}

\begin{sol}\label{sol:metricisomorphism}

Let $v, w \in V$. By bilinearity,
\[
    g(v,\cdot) - g(w,\cdot) = g(v - w, \cdot)
\]
so $g(v,\cdot) - g(w,\cdot) = 0$ implies $v - w = 0$ by non-degeneracy or, equivalently, $g(v,\cdot) = g(w,\cdot)$ implies $v = w$. Therefore the map is injective.

Since the map is injective and, from solution~\ref{sol:gradient}, $\dim(V) = \dim(V^*)$, pick a basis $\left\{e_\mu\right\}$ for $V$ and we get a corresponding basis $\left\{f^\mu\right\}$ for $V^*$.

We claim that we can express any $\omega \in V^*$ as $\omega = g(v,\cdot)$ for some $v \in V$.
\begin{align*}
    \omega &= \omega_\nu f^\nu \\
           &= \omega_\nu g(e_\nu, \cdot) \\
           &= \omega(e_\nu) g(e_\nu, \cdot) \\
           &= g(v, e_\nu) g(e_\nu, \cdot) \\
           &= g(v^\mu e_\mu, e_\nu) g(e_\nu, \cdot) \\
           &= v^\mu g(e_\mu, e_\nu) g(e_\nu, \cdot).
\end{align*}
Because $g$ is non-degenerate, the above is solvable for $v^\mu$ and therefore the map is surjective.

\end{sol}

\begin{ex}\label{ex:loweringindex}

Let $v = v^\mu e_\mu$ be a vector field on a chart. Show that the corresponding 1-form $g(v, \cdot)$ is equal to $v_\nu f^\nu$, where $f^\nu$ is the dual basis of 1-forms and
\[
    v_\nu = g_{\mu\nu}v^\mu.
\]

\end{ex}

\begin{sol}

We'll use the same argument as in solution~\ref{sol:metricisomorphism}. Denote $\omega = g(v, \cdot)$, but since $\omega$ is a 1-form we can express it in components as
\begin{align*}
    \omega &= \omega_\nu f^\nu \\
           &= \omega(e_\nu) f^\nu \\
           &= g(v, e_\nu) f^\nu \\
           &= g(v^\mu e_\mu, e_\nu) f^\nu \\
           &= v^\mu g(e_\mu e_\nu) f^\nu \\
           &= v^\mu g_{\mu\nu} f^\nu \\
           &= g_{\mu\nu} v^\mu f^\nu \\
           &= v_\nu f^\nu
\end{align*}
where we identify $g_{\mu\nu}v^\mu = v_\nu$.

\end{sol}

\begin{ex}\label{ex:raisingindex}

Let $\omega = \omega_\mu f^\mu$ be a 1-form on a chart. Show that the corresponding vector field is equal to $\omega^\nu e_\nu$, where
\[
    \omega^\nu = g^{\mu\nu} \omega_\mu.
\]

\end{ex}

\begin{sol}

Recall that the metric $g$ is symmetric, so $g_{\mu\nu} = g(e_\mu, e_\nu) = g(e_\nu, e_\mu) = g_{\nu\mu}$.
From exercise~\ref{ex:loweringindex} we have that for a vector field $\omega^\mu e_\mu$, the corresponding 1-form is
\[
    \omega = \omega_\mu f^\mu = g_{\mu\nu}\omega^\nu f^\mu.
\]
Applying the inverse $g^{\mu\nu}$ to the components $\omega_\mu = g_{\mu\nu}\omega^\nu$,
\begin{align*}
    g^{\mu\nu} \omega_\mu &= g^{\mu\nu} g_{\mu\nu} \omega^\nu \\
                          &= \omega^\nu.
\end{align*}

\end{sol}

\begin{ex}

Let $\eta$ be the Minkowski metric on $\mathbb{R}^4$ as defined above.
Show that its components in the standard basis are
\[
    \eta_{\mu\nu} = \begin{pmatrix*}[r]
            -1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1
        \end{pmatrix*}.
\]

\end{ex}

\begin{sol}

For $v, w \in \text{Vect}(\mathbb{R}^4)$, the Minkowski metric $\eta$ is defined by
\[
    \eta(v, w) = -v^0 w^0 + v^1 w^1 + v^2 w^2 + v^3 w^3.
\]
Then in an orthonormal basis $\left\{e_\mu\right\}$,
\[
    \eta_{\mu\nu} = \eta(e_\mu, e_\nu) = \begin{dcases*}
            -1            & if $\mu = \nu = 0$, \\ %chktex 1
            \phantom{-} 1 & if $\mu = \nu$, $1 \leq \mu \leq 3$, \\ %chktex 1
            \phantom{-} 0 & otherwise,
        \end{dcases*}
\]
which we can write in matrix form as above.

\end{sol}

\begin{ex}

Show that $g^\mu_\nu$ is equal to the Kronecker delta $\delta^\mu_\nu$, that is, $1$ if $\mu = \nu$ and $0$ otherwise. Note that here the order of indices does not matter, since $g_{\mu\nu} = g_{\nu\mu}$.

\end{ex}

\begin{sol}

Lowering the index, $g_{\mu\lambda}g^\mu_\nu = g_{\lambda\nu}$.
But $g_{\lambda\nu} = g_{\mu\lambda}\delta^\mu_\nu$, so we identify $g^\mu_\nu = \delta^\mu_\nu$.

Alternatively, since $g_{\mu\nu}$ and $g^{\mu\nu}$ are inverses, $g^{\mu\lambda} g_{\lambda\nu} = \delta^\mu_\nu$ by definition.
But $g^{\mu\lambda} g_{\lambda\nu} = g^\mu_\nu$ so $g^\mu_\nu = \delta^\mu_\nu$.

\end{sol}

\begin{ex}\label{ex:innerproductpforms}

Show that the inner product of $p$-forms is non-degenerate by supposing that $(e^1, \ldots, e^n)$ is any orthonormal basis of $1$-forms in some chart, with
\[
    g(e^i, e^i) = \epsilon(i),
\]
where $\epsilon(i) = \pm 1$. Show the $p$-fold wedge products
\[
    e^{i_1} \wedge \cdots \wedge e^{i_p}
\]
form an orthonormal basis of $p$-forms with
\[
    \innerp{e^{i_1} \wedge \cdots \wedge e^{i_p}}
           {e^{i_1} \wedge \cdots \wedge e^{i_p}}
        = \epsilon(i_1) \cdots \epsilon(i_p).
\]

\end{ex}

\begin{sol}

Let $\mu = \mu^1 \wedge \cdots \wedge \mu^p$ be a $p$-form. If $\innerp{\mu}{ \omega} = 0$ for all $p$-forms $\omega = \omega^1 \wedge \cdots \wedge \omega^p$, then
\begin{align*}
    \innerp{\mu}{\omega} &= \innerp{\mu^1 \wedge \cdots \wedge \mu^p}
                                   {\omega^1 \wedge \cdots \wedge \omega^p} \\
        &= \det\big(g(\mu^i, \omega^j)\big) \\
        &= 0.
\end{align*}
But $g$ is non-degenerate, so the determinant of $g(\mu^i, \omega^j)$ must be non-zero unless $\mu = 0$.

The inner product of basis 1-forms is
\[
    g(e^i, e^j) = g^{ij} = \begin{pmatrix}
            \epsilon(1) & & \\
            & \ddots & \\
            & & \epsilon(p)
        \end{pmatrix}.
\]
From the definition of the inner product of $p$-forms,
\[
    \innerp{e^{i_1} \wedge \cdots \wedge e^{i_p}}
           {e^{j_1} \wedge \cdots \wedge e^{j_p}}
        = \det\big(g(e^{i_k}, e^{j_k})\big),
\]
but $g(e^{i_k}, e^{j_k}) = 0$ if $i_k \neq j_k$ and so too is its determinant.
Taking the inner product of a basis $p$-form with itself,
\begin{align*}
    \innerp{e^{i_1} \wedge \cdots \wedge e^{i_p}}
           {e^{i_1} \wedge \cdots \wedge e^{i_p}}
        &= \det\big(g(e^{i_k}, e^{i_k})\big) \\
        &= \prod_{k = 1}^p \epsilon(i_k) \\
        &= \epsilon(i_1) \cdots \epsilon(i_p)
\end{align*}
since $g^{ij}$ is diagonal. Therefore $\left\{e^{i_1} \wedge \cdots \wedge e^{i_p}\right\}$ forms an orthonormal basis.

\end{sol}

\begin{ex}\label{ex:esquaredbsquared}

Let $E = E_x dx + E_y dy + E_z dz$ be a 1-form on $\mathbb{R}^3$ with its Euclidean metric. Show that
\[
    \innerp{E}{E} = E_x^2 + E_y^2 + E_z^2.
\]
Similarly, let
\[
    B = B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy
\]
be a 2-form. Show that
\[
    \innerp{B}{B} = B_x^2 + B_y^2 + B_z^2.
\]
In physics, the quantity
\[
    \frac{1}{2}(\innerp{E}{E} + \innerp{B}{B})
\]
is called the \emph{energy density} of the electromagnetic field. The quantity
\[
    \frac{1}{2}(\innerp{E}{E} - \innerp{B}{B}) \label{eq:lagrangianvacuumem}
\]
is called the \emph{Lagrangian} for the vacuum Maxwell's equations, which we discuss more in Chapter 4 of Part II in greater generality.

\end{ex}

\begin{sol}

From the definition of the inner product of 1-forms,
\begin{align*}
    \innerp{E}{E} &= g^{ij} E_i E_j \\
                  &= \delta^{ij} E_i E_j \\
                  &= E_x^2 + E_y^2 + E_z^2.
\end{align*}
From exercise~\ref{ex:innerproductpforms},
\begin{align*}
    \innerp{dx^a \wedge dx^b}{dx^c \wedge dx^d} &= \det(g(dx^i, dx^j)) \\
        &= g(dx^a, dx^c) g(dx^b, dx^d) \\
        &= \delta^{ac} \delta^{bd},
\end{align*}
so by bilinearity,
\begin{align*}
    \innerp{B}{B} =&\ \innerp{B}{B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy} \\
        =&\ \innerp{B}{B_x dy \wedge dz}
            + \innerp{B}{B_y dz \wedge dx}
            + \innerp{B}{B_z dx \wedge dy} \\
        =&\ \innerp{B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy}
                   {B_x dy \wedge dz} \\
        &+ \innerp{B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy}
                  {B_y dz \wedge dx} \\
        &+ \innerp{B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy}
                  {B_z dx \wedge dy} \\
        &+ \innerp{B_z dx \wedge dy}{B_x dy \wedge dz} \\
        =&\ \innerp{B_x dy \wedge dz}{B_x dy \wedge dz} \\
        &+ \innerp{B_y dz \wedge dx}{B_y dz \wedge dx} \\
        &+ \innerp{B_z dx \wedge dy}{B_z dx \wedge dy} \\
        =&\ B_x^2 + B_y^2 + B_z^2.
\end{align*}
Alternatively, we could use the Hodge star and calculate $\innerp{\star B}{\star B}$ instead.

\end{sol}

\begin{ex}

In $\mathbb{R}^4$, let $F$ be the 2-form given by $F = B + E \wedge dt$, where $E$ and $B$ are given by \hyperref[ex:esquaredbsquared]{the formul\ae\ above}. Using the Minkowski metric on $\mathbb{R}^4$, calculate $-\frac{1}{2} \innerp{F}{F}$ and relate it to \hyperref[eq:lagrangianvacuumem]{the Lagrangian above}.

\end{ex}

\begin{sol}

The inner product of the 2-form $F$ with itself is
\begin{align*}
    \innerp{F}{F} &= \innerp{B + E \wedge dt}{B + E \wedge dt} \\
        &= \innerp{B}{B} + \innerp{B}{E \wedge dt} + \innerp{E \wedge dt}{B} + \innerp{E \wedge dt}{E \wedge dt} \\
        &= \innerp{B}{B} + \innerp{E \wedge dt}{E \wedge dt}
\end{align*}
since each component of $B$ is orthogonal to each component of $E \wedge dt$.
Focusing on the electric term,
\begin{align*}
    \innerp{E \wedge dt}{E \wedge dt} &= \det \begin{pmatrix}
                \eta(E, E) & \eta(E, dt) \\
                \eta(dt, E) & \eta(dt, dt)
            \end{pmatrix} \\
        &= -\innerp{E}{E}
\end{align*}
so
\[
    -\frac{1}{2} \innerp{F}{F} = \frac{1}{2}(\innerp{E}{E} - \innerp{B}{B}),
\]
the Lagrangian density for vacuum electromagnetism on Minkowski spacetime.

\end{sol}

\subsection{The Volume Form}

\begin{ex}

Show that any even permutation of a given basis has the same orientation, while any odd permutation has the opposite orientation.

\end{ex}

\begin{sol}

Let $\left\{e_\mu\right\}$ and $\left\{f_\mu\right\}$ be two bases related by $T: e_\mu \mapsto f_\mu$. We say that $\left\{e_\mu\right\}$ and $\left\{f_\mu\right\}$ have the same orientation if $\det(T) > 0$ and the opposite orientation if $\det(T) < 0$.

Permuting the basis by some permutation $\pi$ corresponds to a transformation by permutation matrix $T_\pi: e_\mu \mapsto f_\mu$. Since $\det(T_\pi) = \text{sign}(\pi)$, this preserves the orientation when $\pi$ is even and reverses it when $\pi$ is odd.

\end{sol}

\begin{ex}

Let $M$ be an oriented manifold. Show that we can cover $M$ with \emph{oriented charts} $\varphi_\alpha: U_\alpha \to \mathbb{R}^n$, that is, charts such that the basis $dx^\mu$ of cotangent vectors on $\mathbb{R}^n$, pulled back to $U_\alpha$ by $\varphi_\alpha$, is positively oriented.

\end{ex}

\begin{sol}

Let $p \in U_\alpha$ and $\dim(M) = n$. We have an oriented chart $\varphi_\alpha: p \mapsto x^\mu(p)$ which gives us a basis $\left\{dx^\mu\right\}$ of the cotangent space $T_p^*M$.
Pulling back by $\varphi_\alpha^*$, we get a basis of $U_\alpha$, $\left\{\varphi_\alpha^*dx^\mu\right\} = \left\{d\varphi_\alpha^*x^\mu\right\}$.

The cotangent basis $\left\{dx^\mu\right\}$ admits a volume form
\[
    \omega = dx^1 \wedge \cdots \wedge dx^n.
\]
Pulling back,
\begin{align*}
    \varphi_\alpha^* \omega &= \varphi_\alpha^* (dx^1 \wedge \cdots \wedge dx^n) \\
        &= \varphi_\alpha^* dx^1 \wedge \cdots \wedge \varphi_\alpha^* dx^n \\
        &= d\varphi_\alpha^* x^1 \wedge \cdots \wedge d\varphi_\alpha^* x^n,
\end{align*}
but this is the volume form corresponding to our basis of $U_\alpha$ and is positively oriented. Since $M$ is oriented, we can cover $M$ in such charts.

\end{sol}

\begin{ex}

Given a diffeomorphism $\phi: M \to N$ from one oriented manifold to another, we say that $\phi$ is \emph{orientation-preserving} if the pullback of any right-handed basis of a cotangent space in $N$ is a right-handed basis of a cotangent space in $M$. Show that if we can cover $M$ with charts such that the transition functions $\varphi_\alpha \circ \varphi_\beta^{-1}$ are orientation-preserving, we can make $M$ into an oriented manifold by using the charts to transfer the standard orientation on $\mathbb{R}^n$ to an orientation on $M$.

\end{ex}

\begin{sol}

Let $\dim(M) = n$ and let $p \in U_\alpha$, $q \in U_\beta$ where $U_\alpha$, $U_\beta$ are overlapping open sets with charts $\varphi_\alpha: p \mapsto \left\{x^\mu\right\}$, $\varphi_\beta: q \mapsto \left\{x'^\nu\right\}$. Each chart admits volume forms
\[
    \omega = dx^1 \wedge \cdots \wedge dx^n, \quad
    \omega' = dx'^1 \wedge \cdots \wedge dx'^n.
\]
On the overlap $U_\alpha \cap U_\beta$, we have
\[
{(\varphi_\alpha \circ \varphi_\beta^{-1})}^* dx'^\nu = T_\mu^\nu dx^\mu
\]
with the explicit representation of $T$ given by partial derivatives as per exercise~\ref{ex:pullbackdx}, so
\begin{align*}
    {(\varphi_\alpha \circ \varphi_\beta^{-1})}^* \omega' &= {(\varphi_\alpha \circ \varphi_\beta^{-1})}^* (dx'^1 \wedge \cdots \wedge dx'^n) \\
        &= {(\varphi_\alpha \circ \varphi_\beta^{-1})}^* dx'^1 \wedge \cdots \wedge {(\varphi_\alpha \circ \varphi_\beta^{-1})}^* dx'^n \\
        &= T_\mu^1 dx^\mu \wedge \cdots \wedge T_\nu^n dx^\nu \\
        &= \det(T) \, dx^1 \wedge \cdots \wedge dx^n \\
        &= \det(T) \, \omega.
\end{align*}
But since the transition function is orientation-preserving, this transfers the standard orientation on $\mathbb{R}^n$ to an orientation on $M$.

\end{sol}

\begin{ex}\label{ex:canonicalvolumeform}

Let $M$ be an oriented $n$-dimensional semi-Riemannian manifold and let $\left\{e^\mu\right\}$ be an oriented orthonormal basis of cotangent vectors\footnote{We use upper indices since we're in the cotangent space.} at some point $p \in M$. Show that
\[
    e^1 \wedge \cdots \wedge e^n = \text{vol}_p,
\]
where $\text{vol}$ is the volume form associated to the metric on $M$ and $\text{vol}_p$ is its value at $p$.

\end{ex}

\begin{sol}

The canonical volume form on $M$ with metric $g_{\mu\nu} = g(\partial_\mu, \partial_\nu)$ is given by
\[
    \text{vol} = \sqrt{|\det(g)|} \, dx^1 \wedge \cdots \wedge dx^n.
\]
We have that $e^\mu = T^\mu_\nu dx^\nu$ with $T$ as per exercise~\ref{ex:transform1form}. Taking the inner product
\begin{align*}
    \innerp{dx^\mu}{dx^\nu} &= \innerp{{(T^{-1})}^\mu_\alpha e^\alpha}{{(T^{-1})}^\nu_\beta e^\beta} \\
        &= {(T^{-1})}^\mu_\alpha {(T^{-1})}^\nu_\beta \innerp{e^\alpha}{e^\beta} \\
        &= {(T^{-1})}^\mu_\alpha {(T^{-1})}^\nu_\beta \delta^{\alpha\beta} \epsilon(\alpha) \\
        &= \pm {(T^{-1})}^\mu_\alpha {(T^{-1})}^\nu_\alpha
\end{align*}
with $\epsilon$ as per exercise~\ref{ex:innerproductpforms}.
But $\innerp{dx^\mu}{dx^\nu} = g^{\mu\nu}$, the inverse of $g_{\mu\nu}$, so
\[
g^{\mu\nu} = \pm {(T^{-1})}^\mu_\alpha {(T^{-1})}^\nu_\alpha
\]
and taking the determinant gives us $\det(T) = \sqrt{|\det(g)|}$. Then
\begin{align*}
    e^1 \wedge \cdots \wedge e^n &= \det(T) \, dx^1 \wedge \cdots \wedge dx^n \\
        &= \sqrt{|\det(g)|} \, dx^1 \wedge \cdots \wedge dx^n \\
        &= \text{vol}
\end{align*}
and, evaluated at $p$,
\[
e_p^1 \wedge \cdots \wedge e_p^n = \text{vol}_p.
\]

\end{sol}

\subsection{The Hodge Star Operator}

\begin{ex}

Show that if we define the Hodge star operator in a chart using this formula, it satisfies the property $\omega \wedge \star \mu = \innerp{\omega}{\mu} \, \text{vol}$. Use the result from exercise~\ref{ex:canonicalvolumeform}.

\end{ex}

\begin{sol}

Let $\left\{e^\mu\right\}$ be a positively oriented orthonormal basis on an $n$-dimensional manifold. Then we define the Hodge star operator in a chart as
\[
    \star (e^{i_1} \wedge \cdots \wedge e^{i_p}) = \pm e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n}
\]
where the sign is determined by $\text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p)$.

$p$-forms $\omega = \omega_I e^I$ and $\mu = \mu_J e^J$ in terms of basis 1-forms are
\[
    \omega = \omega_{i_1 \cdots i_p} e^{i_1} \wedge \cdots \wedge e^{i_p}, \quad
    \mu = \mu_{j_1 \cdots j_p} e^{j_1} \wedge \cdots \wedge e^{j_p}.
\]
Taking the inner product,
\begin{align*}
    \innerp{\omega}{\mu} &= \omega_I \mu_J \innerp{e^{i_1} \wedge \cdots \wedge e^{i_p}}{e^{j_1} \wedge \cdots \wedge e^{j_p}} \\
        &= \omega_I \mu_J \, \det \big(g(e^{i_k}, e^{j_k})\big) \\
        &= \omega_I \mu_J \delta^{IJ} \epsilon(i_1) \cdots \epsilon(i_p)
\end{align*}
where we denote $\delta^{IJ} = \delta^{i_1 j_1} \cdots \delta^{i_p j_p}$.

The Hodge dual of $\mu$ is
\[
    \star \mu = \pm \mu_J e^{j_{p + 1}} \wedge \cdots \wedge e^{j_n}
\]
and so
\[
    \omega \wedge \star \mu = \pm \omega_I \mu_J e^{i_1} \wedge \cdots \wedge e^{i_p} \wedge e^{j_{p + 1}} \wedge \cdots \wedge e^{j_n}.
\]
Notice that this will vanish if any basis elements $e^{i_k}$ of $\omega$ are equal to any basis elements $e^{j_l}$ of $\star \mu$ by antisymmetry or, by Hodge duality, are not equal to any basis element $e^{j_l}$ of $\mu$.
Then,
\begin{align*}
    \omega \wedge \star \mu &= \pm \omega_I \mu_J \delta^{IJ} e^{i_1} \wedge \cdots \wedge e^{i_p} \wedge e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n} \\
        &= \pm \omega_I \mu_J \delta^{IJ} e^{i_1} \wedge \cdots \wedge e^{i_n} \\
        &= \text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \omega_I \mu_J \delta^{IJ} e^{i_1} \wedge \cdots \wedge e^{i_n} \\
        &= {\text{sign}(i_1, \ldots, i_n)}^2 \epsilon(i_1) \cdots \epsilon(i_p) \omega_I \mu_J \delta^{IJ} e^1 \wedge \cdots \wedge e^n \\
        &= \omega_I \mu_J \delta^{IJ} \epsilon(i_1) \cdots \epsilon(i_p) e^1 \wedge \cdots \wedge e^n \\
        &= \innerp{\omega}{\mu} \, \text{vol}.
\end{align*}

\end{sol}

\begin{ex}

Calculate $\star d\omega$ when $\omega$ is a 1-form on $\mathbb{R}^3$.

\end{ex}

\begin{sol}\label{sol:curl1form}

Denote $\omega = \omega_x dx + \omega_y dy + \omega_z dz$. The gradient is
\begin{align*}
    d\omega &= d(\omega_x dx + \omega_y dy + \omega_z dz) \\
        &= d(\omega_x dx) + d(\omega_y dy) + d(\omega_z dz) \\
        &= d\omega_x \wedge dx + d\omega_y \wedge dy + d\omega_z \wedge dz \\
        &= \partial_y \omega_x dy \wedge dx + \partial_z \omega_x dz \wedge dx \\
        &\mathrel{\phantom{=}}{} + \partial_x \omega_y dx \wedge dy + \partial_z \omega_y dz \wedge dy \\
        &\mathrel{\phantom{=}}{} + \partial_x \omega_z dx \wedge dz + \partial_y \omega_z dy \wedge dz \\
        &= (\partial_y \omega_z - \partial_z \omega_y) dy \wedge dz \\
        &\mathrel{\phantom{=}}{} + (\partial_z \omega_x - \partial_x \omega_z) dz \wedge dx \\
        &\mathrel{\phantom{=}}{} +(\partial_x \omega_y - \partial_y \omega_x) dx \wedge dy.
\end{align*}
Then the Hodge dual of $d\omega$ is
\begin{align*}
    \star d\omega &= (\partial_y \omega_z - \partial_z \omega_y) \star (dy \wedge dz) \\
        &\mathrel{\phantom{=}}{} + (\partial_z \omega_x - \partial_x \omega_z) \star (dz \wedge dx) \\
        &\mathrel{\phantom{=}}{} +(\partial_x \omega_y - \partial_y \omega_x) \star (dx \wedge dy) \\
        &= \left(\partial_y \omega_z - \partial_z \omega_y\right) dx
            + \left(\partial_z \omega_x - \partial_x \omega_z\right) dy
            + \left(\partial_x \omega_y - \partial_y \omega_x\right) dz,
\end{align*}
analogous to the curl of $\omega$.

\end{sol}

\begin{ex}

Calculate $\star d \star \omega$ when $\omega$ is a 1-form on $\mathbb{R}^3$.

\end{ex}

\begin{sol}\label{sol:divergence1form}

Denote $\omega = \omega_x dx + \omega_y dy + \omega_z dz$. The Hodge dual is
\begin{align*}
    \star \omega &= \star (\omega_x dx + \omega_y dy + \omega_z dz) \\
        &= \omega_x dy \wedge dz + \omega_y dz \wedge dx + \omega_z dx \wedge dy.
\end{align*}
The gradient of the Hodge dual is then
\begin{align*}
    d \star \omega &= d(\omega_x dy \wedge dz + \omega_y dz \wedge dx + \omega_z dx \wedge dy) \\
        &= d(\omega_x dy \wedge dz) + d(\omega_y dz \wedge dx) + d(\omega_z dx \wedge dy) \\
        &= d\omega_x \wedge dy \wedge dz + d\omega_y \wedge dz \wedge dx + d\omega_z \wedge dx \wedge dy \\
        &= \partial_x \omega_x dx \wedge dy \wedge dz + \partial_y \omega_y dy \wedge dz \wedge dx + \partial_z \omega_z dz \wedge dx \wedge dy \\
        &= (\partial_x \omega_x + \partial_y \omega_y + \partial_z \omega_z) \, dx \wedge dy \wedge dz.
\end{align*}
Taking the Hodge dual of this gives
\[
    \star d \star \omega = \partial_x \omega_x + \partial_y \omega_y + \partial_z \omega_z,
\]
analogous to the divergence of $\omega$.

\end{sol}

\begin{ex}

Give $\mathbb{R}^4$ the Minkowski metric and the orientation in which $(dt, dx, dy, dz)$ is positively oriented. Calculate the Hodge star operator on all wedge products of $dx^\mu$s. Show that on $p$-forms,
\[
    \star^2 = {(-1)}^{p (4 - p) + 1}.
\]

\end{ex}

\begin{sol}

The Hodge dual of the 0-form is $\star 1 = dt \wedge dx \wedge dy \wedge dz = \text{vol}$.

For the 1-forms,
\begin{align*}
    \star dt &= - dx \wedge dy \wedge dz, &
    \star dx &= - dy \wedge dz \wedge dt, \\
    \star dy &= dz \wedge dt \wedge dx, &
    \star dz &= - dt \wedge dx \wedge dy.
\end{align*}
The 2-forms,
\begin{align*}
    \star (dt \wedge dx) &= - dy \wedge dz, &
    \star (dx \wedge dy) &= dt \wedge dz, \\
    \star (dt \wedge dy) &= - dz \wedge dx, &
    \star (dx \wedge dz) &= - dt \wedge dy, \\
    \star (dt \wedge dz) &= - dx \wedge dy, &
    \star (dy \wedge dz) &= dt \wedge dx.
\end{align*}
The 3-forms,
\begin{align*}
    \star(dt \wedge dx \wedge dy) &= - dz, &
    \star(dx \wedge dy \wedge dz) &= - dt, \\
    \star(dy \wedge dz \wedge dt) &= - dx, &
    \star(dz \wedge dt \wedge dx) &= dy.
\end{align*}
Lastly, the Hodge dual of the volume form is $\star (dt \wedge dx \wedge dy \wedge dz) = -1$.

Since all $p$-forms can be written as a linear combination of all wedge products, we can see that $\star^2 = {(-1)}^{p (4 - p) + 1}$ holds by inspection.

\end{sol}

\begin{ex}

Let $M$ be an oriented semi-Riemannian manifold of dimension $n$ and signature $(n-s, s)$. Show that on $p$-forms,
\[
    \star^2 = {(-1)}^{p (n - p) + s}.
\]

\end{ex}

\begin{sol}

Let $\omega = \omega_I e^{i_1} \wedge e^{i_p}$ be a $p$-form on $M$. Then
\[
    \star \omega = \text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \omega_I e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n}
\]
so
\begin{align*}
    \SwapAboveDisplaySkip
    \star^2 \omega &= \text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \star (\omega_I e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n}) \\
        &= \text{sign}(i_1, \ldots, i_n) \text{sign}(i_{p + 1}, \ldots, i_n, i_1, \ldots, i_p) \epsilon(i_1) \cdots \epsilon(i_n) \omega \\
        &= {(-1)}^{p (n - p)} {(-1)}^s \omega \\
        &= {(-1)}^{p (n - p) + s} \omega.
\end{align*}

\end{sol}

\begin{ex}

Let $M$ be an oriented semi-Riemannian manifold of dimension $n$ and signature $(s, n - s)$. Let $e^\mu$ be an orthonormal basis of 1-forms on some chart. Define the \emph{Levi--Civita symbol} for $1 \leq i_j \leq n$ by
\[
    \epsilon_{i_1 \cdots i_n} = \begin{dcases*}
            \text{sign}(i_1, \ldots, i_n) & \text{all $i_j$ distinct}, \\
            0                             & otherwise.
        \end{dcases*}
\]
Show that for any $p$-form
\[
    \omega = \frac{1}{p!} \omega_{i_1 \cdots i_p} e^{i_1} \wedge \cdots \wedge e^{i_p}
\]
we have
\[
    {(\star \omega)}_{j_1 \cdots j_{n - p}} = \frac{1}{p!} {\epsilon^{i_1 \cdots i_p}}_{j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p}.
\]

\end{ex}

\begin{sol}

Taking the Hodge dual of $\omega$,
\begin{align*}
    \star \omega &= \frac{1}{p!} \text{sign}(i_1, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \omega_{i_1 \cdots i_p} e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n} \\
        &= \frac{1}{p!} \text{sign}(i_1, \ldots, i_p, i_{p + 1}, \ldots, i_n) \epsilon(i_1) \cdots \epsilon(i_p) \omega_{i_1 \cdots i_p} e^{i_{p + 1}} \wedge \cdots \wedge e^{i_n}.
\end{align*}
We're free to rename $i_{p + 1}, \ldots, i_n$ to $j_1, \ldots, j_{n - p}$ and, if we use the Levi--Civita symbol,
\begin{align*}
    \star \omega &= \frac{1}{p!} \epsilon(i_1) \cdots \epsilon(i_p) \epsilon_{i_1 \cdots i_p j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}} \\
        &= \frac{1}{p!} \epsilon(i_1) \cdots \epsilon(i_p) \delta^{i_1 k_1} \cdots \delta^{i_p k_p} \epsilon_{k_1 \cdots k_p j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}} \\
        &= \frac{1}{p!} g^{i_1 k_1} \cdots g^{i_p k_p} \epsilon_{k_1 \cdots k_p j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}} \\
        &= \frac{1}{p!} {\epsilon^{i_1 \cdots i_p}}_{j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}}
\end{align*}
so if $\star \omega$ in terms of components is
\[
    \star \omega = {(\star \omega)}_{j_1 \cdots j_{n - p}} e^{j_1} \wedge \cdots \wedge e^{j_{n - p}},
\]
then
\[
    {(\star \omega)}_{j_1 \cdots j_{n - p}} = \frac{1}{p!} {\epsilon^{i_1 \cdots i_p}}_{j_1 \cdots j_{n - p}} \omega_{i_1 \cdots i_p}.
\]

\end{sol}

\subsection{The Second Pair of Equations}

\begin{ex}

Check this result.

\end{ex}

\begin{sol}

The claim is that on Minkowski space, the second pair of Maxwell equations,
\[
    \nabla \cdot \vec{E} = \rho, \qquad
    \nabla \times \vec{B} - \frac{\partial \vec{E}}{\partial t} = \vec{\jmath},
\]
can be rewritten as
\[
    \star_S \, d_S \star_S E = \rho, \qquad
    -\partial_t E + \star_S \, d_S \star_S B = j
\]
where $\star_S$ denotes the Hodge star operator on space, that is, $\mathbb{R}^3$ with its usual Euclidean metric.

Since $E = E_x dx + E_y dy + E_z dz$ is a 1-form, we have from solution~\ref{sol:divergence1form} that
\begin{align*}
    \star_S \, d_S \star_S E &= \partial_x E_x + \partial_y E_y + \partial_z E_z \\
                             &= \nabla \cdot \vec{E} \\
                             &= \rho.
\end{align*}

Consider now the Hodge dual in space of the 2-form $B$,
\begin{align*}
    \star_S B &= \star_S (B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy) \\
              &= B_x dx + B_y dy + B_z dz.
\end{align*}
From solution~\ref{sol:curl1form}, we get
\begin{align*}
    \star_S \, d_S \star_S B &= \star_S \, d_S (B_x dx + B_y dy + B_z dz) \\
        &= \left(\partial_y B_z - \partial_z B_y\right) dx
            + \left(\partial_z B_x - \partial_x B_z\right) dy
            + \left(\partial_x B_y - \partial_y B_x\right) dz \\
        &= {(\nabla \times \vec{B})}_i dx^i.
\end{align*}
Since we're in Euclidean space, we can turn vector fields into 1-forms easily. As in exercise~\ref{ex:raisingindex},
\begin{gather*}
    g(\star_S \, d_S \star_S B, \cdot)
        = \delta^{ij} {(\nabla \times \vec{B})}_i \partial_j
        = \nabla \times \vec{B}, \\
    g(-\partial_t E, \cdot) = -\delta^{ij} \partial_t E_i \partial_j = -\partial_t \vec{E}
\end{gather*}
and $g(j, \cdot) = \vec{\jmath}$,
so $-\partial_t E + \star_S \, d_S \star_S B = j$ is component-by-component equivalent to the last Maxwell equation.

\end{sol}

\begin{ex}

Check the calculations above.

\end{ex}

\begin{sol}

Assume that $M = \mathbb{R} \times S$ is an oriented semi-Riemannian manifold where $S$ is space and let the current be given by $J = j - \rho \, dt$. Suppose $\dim(S) = 3$ and the metric is static and of the form $g = -dt^2 + ^3 g$ where $^3 g$ is a Riemannian metric on $S$. We want to show that
\[
    \star d \star F = J
\]
is equivalent to the second pair of Maxwell equations.

Taking the Hodge dual of the electromagnetic 2-form,
\[
    \star F = \star B + \star (E \wedge dt)
\]
and looking at the electric and magnetic terms separately, we get
\begin{align*}
    \star (E \wedge dt) &= \star (E_x dx \wedge dt + E_y dy \wedge dt + E_z dz \wedge dt) \\
        &= E_x dy \wedge dz + E_y dz \wedge dx + E_z dx \wedge dy \\
        &= \star_S E
\end{align*}
and
\begin{align*}
    \SwapAboveDisplaySkip
    \star B &= \star (B_x dy \wedge dz + B_y dz \wedge dx + B_z dx \wedge dy) \\
            &= B_x dt \wedge dx + B_y dt \wedge dy + B_x dt \wedge dz \\
            &= - B_x dx \wedge dt - B_y dy \wedge dt - B_z dz \wedge dt \\
            &= - \star_S B \wedge dt
\end{align*}
so
\[
    \star F = \star_S E - \star_S B \wedge dt.
\]
The exterior derivative of this is then
\[
    d \star F = d \star_S E - d (\star_S B \wedge dt)
\]
and we again look at the electric and magnetic terms separately to get
\begin{align*}
     d (\star_S B \wedge dt) &= dt \wedge \partial_t \star_S B \wedge dt + d_S \star_S B \wedge dt \\
        &= d_S \star_S B \wedge dt.
\end{align*}
and
\begin{align*}
    \SwapAboveDisplaySkip
    d \star_S E &= dt \wedge \partial_t \star_S E + d_S \star_S E \\
        &= \partial_t \star_S E \wedge dt + d_S \star_S E \\
        &= \star_S \partial_t E \wedge dt + d_S \star_S E
\end{align*}
by making use of the result from exercise~\ref{ex:pformspaceandtime} and reversing the exterior product without a sign change since $\star_S E$ is a 2-form, so
\[
    d \star F = \star_S \partial_t E \wedge dt + d_S \star_S E - d_S \star_S B \wedge dt.
\]
Applying the Hodge star to each term, for $\star_S \partial_t E \wedge dt$ we get
\begin{align*}
    \star (\star_S \partial_t E \wedge dt) &= \star \bigl(
            \partial_t E_x dy \wedge dz \wedge dt \\
            &\phantom{= \star \,\, } + \partial_t E_y dz \wedge dx \wedge dt \\
            &\phantom{= \star \,\, } + \partial_t E_z dx \wedge dy \wedge dt \bigr) \\
        &= - \partial_t E,
\end{align*}
for the 3-form on space $d_S \star_S E$ we get
\[
    \star d_S \star_S E = - \star_S d_S \star_S E \wedge dt
\]
and for the 3-form on spacetime $d_S \star_S B \wedge dt$ we get
\[
    \star d_S \star_S B \wedge dt = - \star_S d_S \star_S B.
\]
Combining,
\[
    \star d \star F = - \partial_t E - \star_S d_S \star_S E \wedge dt + \star_S d_S \star_S B.
\]
But $\star d \star F = J$, so
\[
    - \partial_t E - \star_S d_S \star_S E \wedge dt + \star_S d_S \star_S B = j - \rho \, dt
\]
and equating coefficients gives us
\[
    \star_S d_S \star_S E = \rho, \qquad
    - \partial_t E + \star_S d_S \star_S B = j.
\]

\end{sol}

\end{document}
